<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Postgres | Craig Kerstiens]]></title>
  <link href="http://www.craigkerstiens.com/categories/postgres/atom.xml" rel="self"/>
  <link href="http://www.craigkerstiens.com/"/>
  <updated>2015-12-27T09:06:05-08:00</updated>
  <id>http://www.craigkerstiens.com/</id>
  <author>
    <name><![CDATA[Craig Kerstiens]]></name>
    <email><![CDATA[craig.kerstiens@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Postgres 9.5 - The feature rundown]]></title>
    <link href="http://www.craigkerstiens.com/2015/12/27/postgres-9-5-feature-rundown/"/>
    <updated>2015-12-27T00:00:00-08:00</updated>
    <id>http://www.craigkerstiens.com/2015/12/27/postgres-9-5-feature-rundown</id>
    <content type="html"><![CDATA[<p>The headline of Postgres 9.5 is undoubtedly: Insert&hellip; on conflict do nothing/update or more commonly known as Upsert or Merge. This removes one of the last remaining features which other databases had over Postgres. Sure we&rsquo;ll take a look at it, but first let&rsquo;s browse through some of the other features you can look forward to when Postgres 9.5 lands:</p>

<!--more-->


<h3>Grouping sets, cube, rollup</h3>

<p>Pivoting in Postgres has <a href="http://www.craigkerstiens.com/2013/06/27/Pivoting-in-Postgres/">sort of been possible</a> as has rolling up data, but it required you to know what those values and what you were projecting to, to be known. With the new functionality to allow you to group various sets together rollups as you&rsquo;d normally expect to do in something like Excel become trivial.</p>

<p>So now instead you simply add the grouping type just as you would on a normal group by:</p>

<pre><code>SELECT department, role, gender, count(*)
FROM employees
GROUP BY your_grouping_type_here;
</code></pre>

<p>By simply selecting the type of rollup you want to do Postgres will do the hard work for you. Let&rsquo;s take a look at the given example of department, role, gender:</p>

<ul>
<li><code>grouping sets</code> will project out the count for each specific key. As a result you&rsquo;d get each department key, with other keys as null, and the count for each that met that department.</li>
<li><code>cube</code> will give you the same values as above, but also the rollups of every individual combination. So in addition to the total for each department, you&rsquo;d get breakups by the department and gender, and department and role, and department and role and gender.</li>
<li><code>rollup</code> will give you a slightly similar version to cube but only give you the detailed groupings in the order they&rsquo;re presented. So if you specified <code>roll (department, role, gender)</code> you&rsquo;d have no rollup for department and gender alone.</li>
</ul>


<p><em>Check the what&rsquo;s new wiki for a bit more clarity on <a href="https://wiki.postgresql.org/wiki/What's_new_in_PostgreSQL_9.5#GROUPING_SETS.2C_CUBE_and_ROLLUP">examples and output</a></em></p>

<h3>Import foreign  schemas</h3>

<p>I only use foreign tables about once a month, but when I do use them they&rsquo;ve inevitably saved many hours of creating a one off ETL process. Even still the effort to setup new foreign tables has shown a bit of their infancy in Postgres. Now once you&rsquo;ve setup your foreign database, you can import the schema, either all of it or specific tables you prefer.</p>

<p>It&rsquo;s as simple as:</p>

<pre><code>IMPORT FOREIGN SCHEMA public
FROM SERVER some_other_db INTO reference_to_other_db;
</code></pre>

<h3>pg_rewind</h3>

<p>If you&rsquo;re managing your own Postgres instance for some reason and running HA, pg_rewind could become especially handy. Typically to spin up replication you have to first download the physical, also known as base, backup. Then you have to replay the Write-Ahead-Log or WALâ€“so it&rsquo;s up to date then you actually flip on replication.</p>

<p>Typically with databases when you fail over you shoot the other node in the head or <a href="https://en.wikipedia.org/wiki/STONITH">STONITH</a>. This means just get rid of it, completely throw it out. This is still a good practice, so bring it offline, make it inactive, but from there now you could then flip it into a mode and us pg_rewind. This could save you pulling down lots and lots of data to get a replica back up once you have failed over.</p>

<h3>Upsert</h3>

<p>Upsert of course will be the highlight of Postgres 9.5. I already talked about it some when <a href="http://www.craigkerstiens.com/2015/05/08/upsert-lands-in-postgres-9.5/">it initially landed</a>. The short of it is, if you&rsquo;re inserting a record and there&rsquo;s a conflict, you can choose to:</p>

<ul>
<li>Do nothing</li>
<li>Do some form of update</li>
</ul>


<p>Essentially this will let you have the typically experience of create or update that most frameworks provide but without a potential race condition of incorrect data.</p>

<h3>JSONB pretty</h3>

<p>There&rsquo;s a few updates <a href="https://wiki.postgresql.org/wiki/What's_new_in_PostgreSQL_9.5#JSONB-modifying_operators_and_functions">to JSONB</a>. The one I&rsquo;m most excited about is making JSONB output in psql read much more legibly.</p>

<p>If you&rsquo;ve got a JSONB field just give it a try with:</p>

<pre><code>SELECT jsonb_pretty(jsonb_column)
FROM foo;
</code></pre>

<h3>Give it a try</h3>

<p>Just in time for the new year <a href="http://www.postgresql.org/about/news/1631/">the RC is ready</a> and you can get hands on with it. Give it a try, and if there&rsquo;s more you&rsquo;d like to hear about Postgres please feel free to drop me a note <a href="mailto:craig.kerstiens@gmail.com">craig.kerstiens@gmail.com</a>.</p>

<script type="text/javascript">
  (function() {
    window._pa = window._pa || {};
    // _pa.orderId = "myOrderId"; // OPTIONAL: attach unique conversion identifier to conversions
    // _pa.revenue = "19.99"; // OPTIONAL: attach dynamic purchase values to conversions
    // _pa.productId = "myProductId"; // OPTIONAL: Include product ID for use with dynamic ads
    var pa = document.createElement('script'); pa.type = 'text/javascript'; pa.async = true;
    pa.src = ('https:' == document.location.protocol ? 'https:' : 'http:') + "//tag.marinsm.com/serve/517fd07cf1409000020002dc.js";
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(pa, s);
  })();
</script>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Postgres and Node - Hands on using Postgres as a Document Store with MassiveJS]]></title>
    <link href="http://www.craigkerstiens.com/2015/12/08/massive-json/"/>
    <updated>2015-12-08T00:00:00-08:00</updated>
    <id>http://www.craigkerstiens.com/2015/12/08/massive-json</id>
    <content type="html"><![CDATA[<p>JSONB in Postgres is absolutely awesome, but it&rsquo;s taken a little while for libraries to come around to make it as useful as would be ideal. For those not following along with Postgres lately, here&rsquo;s the quick catchup for it as a NoSQL database.</p>

<ul>
<li>In Postgres 8.3 over 5 years ago Postgres received <a href="http://www.craigkerstiens.com/2013/07/03/hstore-vs-json/">hstore a key/value</a> store directly in Postgres. It&rsquo;s big limitation was it was only for text</li>
<li>In the years after it got GIN and GiST indexes to make queries over hstore extremely fast indexing the entire collection</li>
<li>In Postgres 9.2 we got JSON&hellip; sort of. Really this way only text validation, but allowed us to create some <a href="http://www.craigkerstiens.com/2013/05/29/postgres-indexes-expression-or-functional-indexes/">functional indexes</a> which were still nice.</li>
<li>In Postgres 9.4 we got JSONB &ndash; the B stands for Better according to <a href="http://www.twitter.com/leinweber">@leinweber</a>. Essentially this is a full binary JSON on disk, which can perform as fast as other NoSQL databases using JSON.</li>
</ul>


<!--more-->


<p>This is all great, but when it comes to using JSON you need a library that plays well here. As you might have guessed it from <a href="http://www.craigkerstiens.com/2015/11/30/massive-node-postgres-an-overview-and-intro/">my previous post this is where MassiveJS comes in</a>. Most ORMs take a more legacy approach to <a href="http://www.craigkerstiens.com/2014/01/24/rethinking-limits-on-relational/">how they work with the database</a>, in contrast the other side of the world believes in document only storage way is the future. In contrast Postgres believes there is a time and place for everything, just like Massive, except it believes Postgres is the path <a href="http://www.craigkerstiens.com/2012/04/30/why-postgres/">just as I do</a>.</p>

<p>Alright, enough context, let&rsquo;s take a look.</p>

<h3>Getting all setup</h3>

<p>First go ahead and create a database, let&rsquo;s call it massive, and then let&rsquo;s connect to it and create our example table:</p>

<pre><code>$ createdb massive
$ psql massive
# create table posts (id serial primary key, body jsonb);
</code></pre>

<p>Now that we&rsquo;ve got our database setup let&rsquo;s seed it with some data. If you want you can simple hop over to the github repo and pull it down then run <code>node load_json.js</code> to load the example data. A quick look at it, given an <code>example.json</code> file we&rsquo;re going to iterate over it. For each record in there, we&rsquo;re going to call saveDoc. Based on our table which has a unique id key and a body jsonb field it&rsquo;ll simply save our JSON document into that table:</p>

<pre><code>var parsedJSON = require('./example.json');

for(i = 0; i &lt; parsedJSON.posts.length; i++) {
    db.saveDoc("posts", parsedJSON.posts[1], function(err,res){});
};
</code></pre>

<p><em>If you want to just take a look at this <a href="https://github.com/craigkerstiens/json_node_example">github repo</a>, once you create a database you can run <code>node load_json.js</code> to seed it.</em></p>

<h3>Why JSON at all?</h3>

<p>JSON data is all over the place, in many cases it&rsquo;s fast and flexible and allows you to move more quickly. Yes, much of the time normalizing your data can be useful, but there is something to be said for expediency saving some data and querying across it. Querying across some giant document also used to be much more expensive, but now with JSONB and it&rsquo;s indexes that can be extremely fast.</p>

<h3>Querying</h3>

<p>So how do we go about querying? Well it&rsquo;s pretty simple with Massive, they provide a nice <code>findDoc</code> function to let you just search for contents of a specific key within the document. Let&rsquo;s say I wanted to pull back all posts that are in the Postgres category, well it&rsquo;s as simple as:</p>

<pre><code>db.posts.findDoc({title : 'Postgres'}, function(err,docs){
    console.log(docs);
});
</code></pre>

<p>The real beauty of this is if you added a GIN index (which will index the entire document) this query will be <a href="http://obartunov.livejournal.com/175235.html">quite performant</a>.</p>

<p><em>Just make sure to add your GIN index</em>:</p>

<pre><code>CREATE INDEX idx_posts ON posts USING GIN(body jsonb_path_ops); 
CREATE INDEX idx_posts_search ON posts USING GIN(search);  
</code></pre>

<p>But even better, with Massive it&rsquo;ll automatically add these for you if you just start saving docs. It will automatically create the table and appropriate indexes, just doing the correct thing out of the box.</p>

<h3>Full text and JSON</h3>

<p>Cool, so you can do an exact look up. Which is great when you&rsquo;re matching a category&hellip; which could be easily normalized. It&rsquo;s great when you&rsquo;re matching numbers, which also could likely reside in their own column. But what about when you&rsquo;re searching over a large document, or a set of keys within some document which may require several joins, or indeterminate data structure, well you may want to search for the presence of that string at all. As you may have guessed this is quite trivial.</p>

<pre><code>db.posts.searchDoc({
    keys : ["title", "category"],
    term : ["Postgres"]
}, function(err,docs){
    console.log(docs);
})
</code></pre>

<p>Hopefully it&rsquo;s pretty straight forward, but to be very clear. Call out the document table you want to search, then the keys you&rsquo;ll want to include in the search, then the term. This will search for any place the contents that string are found in matching values for those keys.</p>

<p>Which will nicely yield the expected documents:</p>

<pre><code>[ { link: 'http://www.craigkerstiens.com/2015/05/08/upsert-lands-in-postgres-9.5/',
    title: 'Upsert Lands in PostgreSQL 9.5 â€“ a First Look',
    category: 'Postgres',
    comments: [ [Object] ],
    id: 2 },
  { link: 'http://www.craigkerstiens.com/2015/11/30/massive-node-postgres-an-overview-and-intro/',
    title: 'Node, Postgres, MassiveJS - a Better Database Experience',
    id: 3 } ]
</code></pre>

<h3>In conclusion</h3>

<p>While Massive isn&rsquo;t perfect, its approach to storing queries in files, using the schema vs. having to define your models in code and the database, and it&rsquo;s smooth document integration makes it a real contender as a better database library when working with Node. Give it a try and let me know your thoughts.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Node, Postgres, MassiveJS - A better database experience]]></title>
    <link href="http://www.craigkerstiens.com/2015/11/30/massive-node-postgres-an-overview-and-intro/"/>
    <updated>2015-11-30T00:00:00-08:00</updated>
    <id>http://www.craigkerstiens.com/2015/11/30/massive-node-postgres-an-overview-and-intro</id>
    <content type="html"><![CDATA[<p>First some backgroundâ€“I&rsquo;ve always had a bit of a love hate relationship with ORMs. ORMs are great for basic crud applications, which inevitably happens at some point for an app. The main two problems I have with ORMs is:</p>

<ol>
<li>They treat all databases as equal (yes, this is a little overgeneralized but typically true). They claim to do this for database portability, but in reality an app still can&rsquo;t just up and move from one to another.</li>
<li>They don&rsquo;t handle complex queries well at all. As someone that sees SQL as a very powerful language, taking away all the power just leaves me with pain.</li>
</ol>


<p><em>Of course these aren&rsquo;t the <a href="https://kev.inburke.com/kevin/faster-correct-database-queries/">only issues</a> with them, just the two ones I personally run into over and over.</em></p>

<p>In some playing with Node I was optimistic to explore <a href="http://massive-js.readthedocs.org">Massive.JS</a> as it seems to buck the trend of just imitating all other ORMs. My initial resultsâ€“it makes me want to do more with Node just for this library. After all the power of a language is the ecosystem of libraries around it, not just the core language. So let&rsquo;s take a quick tour through with a few highlights of what makes it really great.</p>

<!--more-->


<h3>Getting setup</h3>

<p>Without further adieu here&rsquo;s a quick tour around it.</p>

<p>First let&rsquo;s pull down the example database from <a href="http://postgresguide.com/setup/example.html">PostgresGuide</a></p>

<p>Then let&rsquo;s setup out Node app:</p>

<pre><code>$ npm init
$ npm install massive --save
</code></pre>

<h3>Connecting and querying</h3>

<p>Now let&rsquo;s try to connect and say query a user from within our database. Create the following as an <code>index.js</code> file, then run with <code>node index.js</code>:</p>

<pre><code>var massive = require("massive");
var connectionString = "postgres://ckerstiens:@localhost/example";

var db = massive.connectSync({connectionString : connectionString});

db.users.find(1, function(err,res){
  console.log(res);
});
</code></pre>

<p>Upon first run if you&rsquo;re like me and use the <a href="http://postgresguide.com/setup/example.html">PostgresGuide example database</a> (which I now need to go back and tidy up), you&rsquo;ll get the following:</p>

<pre><code>db.users.find(1, function(err,res){
        ^
TypeError: Cannot read property 'find' of undefined
</code></pre>

<p>I can&rsquo;t describe how awesome it is to see this. What&rsquo;s happening is when Massive loads up it&rsquo;s connecting to your database, checking what tables you have. In this case though because we don&rsquo;t have a proper primary key defined it doesn&rsquo;t load them. It could treat id as some magical field of course like Rails used to and ignore the need for an index, but instead it not only encourages a good database design but requires it.</p>

<p>So let&rsquo;s go back and create our index in our database:</p>

<pre><code>$ psql example
$ alter table users add primary key (id);
</code></pre>

<p>Alright now let&rsquo;s run our script again with <code>node index.js</code> and see what we have:</p>

<pre><code>{ id: 1,
  email: 'john.doe@gmail.com',
  created_at: Thu Sep 24 2015 03:42:52 GMT-0700 (PDT),
  deleted_at: null }
</code></pre>

<p>Perfect! Now we&rsquo;re all connected and it even queried our database for us. Now let&rsquo;s take a few more look at some of the operators.</p>

<h3>Running an arbitrary query</h3>

<p><code>db.run</code> will let me run any arbritrary SQL. An example such as <code>db.run("select 'hello'")</code> will produce [ { &lsquo;?column?&rsquo;: &lsquo;hello&rsquo; } ].</p>

<p>This makes it nice and easier for us to break out of the standard ORM model and just run SQL.</p>

<h3>Find for quick look ups</h3>

<p>Similar to so many other database tools <code>find</code> will offer you the most common quick look ups:</p>

<pre><code>$ db.users.find({email: 'jane.doe@gmail.com'}, function(err, res){console.log(res)});
$ db.users.find({'created_at &gt;': '2015-09-24'}, function(err, res){console.log(res)});
</code></pre>

<p>And of course there&rsquo;s a where operator for multiple conditions.</p>

<h3>Structuring queries in your application</h3>

<p>While in the next post I&rsquo;ll dig in deep to JSON, this is perhaps my favorite feature of Massive&hellip; It&rsquo;s design for pulling out queries into individudal SQL files. Simply create a <code>db</code> folder and put your SQL in there. Let&rsquo;s take the most basic example of our user email lookup and put it in <code>user_lookup.sql</code></p>

<pre><code>SELECT *
FROM users
WHERE email = $1
</code></pre>

<p>Now back in our application we can run this and pass in a parameter to it:</p>

<pre><code>db.user_lookup(['jane.doe@gmail.com'], function(err,res){
  console.log(res);
});
</code></pre>

<p>This separation of our queries from our code makes it easier to track them, view diffs, and even more so <a href="http://www.craigkerstiens.com/2012/11/17/how-i-write-sql/">create very readable SQL</a>.</p>

<h3>Up next</h3>

<p>So sure, you can connect to a database, you can query some things. There were a couple of small but more novel things that we blew through in here. First is the fact I didn&rsquo;t have to define all my schema, it just knew it as <a href="http://www.craigkerstiens.com/2014/01/24/rethinking-limits-on-relational/">it really should</a>. The separation of SQL queries you&rsquo;ll custom write into files is simple, but will make for much more maintainable applications over the long term. And best of all is the JSON support, which I&rsquo;ll get to soon&hellip;</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Moving past averages in SQL (Postgres) â€“ Percentiles]]></title>
    <link href="http://www.craigkerstiens.com/2015/06/07/Moving-past-averages-in-sql/"/>
    <updated>2015-06-07T00:00:00-07:00</updated>
    <id>http://www.craigkerstiens.com/2015/06/07/Moving-past-averages-in-sql</id>
    <content type="html"><![CDATA[<p>Often when you&rsquo;re tracking a metric for the first time you take a look at your average. For example what is your ARPU &ndash; Average Revenue Per User. In theory this tells you if you can acquire new user how much you&rsquo;ll make off that user. Or maybe what&rsquo;s your average life time value of a customer. Yet, many that are more familiar looking and extracting meaning from data median or a few different looks at <a href="http://apmblog.dynatrace.com/2012/11/14/why-averages-suck-and-percentiles-are-great/">percentiles can be much more meaningful</a>.</p>

<!--more-->


<p>And while you can very easily get the <code>AVG</code> in Postgres, with a small amount more effort you can report on percentiles as well. Window functions have been around for some time in Postgres. They allow you to order your result set over a certain group. The most basic example is if you want to order by date, but know which one falls at place 10 in order you can use a window function and project out the <code>rank()</code>.</p>

<p>Beyond outputting the rank yourself and doing extra manipulation Postgres has some great utilities to make the most common uses even easier. Being able to compute things such as the perc 95 directly on the data, or lay out for every record in the result where it falls within a percentile is hugely useful. Let&rsquo;s take a look:</p>

<p>Assuming you have a table called purchases, which has a total in it we could try:</p>

<pre><code>SELECT id,
       total,
       ntile(100) OVER (ORDER BY total) AS perc_rank
FROM purchases
</code></pre>

<p>This would give us something like:</p>

<pre><code>    id    |  total  | perc_rank
----------|---------|-----------
   264    |  12034  |    100
   643    |  11830  |    100
...
...
   304    |   751   |     95
</code></pre>

<p>What this would tell us is we have less than 5% of our purchases that have a total over 751. From here you can start to dig in and extract all sorts of different meanings, and by doing directly in SQL you&rsquo;re closer to the data and have one less processing step.</p>

<p>Percentiles get even more fun with the ordered set functions that came out in <a href="/2014/02/02/Examining-PostgreSQL-9.4/">Postgres 9.4</a>. They even allow you to project out hypothetical values in certain cases. For now I&rsquo;d encourage adding ntile to your toolbox anytime you&rsquo;re analyzing average or medians it will make your world a bit better, and then consider exploring further on the <a href="http://www.postgresql.org/docs/9.4/static/functions-aggregate.html#FUNCTIONS-ORDEREDSET-TABLE">ordered set functions</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Upsert lands in PostgreSQL 9.5 â€“ A first look]]></title>
    <link href="http://www.craigkerstiens.com/2015/05/08/upsert-lands-in-postgres-9.5/"/>
    <updated>2015-05-08T00:00:00-07:00</updated>
    <id>http://www.craigkerstiens.com/2015/05/08/upsert-lands-in-postgres-9.5</id>
    <content type="html"><![CDATA[<p>If youâ€™ve followed anything Iâ€™ve <a href="/2012/04/30/why-postgres/">written about Postgres</a>, you know that Iâ€™m a fan. At the same time you know that thereâ€™s been one feature that so many other databases have, which Postgres lacks and it <a href="/2014/08/15/my-postgres-wishlist-for-9.5/">causes a huge amount of angst for not being in Postgres</a>â€¦ Upsert. Well the day has come, itâ€™s finally committed and will be available <a href="http://git.postgresql.org/gitweb/?p=postgresql.git;a=commit;h=168d5805e4c08bed7b95d351bf097cff7c07dd65">in Postgres 9.5</a>.</p>

<p>Sure weâ€™re still several months away from Postgres 9.5 being released, anywhere from 3-6 months as a best guess. That doesnâ€™t mean we canâ€™t take a first look at this feature. Though before we get into it a few special call outs of thanks to Peter Geoghegan of the <a href="http://www.heroku.com/postgres">Heroku Postgres</a> team for being the primary author on it, Andres Freund who recently just joined <a href="https://www.citusdata.com">Citus Data</a> for his heavy contributions, and Heikki Linnakangas as well for his contributions.</p>

<!-- more -->


<p>And now onto the exploration. Upsert is the common name, but if youâ€™re unfamiliar upsert is essentially create or update â€“ Create this new record, but if a conflict exists update it. Letâ€™s take a practical example.</p>

<p>Assume you have a web scraper that imports product information into a table. Each product has a UPC code, title, description, and link. Thereâ€™s a unique constraint on the UPC code. Now, if your web scraper tries to insert a new product, and a product with the same UPC already exists, youâ€™d usually get an error. But you donâ€™t want the query to fail, youâ€™d want to update the existing product instead. Maybe with a new image, maybe a new description, whatever have you, but I donâ€™t want it to blow upâ€¦ I simply want to capture the new data and save it.</p>

<p><strong>So before</strong>: Insert a recordâ€¦ Exception this violates a unique constraintâ€¦ Let your app figure out what to do. <em>protip: often applications would try to work around this, but you can run a chance of a race condition and duplicate records if thereâ€™s a conflict. TLDR; itâ€™s not a perfect solution.</em></p>

<p><strong>Now</strong>: Insert a recordâ€¦ Thereâ€™s a unique constraint violationâ€¦ Okay, letâ€™s just update all the new recordâ€™s fields <strong>inside a single transaction</strong></p>

<p>So enough explanation, hereâ€™s how it actually looks in the syntax:</p>

<pre><code>INSERT INTO products (
    upc, 
    title, 
    description, 
    link) 
VALUES (
    123456789, 
    â€˜Figment #1 of 5â€™, 
    â€˜THE NEXT DISNEY ADVENTURE IS HERE - STARRING ONE OF DISNEY'S MOST POPULAR CHARACTERS! â€™, 
    â€˜http://www.amazon.com/dp/B00KGJVRNE?tag=mypred-20â€™
    )
ON CONFLICT DO UPDATE SET description=excluded.description;
</code></pre>

<p>Itâ€™s been a long time coming for this, and it makes building applications that need this kind of behavior even easier. While it would have been great for this to be available years ago, kudos to Postgres and its community for taking the approach that is safe for your data. The result we have now both provides the desired behavior of create or update, <strong>and</strong> is performant without the risk of race conditions for your data.</p>
]]></content>
  </entry>
  
</feed>
