<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Postgres, | Craig Kerstiens]]></title>
  <link href="http://www.craigkerstiens.com/categories/postgres/atom.xml" rel="self"/>
  <link href="http://www.craigkerstiens.com/"/>
  <updated>2017-10-12T09:50:57-07:00</updated>
  <id>http://www.craigkerstiens.com/</id>
  <author>
    <name><![CDATA[Craig Kerstiens]]></name>
    <email><![CDATA[craig.kerstiens@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Dear Postgres]]></title>
    <link href="http://www.craigkerstiens.com/2017/10/12/dear-postgres/"/>
    <updated>2017-10-12T08:00:00-07:00</updated>
    <id>http://www.craigkerstiens.com/2017/10/12/dear-postgres</id>
    <content type="html"><![CDATA[<p><img src="https://d3vv6lp55qjaqc.cloudfront.net/items/242D1y2p2g0g281S0O3W/dear_postgres.png?X-CloudApp-Visitor-Id=e4475d145dcf11ebcffabf840edcc11f&v=977a17bd" style="border:0px;"/></p>

<p>Dear Postgres,</p>

<p>I&rsquo;ve always felt an affinity for you in my 9 years of working with you. I know others have known you longer, but that doesn&rsquo;t mean they love you more. Years ago when others complained about your rigidness or that you weren&rsquo;t as accommodating as others I found solace in your steadfast values:</p>

<ol>
<li>Don&rsquo;t lose data</li>
<li>Adhere to standards</li>
<li>Move forward with a balancing act between new fads of the day while still continuously improving</li>
</ol>


<p>You&rsquo;ve been there and seen it all. Years ago you were being disrupted by XML databases. As companies made heavy investment into what such a document database would do for their organization you proceeded to &ldquo;simply&rdquo; add a datatype that accomplished the same and brought your years of progress along with it.</p>

<p>In the early years you had the standard format of index <em>b-tree</em> that most database engines leveraged. Then quietly but confidently you started adding more. Then came K-nearest neighbor, generalized inverted indexes (GIN), and generalized search-tree (GiST), only to be followed by space partitioned GiST and block range indexes (BRIN). Now the only question is which do I use?</p>

<p><img src="https://d3vv6lp55qjaqc.cloudfront.net/items/2I43101m250a363W0s0m/Image%202017-10-12%20at%209.11.57%20AM.gif?X-CloudApp-Visitor-Id=e4475d145dcf11ebcffabf840edcc11f&v=100f9c67" style="width:50%" /></p>

<p>All the while there was this other camp using for something that felt cool but outside my world: GIS. GIS, geographical information systems, I thought was something only civil engineers used. Then GPS came along, then the iPhone and location based devices came along and suddenly I wanted to find out the nearest path to my Peets, or manage geographical region for my grocery delivery service. PostGIS had been there all along building up this powerful feature set, sadly to this day I still mostly marvel from the sideline at this whole other feature set I long to take advantage of&hellip; <em>one day&hellip; one day</em>.</p>

<p>A little over 5 years ago I fell in love with your fastly improving analytical capabilities. No you weren&rsquo;t an MPP system yet, but here came window functions and CTEs, then I almost understood recursive CTEs <em>(still working on that one)</em>. I can iterate over data in a recursive fashion without PL/PgSQL? Yes please! I only want to use it more.</p>

<p>And then five years ago, document stores start taking over the world. I feel like I&rsquo;ve seen this story before, wasn&rsquo;t XML going to change the internet? Enter JSON, the JSON datatype, and JSONB. Wow, this is really nice to mix relational, document storage, join against things. I suddenly don&rsquo;t get why more don&rsquo;t take this flexible approach to building on a good foundation and layering on the refinements.</p>

<p>Extensions! Where have you been all my life? There’s <a href="https://www.citusdata.com">Citus</a>, and <a href="https://github.com/aggregateknowledge/postgresql-hll">HyperLogLog</a>, and <a href="https://www.zombodb.com/">ZomboDB</a>, with each I can add functionality to Postgres without it being limited to the standard release, they can be in C or not. Wait, all along so much has been built on this foundation? PostGIS, full-text search, hstore? I like all those things, why didn&rsquo;t you tell me all along about this foundation? Postgres, I like what I&rsquo;m seeing how you&rsquo;re allowing others to do more without having it be in the core of Postgres. This extension stuff is really kinda cool that it&rsquo;s Postgres and then some, kinda like C and then ++, wait nevermind scratch that analogy.</p>

<p>Sorry, I&rsquo;ve rambled a bit. You&rsquo;re a little over twenty years old now. I&rsquo;ve known you for nearly ten of those years so I know there&rsquo;s so much about your background I don&rsquo;t know, I hope we get to spend the time together to share it all. This 10 release is really an exciting one to me. We&rsquo;ve spent all this time together and I feel like each passing year the bond grows fonder.</p>

<p>Now you&rsquo;ve brought me better parallelism so I can further utilize my system resources. I now have partitioning. Thank you! I don&rsquo;t have to roll my own hacks to help age out old data for my time series database. Logical replication will make so many other things possible, such as more online upgrades and integration with other systems.</p>

<p>Postgres, I just want to say thank you for the past ten years together. Thank you for all you’ve done and for all you’ll continue to do in the future.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Tracking and managing your Postgres connections]]></title>
    <link href="http://www.craigkerstiens.com/2017/09/18/postgres-connection-management/"/>
    <updated>2017-09-18T16:00:00-07:00</updated>
    <id>http://www.craigkerstiens.com/2017/09/18/postgres-connection-management</id>
    <content type="html"><![CDATA[<p>Managing connections in Postgres is a topic that seems to come up several times a week in conversations. I&rsquo;ve written some about scaling your connections and the right approach when you truly need a high level of connections, which is to use a connection pooler like pgBouncer. But what do you do before that point and how can you better track what is going on with your connections in Postgres? <!--more--></p>

<p>Postgres under the covers has a lot of metadata about both historical and current activity against a system. Within Postgres you can run the following query which will give you a few results:</p>

<p>```sql
SELECT count(*),</p>

<pre><code>   state 
</code></pre>

<p>FROM pg_stat_activity
GROUP BY 2;
 count |             state
&mdash;&mdash;&mdash;&ndash;+&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;</p>

<pre><code> 7 | active
69 | idle
26 | idle in transaction
11 | idle in transaction (aborted)
</code></pre>

<p>(4 rows)</p>

<p>Time: 30.337 ms
```</p>

<p>Each of these is useful in determining what you should do to better manage your connection count. All of these numbers can be useful to record every say 30 seconds and chart on your own internal monitoring. Lets break down each:</p>

<ul>
<li><strong>active</strong> &ndash; This is currently running queries, in a sense this is truly how many connections you may require at a time</li>
<li><strong>idle</strong> &ndash; This is where you have opened a connection to the DB (most frameworks do this and maintain a pool of them), but nothing is happening. This is the one area that a connection pooler like pgBouncer can most help.</li>
<li><strong>idle in transaction</strong> &ndash; This is where your app has run a <code>BEGIN</code> but it&rsquo;s now waiting somewhere in a transaction and not doing work.</li>
</ul>


<p>For <strong>idle</strong> as mentioned above it&rsquo;s one that you do want to monitor and if you see a high number here it&rsquo;s worth investing in setting up a pgBouncer.</p>

<p>For <strong>idle in transaction</strong> this one is a bit more interesting. Here what you likely want to do when first investigating is get an idea of how old those are. You can do this by querying pg_stat_activity and filtering for where the state is <code>idle in transaction</code> and checking how old those queries are. For ones that have been running too long you may want to manually kill them.</p>

<p>If you find that you have some stale transactions hanging around this could be for days, hours, or even just a few minutes you may want to set a default to kill those transactions.</p>

<p>To help with this Postgres has a nice feature of a <code>statement_timeout</code>. A statement timeout will automatically kill queries that run longer than the allotted time. You can set this at both a global level and for a specific session. To do this at the database level you&rsquo;d run this with an <code>alter database dbnamehere set statement_timeout = 60000;</code> which is 60 seconds. To do so during a given session simply run <code>set statment_timeout = 6000000;</code>.</p>

<p>For <em>idle in transaction</em> that have been running too long there is its own setting setting that you can set in a similar fashion <code>idle_in_transaction_session_timeout</code> (on Postgres 9.6 and up). Setting both <code>statement_timeout</code> and <code>idle_in_transaction_session_timeout</code> will help with cancelling long running queries and transactions.</p>

<p>Keeping your connection limits in check should lead to a much healthier performing database and thus app.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Better database migrations in Postgres]]></title>
    <link href="http://www.craigkerstiens.com/2017/09/10/better-postgres-migrations/"/>
    <updated>2017-09-10T00:00:00-07:00</updated>
    <id>http://www.craigkerstiens.com/2017/09/10/better-postgres-migrations</id>
    <content type="html"><![CDATA[<p>As your database grows and scales there are some operations that you need to take more care of than you did when you were just starting. When working with your application in your dev environment you may not be fully aware of the cost of some operations until you run them against production. And at some point most of us have been guilty of it, running some migration that starts at 5 minutes, then 15 minutes in it&rsquo;s still running, and suddenly production traffic is impacted.</p>

<p>There are two operations that tend to happen quite frequently, each with some straightforward approaches to mitigate having any noticable amount of downtime. Let&rsquo;s look at each of the operations, how they work and then how you can approach them in a safer way. <!--more--></p>

<h3>Adding new columns</h3>

<p>Adding a new column is actually quite cheap in Postgres. When you do this it updates its underlying tracking of the columns that exist–which is almost instant. The part that becomes expensive is when you have some constraint against the column. A constraint could be a primary or foreign key, or some uniqueness constraint. Here Postgres has to scan through all the records in the table to ensure that it&rsquo;s not being violated. Adding some constraint such as <code>not null</code> does happen some, but is not the most common cause.</p>

<p>The most common reason for slowness of adding a new column is that most frameworks make it very simple for you to set a default value for the new column. It&rsquo;s one thing to do this for all new records, but when you do this when an existing table it means the database has to read all the records and re-write them with the new default value attached. This isn&rsquo;t so bad for a table with a few hundred records, but for a few hundred million run it then go get yourself coffee, or lunch, or a 5 course meal because you&rsquo;ll be waiting for a while.</p>

<p>In short, <code>not null</code> and setting a default value (on creation) of your new column will cause you pain. The solution is to not do those things. But, what if you want to have a default value and don&rsquo;t want to allow <code>nulls</code>. There&rsquo;s a few simple steps you can take, by essentially splitting your migration up from 1 step to 4 migrations:</p>

<ol>
<li>Add your new column <em>that allows nulls</em></li>
<li>Start writing your default value on all new records and updates</li>
<li>Gradually backfill the default value</li>
<li>Apply your constraint</li>
</ol>


<p>Yes, this is a little more work, but it doesn&rsquo;t impact production in nearly the same magnitude.</p>

<h3>Indexes</h3>

<p>Index creation like most DDL operations holds a lock while it&rsquo;s occurring, this means any new data has to wait for the index to be created and then the new writes flow through. Again when firsting creating the table or on a small table this time is not very noticable. On a large database though, you can again wait minutes to possibly even hours. <em>It&rsquo;s a bit ironic when you think about it that adding an index to speed things up can slow things down while it&rsquo;s happening.</em></p>

<p>Postgres of course has the answer for this with <code>CONCURRENT</code> index creation. What this does is gradually build up the index in the background. You can create your index concurrently with: <code>CREATE INDEX CONCURRENTLY</code>. As soon as the index is created and available as long as you did what you were hoping to Postgres will swap over to using it on queries.</p>

<h3>A tool to help</h3>

<p>It&rsquo;s a good practice to understand what is happening when you run a migration and its performance impact. That said you don&rsquo;t have to manage this all on your own. At least for Rails there&rsquo;s a tool to help enforce more of these as you&rsquo;re developing to catch it earlier. <a href="https://github.com/ankane/strong_migrations">Strong migrations</a> aims to catch many of these expensive operations for you to have your back, if you&rsquo;re on Rails consider giving it a look.</p>

<p>Have other tools or tips that can help with database migrations in Postgres? <a href="https://www.twitter.com/craigkerstiens">Drop me a note</a> and I&rsquo;ll work to add them to the list.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Postgres backups: Logical vs. Physical an overview]]></title>
    <link href="http://www.craigkerstiens.com/2017/09/03/postgres-backups-physical-vs-logical/"/>
    <updated>2017-09-03T00:00:00-07:00</updated>
    <id>http://www.craigkerstiens.com/2017/09/03/postgres-backups-physical-vs-logical</id>
    <content type="html"><![CDATA[<p>It&rsquo;s not a very disputed topic that you should backup your database, and further test your backups. What is a little less discussed, at least for Postgres, is the types of backups that exist. Within Postgres there are two forms of backups and understanding them is a useful foundation for anyone working with Postgres. The two backup types are</p>

<ol>
<li>Physical: which consist of the actual bytes on disk,</li>
<li>Logical: which is a more portable format.</li>
</ol>


<p>Let&rsquo;s dig into each a bit more so you can better assess which makes sense for you. <!--more--></p>

<h3>Logical backups</h3>

<p>Logical backups are the most well known type within Postgres. This is what you get when you run <code>pg_dump</code> against a database. There are a number of different formats you can get from <a href="http://postgresguide.com/utilities/backup-restore.html">logical backups</a> and Postgres does a good job of making it easy to compress and configure this backup how you see fit.</p>

<p>When a logical backup is run against a database it is not throttled, this introduces a noticable load on your database.</p>

<p>As it&rsquo;s reading the data from disk and generating (in layman terms) a bunch of SQL <code>INSERT</code> statements, it has to actually see the data. It&rsquo;s of note that older Postgres databases (read: prior to 9.3) there were no checksums against your database. Checksums are just one tool for you to help check against data corruption. Because a logical dump has to actually read and generate the data to insert it will discover any corruption that exists for you.</p>

<p>This portable format is also very useful to pull down copies from production to different environments. I.e. if you need a copy of production data down on your local laptop <code>pg_dump</code> is the way to do it. Logical backups are also database specific, but then allow you to dump only certain tables.</p>

<p>All in all logical backups bring some good features, but come at two cost:</p>

<ul>
<li>Load on your system</li>
<li>The backup contains data as of the time when it ran</li>
</ul>


<h3>Physical backups</h3>

<p>Physical backups are another option when it comes to backing up your database. As we mentioned earlier it is the physical bytes on disk. To understand physical backups we need to know a bit more under the covers about how Postgres works.</p>

<p>Postgres, under the covers, is essentially one giant append only log. When you insert data it gets written to the log known as the write-ahead log (commonly called WAL). When you update data a new record gets written to the WAL. When you delete data a new record gets written to the WAL. Nearly all changes in Postgres including to indexes and otherwise cause an update to the WAL.</p>

<p>With physical backups what you require to be able to create a restore of your database is two things:</p>

<ol>
<li>A <code>base backup</code>, which is a copy of the bytes on disk as of that point and time</li>
<li>Additional segments of the WAL to put the database in some consistent state.</li>
</ol>


<p>A physical backup only requires a small amount of WAL to restore the database to some valid state, <em>but</em> this also gives you some new flexibility. With a base backup plus WAL you can start to replay transactions up to a specific point in time. This is often how point-in-time recovery is performed within Postgres. If you accidentally drop a table, yes&hellip; it happens, you can:</p>

<ol>
<li>Find a base backup before you dropped the table</li>
<li>Restore that base backup</li>
<li>Replay wal segments up to roughly that time just before you dropped the table.</li>
</ol>


<p><em>If you&rsquo;re considering setting up physical backups, consider using a tool like <a href="https://www.citusdata.com/blog/2017/08/18/introducing-wal-g-faster-restores-for-postgres/">WAL-G</a> to help.</em></p>

<h3>Logical vs. Physical which to choose</h3>

<p>Both are useful and provide different benefits. At smaller scale, say under 100 GB of data logical backups via <code>pg_dump</code> are something you should absolutely be doing. Because backups happen quickly on smaller databases you may be able to get out without functionality like point-in-time recovery. At larger scale, as you approach 1 TB physical backups start to become your only option. Because of the load introduced by logical backups and the time lapse between capturing them they become less suitable for production.</p>

<p>Hopefully this primer helps provide a high level overview of the two primary types of backups that exist as options for Postgres. Of course there is much deeper you can go on each, but consider ensuring you have at least one of the two if not both in place. Oh and make sure to test them, an un-tested backup isn&rsquo;t a backup at all.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Postgres Open Silicon Valley line-up: First take]]></title>
    <link href="http://www.craigkerstiens.com/2017/07/01/postgresopen-sv-line-up/"/>
    <updated>2017-07-01T00:00:00-07:00</updated>
    <id>http://www.craigkerstiens.com/2017/07/01/postgresopen-sv-line-up</id>
    <content type="html"><![CDATA[<p>This year Postgres open and PGConf SV have combined to great a bigger and better conference right in downtown San Francisco. <em>I&rsquo;m obviously biased as I&rsquo;m one of the co-chairs, and I know every conference organizer says picking the talks was hard, but I&rsquo;m especially excited for the line-up this year</em>. The hard part for me is going to be which talks do I miss out on because I&rsquo;m sitting in the other session that&rsquo;s ongoing. You can see the full list of <a href="https://postgresql.us/events/sessions/pgopen2017/">talk and tutorial sessions</a>, but I thought it&rsquo;d be fun to do a rundown of some of my favorites. <!--more--></p>

<h3><a href="https://postgresql.us/events/sessions/pgopen2017/session/399-how-postgres-could-index-itself/">How Postgres could index itself</a></h3>

<p><a href="https://postgresql.us/events/sessions/pgopen2017/session/399-how-postgres-could-index-itself/">Postgres indexing itself</a> has long been on my wishlist. <a href="https://www.twitter.com/akane">Andrew Kane</a> from Instacart, and creator of <a href="https://github.com/ankane/pghero/">PgHero</a> has bottled up many learnings into a new tool: <a href="https://medium.com/@ankane/introducing-dexter-the-automatic-indexer-for-postgres-5f8fa8b28f27">Dexter</a>. I suspect we&rsquo;ll get a look at all that went into this, how it works, and how you can leverage it to have a more automatically tuned database.</p>

<h3><a href="https://postgresql.us/events/sessions/pgopen2017/session/376-scaling-a-saas-application-beyond-a-single-postgres-with-citus/">Scaling a SaaS Application Beyond a Single Postgres with Citus</a></h3>

<p>Migration talks are all to common, from Postgres to MySQL from MySQL to Postgres, or from <a href="https://containership.engineering/dynamodb-to-postgres-why-and-how-aa891681af4d">Dynamo to Postgres</a>. But this one is a little different flavor from Postgres to sharded Postgres with <a href="https://www.citusdata.com">Citus</a>. Sharding into a distributed system of course brings new things to consider and think about, and <a href="https://postgresql.us/events/sessions/pgopen2017/session/376-scaling-a-saas-application-beyond-a-single-postgres-with-citus/">here you&rsquo;ll learn about them</a> from first hand experience so hopefully you can avoid mistakes yourself.</p>

<h3><a href="https://postgresql.us/events/sessions/pgopen2017/session/374-concurrency-deep-dive/">Concurrency Deep Dive</a></h3>

<p><a href="https://postgresql.us/events/sessions/pgopen2017/session/374-concurrency-deep-dive/">This one</a> looks to be a great under the hood look as well as likely very practical. It&rsquo;ll cover MVCC which is really at so much of the core of how Postgres works, but then bring it up to what it means for things like locks. Best of all, this one like so many others comes with lots of real world experience from Segment.</p>

<h3><a href="https://postgresql.us/events/sessions/pgopen2017/session/364-postgres-window-magic/">Postgres window magic</a></h3>

<p><img src="https://d3vv6lp55qjaqc.cloudfront.net/items/140p363n3b1N1X440Y0a/Image%202017-06-30%20at%2010.43.55%20AM.png?X-CloudApp-Visitor-Id=e4475d145dcf11ebcffabf840edcc11f&v=e35b6d4d" style="float:right; width:20%; margin-left:15px; " />
I love me some windows, though not always the easiest things to work with. They can let you easily do things like compute month over month growth between rows in a single query. Bruce whose always a great presenter <a href="https://postgresql.us/events/sessions/pgopen2017/session/364-postgres-window-magic/">walks us through them</a> and all the things they&rsquo;re capable of.</p>

<h3><a href="https://postgresql.us/events/sessions/pgopen2017/session/371-running-postgresql-instagram/">Running PostgreSQL @ Instagram</a></h3>

<p>Instagram is well known as one of the largest apps in the world. They optimized and changed their setup multiple times and probably scaled in about every way possible. <a href="https://postgresql.us/events/sessions/pgopen2017/session/371-running-postgresql-instagram/">Here we get to learn</a> about all the various things you need in running at a truly astonishing scale.</p>

<h3><a href="https://postgresql.us/events/sessions/pgopen2017/">Many many more</a></h3>

<p>Of course there&rsquo;s many more. Talks range from looks at new features, to how certain companies are using Postgres. We&rsquo;ve got companies like Instacart and Instagram as mentioned giving talks, to Postgres core committers. Whether you want to learn about the inner workings of Postgres (which often hurts my brain) to how you can simply speed up your app you should find something you like, as long as you like Postgres that is. Take a look at the <a href="https://postgresql.us/events/sessions/pgopen2017/">full list of sessions</a> and we hope to see you there.</p>
]]></content>
  </entry>
  
</feed>
