<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Postgres | Craig Kerstiens]]></title>
  <link href="http://www.craigkerstiens.com/categories/postgres/atom.xml" rel="self"/>
  <link href="http://www.craigkerstiens.com/"/>
  <updated>2015-11-30T08:53:21-08:00</updated>
  <id>http://www.craigkerstiens.com/</id>
  <author>
    <name><![CDATA[Craig Kerstiens]]></name>
    <email><![CDATA[craig.kerstiens@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Node, Postgres, MassiveJS - A better database experience]]></title>
    <link href="http://www.craigkerstiens.com/2015/11/30/massive-node-postgres-an-overview-and-intro/"/>
    <updated>2015-11-30T00:00:00-08:00</updated>
    <id>http://www.craigkerstiens.com/2015/11/30/massive-node-postgres-an-overview-and-intro</id>
    <content type="html"><![CDATA[<p>First some background–I&rsquo;ve always had a bit of a love hate relationship with ORMs. ORMs are great for basic crud applications, which inevitably happens at some point for an app. The main two problems I have with ORMs is:</p>

<ol>
<li>They treat all databases as equal (yes, this is a little overgeneralized but typically true). They claim to do this for database portability, but in reality an app still can&rsquo;t just up and move from one to another.</li>
<li>They don&rsquo;t handle complex queries well at all. As someone that sees SQL as a very powerful language, taking away all the power just leaves me with pain.</li>
</ol>


<p><em>Of course these aren&rsquo;t the <a href="https://kev.inburke.com/kevin/faster-correct-database-queries/">only issues</a> with them, just the two ones I personally run into over and over.</em></p>

<p>In some playing with Node I was optimistic to explore <a href="http://massive-js.readthedocs.org">Massive.JS</a> as it seems to buck the trend of just imitating all other ORMs. My initial results–it makes me want to do more with Node just for this library. After all the power of a language is the ecosystem of libraries around it, not just the core language. So let&rsquo;s take a quick tour through with a few highlights of what makes it really great.</p>

<!--more-->


<h3>Getting setup</h3>

<p>Without further adieu here&rsquo;s a quick tour around it.</p>

<p>First let&rsquo;s pull down the example database from <a href="http://postgresguide.com/setup/example.html">PostgresGuide</a></p>

<p>Then let&rsquo;s setup out Node app:</p>

<pre><code>$ npm init
$ npm install massive --save
</code></pre>

<h3>Connecting and querying</h3>

<p>Now let&rsquo;s try to connect and say query a user from within our database. Create the following as an <code>index.js</code> file, then run with <code>node index.js</code>:</p>

<pre><code>var massive = require("massive");
var connectionString = "postgres://:@localhost/example";

var db = massive.connectSync({connectionString : connectionString});

db.users.find(1, function(err,res){
  console.log(res);
});
</code></pre>

<p>Upon first run if you&rsquo;re like me and use the <a href="http://postgresguide.com/setup/example.html">PostgresGuide example database</a> (which I now need to go back and tidy up), you&rsquo;ll get the following:</p>

<pre><code>db.users.find(1, function(err,res){
        ^
TypeError: Cannot read property 'find' of undefined
</code></pre>

<p>I can&rsquo;t describe how awesome it is to see this. What&rsquo;s happening is when Massive loads up it&rsquo;s connecting to your database, checking what tables you have. In this case though because we don&rsquo;t have a proper primary key defined it doesn&rsquo;t load them. It could treat id as some magical field of course like Rails used to and ignore the need for an index, but instead it not only encourages a good database design but requires it.</p>

<p>So let&rsquo;s go back and create our index in our database:</p>

<pre><code>$ psql example
$ alter table users add primary key (id);
</code></pre>

<p>Alright now let&rsquo;s run our script again with <code>node index.js</code> and see what we have:</p>

<pre><code>{ id: 1,
  email: 'john.doe@gmail.com',
  created_at: Thu Sep 24 2015 03:42:52 GMT-0700 (PDT),
  deleted_at: null }
</code></pre>

<p>Perfect! Now we&rsquo;re all connected and it even queried our database for us. Now let&rsquo;s take a few more look at some of the operators.</p>

<h3>Running an arbitrary query</h3>

<p><code>db.run</code> will let me run any arbritrary SQL. An example such as <code>db.run("select 'hello'")</code> will produce [ { &lsquo;?column?&rsquo;: &lsquo;hello&rsquo; } ].</p>

<p>This makes it nice and easier for us to break out of the standard ORM model and just run SQL.</p>

<h3>Find for quick look ups</h3>

<p>Similar to so many other database tools <code>find</code> will offer you the most common quick look ups:</p>

<pre><code>$ db.users.find({email: 'jane.doe@gmail.com'}, function(err, res){console.log(res)});
$ db.users.find({'created_at &gt;': '2015-09-24'}, function(err, res){console.log(res)});
</code></pre>

<p>And of course there&rsquo;s a where operator for multiple conditions.</p>

<h3>Structuring queries in your application</h3>

<p>While in the next post I&rsquo;ll dig in deep to JSON, this is perhaps my favorite feature of Massive&hellip; It&rsquo;s design for pulling out queries into individudal SQL files. Simply create a <code>db</code> folder and put your SQL in there. Let&rsquo;s take the most basic example of our user email lookup and put it in <code>user_lookup.sql</code></p>

<pre><code>SELECT *
FROM users
WHERE email = $1
</code></pre>

<p>Now back in our application we can run this and pass in a parameter to it:</p>

<pre><code>db.user_lookup(['jane.doe@gmail.com'], function(err,res){
  console.log(res);
});
</code></pre>

<p>This separation of our queries from our code makes it easier to track them, view diffs, and even more so <a href="http://www.craigkerstiens.com/2012/11/17/how-i-write-sql/">create very readable SQL</a>.</p>

<h3>Up next</h3>

<p>So sure, you can connect to a database, you can query some things. There were a couple of small but more novel things that we blew through in here. First is the fact I didn&rsquo;t have to define all my schema, it just knew it as <a href="http://www.craigkerstiens.com/2014/01/24/rethinking-limits-on-relational/">it really should</a>. The separation of SQL queries you&rsquo;ll custom write into files is simple, but will make for much more maintainable applications over the long term. And best of all is the JSON support, which I&rsquo;ll get to soon&hellip;</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Moving past averages in SQL (Postgres) – Percentiles]]></title>
    <link href="http://www.craigkerstiens.com/2015/06/07/Moving-past-averages-in-sql/"/>
    <updated>2015-06-07T00:00:00-07:00</updated>
    <id>http://www.craigkerstiens.com/2015/06/07/Moving-past-averages-in-sql</id>
    <content type="html"><![CDATA[<p>Often when you&rsquo;re tracking a metric for the first time you take a look at your average. For example what is your ARPU &ndash; Average Revenue Per User. In theory this tells you if you can acquire new user how much you&rsquo;ll make off that user. Or maybe what&rsquo;s your average life time value of a customer. Yet, many that are more familiar looking and extracting meaning from data median or a few different looks at <a href="http://apmblog.dynatrace.com/2012/11/14/why-averages-suck-and-percentiles-are-great/">percentiles can be much more meaningful</a>.</p>

<!--more-->


<p>And while you can very easily get the <code>AVG</code> in Postgres, with a small amount more effort you can report on percentiles as well. Window functions have been around for some time in Postgres. They allow you to order your result set over a certain group. The most basic example is if you want to order by date, but know which one falls at place 10 in order you can use a window function and project out the <code>rank()</code>.</p>

<p>Beyond outputting the rank yourself and doing extra manipulation Postgres has some great utilities to make the most common uses even easier. Being able to compute things such as the perc 95 directly on the data, or lay out for every record in the result where it falls within a percentile is hugely useful. Let&rsquo;s take a look:</p>

<p>Assuming you have a table called purchases, which has a total in it we could try:</p>

<pre><code>SELECT id,
       total,
       ntile(100) OVER (ORDER BY total) AS perc_rank
FROM purchases
</code></pre>

<p>This would give us something like:</p>

<pre><code>    id    |  total  | perc_rank
----------|---------|-----------
   264    |  12034  |    100
   643    |  11830  |    100
...
...
   304    |   751   |     95
</code></pre>

<p>What this would tell us is we have less than 5% of our purchases that have a total over 751. From here you can start to dig in and extract all sorts of different meanings, and by doing directly in SQL you&rsquo;re closer to the data and have one less processing step.</p>

<p>Percentiles get even more fun with the ordered set functions that came out in <a href="/2014/02/02/Examining-PostgreSQL-9.4/">Postgres 9.4</a>. They even allow you to project out hypothetical values in certain cases. For now I&rsquo;d encourage adding ntile to your toolbox anytime you&rsquo;re analyzing average or medians it will make your world a bit better, and then consider exploring further on the <a href="http://www.postgresql.org/docs/9.4/static/functions-aggregate.html#FUNCTIONS-ORDEREDSET-TABLE">ordered set functions</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Upsert lands in PostgreSQL 9.5 – A first look]]></title>
    <link href="http://www.craigkerstiens.com/2015/05/08/upsert-lands-in-postgres-9.5/"/>
    <updated>2015-05-08T00:00:00-07:00</updated>
    <id>http://www.craigkerstiens.com/2015/05/08/upsert-lands-in-postgres-9.5</id>
    <content type="html"><![CDATA[<p>If you’ve followed anything I’ve <a href="/2012/04/30/why-postgres/">written about Postgres</a>, you know that I’m a fan. At the same time you know that there’s been one feature that so many other databases have, which Postgres lacks and it <a href="/2014/08/15/my-postgres-wishlist-for-9.5/">causes a huge amount of angst for not being in Postgres</a>… Upsert. Well the day has come, it’s finally committed and will be available <a href="http://git.postgresql.org/gitweb/?p=postgresql.git;a=commit;h=168d5805e4c08bed7b95d351bf097cff7c07dd65">in Postgres 9.5</a>.</p>

<p>Sure we’re still several months away from Postgres 9.5 being released, anywhere from 3-6 months as a best guess. That doesn’t mean we can’t take a first look at this feature. Though before we get into it a few special call outs of thanks to Peter Geoghegan of the <a href="http://www.heroku.com/postgres">Heroku Postgres</a> team for being the primary author on it, Andres Freund who recently just joined <a href="https://www.citusdata.com">Citus Data</a> for his heavy contributions, and Heikki Linnakangas as well for his contributions.</p>

<!-- more -->


<p>And now onto the exploration. Upsert is the common name, but if you’re unfamiliar upsert is essentially create or update – Create this new record, but if a conflict exists update it. Let’s take a practical example.</p>

<p>Assume you have a web scraper that imports product information into a table. Each product has a UPC code, title, description, and link. There’s a unique constraint on the UPC code. Now, if your web scraper tries to insert a new product, and a product with the same UPC already exists, you’d usually get an error. But you don’t want the query to fail, you’d want to update the existing product instead. Maybe with a new image, maybe a new description, whatever have you, but I don’t want it to blow up… I simply want to capture the new data and save it.</p>

<p><strong>So before</strong>: Insert a record… Exception this violates a unique constraint… Let your app figure out what to do. <em>protip: often applications would try to work around this, but you can run a chance of a race condition and duplicate records if there’s a conflict. TLDR; it’s not a perfect solution.</em></p>

<p><strong>Now</strong>: Insert a record… There’s a unique constraint violation… Okay, let’s just update all the new record’s fields <strong>inside a single transaction</strong></p>

<p>So enough explanation, here’s how it actually looks in the syntax:</p>

<pre><code>INSERT INTO products (
    upc, 
    title, 
    description, 
    link) 
VALUES (
    123456789, 
    ‘Figment #1 of 5’, 
    ‘THE NEXT DISNEY ADVENTURE IS HERE - STARRING ONE OF DISNEY'S MOST POPULAR CHARACTERS! ’, 
    ‘http://www.amazon.com/dp/B00KGJVRNE?tag=mypred-20’
    )
ON CONFLICT DO UPDATE SET description=excluded.description;
</code></pre>

<p>It’s been a long time coming for this, and it makes building applications that need this kind of behavior even easier. While it would have been great for this to be available years ago, kudos to Postgres and its community for taking the approach that is safe for your data. The result we have now both provides the desired behavior of create or update, <strong>and</strong> is performant without the risk of race conditions for your data.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[A simple guide for DB migrations]]></title>
    <link href="http://www.craigkerstiens.com/2014/10/01/a-simple-guide-for-db-migrations/"/>
    <updated>2014-10-01T00:00:00-07:00</updated>
    <id>http://www.craigkerstiens.com/2014/10/01/a-simple-guide-for-db-migrations</id>
    <content type="html"><![CDATA[<p>Most web applications will add/remove columns over time. This is extremely common early on and even mature applications will continue modifying their schemas with new columns. An all too common pitfall when adding new columns is setting a not null constraint in Postgres.</p>

<!--more-->


<h3>Not null constraints</h3>

<p>What happens when you have a not null constraint on a table is it will re-write the entire table. Under the cover Postgres is really just an append only log. So when you update or delete data it&rsquo;s really just writing new data. This means when you add a column with a new value it has to write a new record. If you do this requiring columns to not be null then you&rsquo;re re-writing your entire table.</p>

<p>Where this becomes problematic for larger applications is it will hold a lock preventing you from writing new data during this time.</p>

<h3>A better way</h3>

<p>Of course you may want to not allow nulls and you may want to set a default value, the problem simply comes when you try to do this all at once. The safest approach at least in terms of uptime for your table &ndash;> data &ndash;> application is to break apart these steps.</p>

<ol>
<li>Start by simply adding the column with allowing nulls but setting a default value</li>
<li>Run a background job that will go and retroactively update the new column to your default value</li>
<li>Add your not null constraint.</li>
</ol>


<p>Yes it&rsquo;s a few extra steps, but I can say from having walked through this with a number of developers and their apps it makes for a much smoother process for making changes to your apps.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[My wishlist for Postgres 9.5]]></title>
    <link href="http://www.craigkerstiens.com/2014/08/15/my-postgres-wishlist-for-9.5/"/>
    <updated>2014-08-15T00:00:00-07:00</updated>
    <id>http://www.craigkerstiens.com/2014/08/15/my-postgres-wishlist-for-9.5</id>
    <content type="html"><![CDATA[<p>As I followed along with the <a href="/2014/03/24/Postgres-9.4-Looking-up/">9.4 release</a> of Postgres I had a few posts of things that I was excited about, some things that missed, and a bit of a wrap-up. I thought this year (year in the sense of PG releases) I&rsquo;d jump the gun and lay out areas I&rsquo;d love to see addressed in PostgreSQL 9.5. And here it goes:</p>

<!--more-->


<h3>Upsert</h3>

<p>Merge/Upsert/Insert or Update whatever you want to call it this is still a huge wart that it doesn&rsquo;t exist. There&rsquo;s been a few implementations show up on mailing lists, and to the best of my understanding there&rsquo;s been debate on if it&rsquo;s performant enough or that some people would prefer another implementation or I don&rsquo;t know what other excuse. The short is this really needs to happen, until that time you can always <a href="http://stackoverflow.com/questions/1109061/insert-on-duplicate-update-in-postgresql/8702291#8702291">implement it with a CTE</a> which can have a race condition.</p>

<h3>Foreign Data Wrappers</h3>

<p>There&rsquo;s so much opportunity here, and this has easily been my <a href="/2013/08/05/a-look-at-FDWs/">favorite feature of the past 2-3 years in Postgres</a>. Really any improvement is good here, but a hit list of a few valuable things:</p>

<ul>
<li>Pushdown of conditions</li>
<li>Ability to accept a DSN to a utility function to create foreign user and tables.</li>
<li>Better security around creds of foreign tables</li>
<li>More out of the box FDWs</li>
</ul>


<h3>Stats/Analytics</h3>

<p>Today there&rsquo;s <a href="http://madlib.net/">madlib</a> for machine learning, and 9.4 got support for <a href="http://www.depesz.com/2014/01/11/waiting-for-9-4-support-ordered-set-within-group-aggregates/">ordered set aggregates</a>, but even still Postgres needs to keep moving forward here. PL-R and PL-Python can help a good bit as well, but having more out of the <a href="http://www.postgresql.org/docs/9.3/static/functions-aggregate.html">box functions</a> for stats can continue to keep it at the front of the pack for a database that&rsquo;s not only safe for your data, but powerful to do analysis with.</p>

<h3>Multi-master</h3>

<p>This is definitely more of a dream than not. Full multi-master replication would be amazing, and it&rsquo;s getting closer to possible. The sad truth is even once it lands it will probably require a year of maturing, so even more reason for it to hopefully hit in 9.5</p>

<h3>Logical Replication</h3>

<p>The foundation made it in for 9.4 which is huge. This means we&rsquo;ll probably see a good working out of the box logical replication in 9.5. For those less familiar this means the replication is SQL based vs. the binary WAL stream. This means things like using replication to upgrade across versions is possible. So not quite 0 downtime, but ~ a minute or two to upgrade versions. Even of large DBs.</p>

<h3>An official GUI</h3>

<p>Alright this one is probably a pipe dream. And to kick it off, no pgAdmin doesn&rsquo;t cut it. A good end user tool for connecting/querying would be huge. Fortunately the ecosystem is improving here with <a href="http://www.jackdb.com">JackDB</a> (web based) and <a href="https://eggerapps.at/pgcommander/">PG Commander</a> (mac app), but these still aren&rsquo;t discoverable enough for most users.</p>

<h3>What do you want?</h3>

<p>So there&rsquo;s my wishlist, what&rsquo;s yours for 9.5? Let me know &ndash; <a href="http://www.twitter.com/craigkerstiens">@craigkerstiens</a>.</p>
]]></content>
  </entry>
  
</feed>
