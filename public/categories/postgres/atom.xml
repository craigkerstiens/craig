<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Postgres | Craig Kerstiens]]></title>
  <link href="http://www.craigkerstiens.com/categories/postgres/atom.xml" rel="self"/>
  <link href="http://www.craigkerstiens.com/"/>
  <updated>2016-06-07T10:27:45-07:00</updated>
  <id>http://www.craigkerstiens.com/</id>
  <author>
    <name><![CDATA[Craig Kerstiens]]></name>
    <email><![CDATA[craig.kerstiens@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Five mistakes beginners make when working with databases]]></title>
    <link href="http://www.craigkerstiens.com/2016/06/07/five-mistakes-databases/"/>
    <updated>2016-06-07T00:00:00-07:00</updated>
    <id>http://www.craigkerstiens.com/2016/06/07/five-mistakes-databases</id>
    <content type="html"><![CDATA[<p>When you start out as a developer there&rsquo;s an overwhelming amount of things to grasp. First there&rsquo;s the language itself, then all the quirks of the specific framework you&rsquo;re using,and after that (or maybe before) we&rsquo;ll throw front-end development into the mix, and somewhere along the line you have to decide to store your data somewhere.</p>

<p>Early on, with so many things to quickly master, the database tends to be an after-though in application design (perhaps because it doesn&rsquo;t make an impact to end user experience). As a result there&rsquo;s a number of bad practices that tend to get picked up when working with databases, here&rsquo;s a rundown of just a few.</p>

<!--more-->


<h3>1. Storing images</h3>

<p>Images don&rsquo;t belong in your database. Just because you can do something, it doesn&rsquo;t mean you should.Images take up a massive amount of space in databases, and slow applications down by unnecessarily eating your database&rsquo;s IO resources. The most common way this mistake occurs is when new developers base64 encode an image and store it in a database large text/blob field.</p>

<p>The better approach is to upload your images directly to a service like Amazon S3, then store the image URL (hosted by Amazon) in your database as a text field. This way, each time you need to load an image, you need to simply output the image URL into a valid <img> tag. This will greatly improve website responsiveness, and generally help scale web applications.</p>

<h3>2. Limit/Offset</h3>

<p>Pagination is extremely common in a number of applications.As soon as you start to learn SQL, the most straight-forward way to handle pagination is to <code>ORDER BY</code> some column then <code>LIMIT</code> the number of results returned, and for each extra page you&rsquo;ll <code>OFFSET</code> by so many records. This all seems entirely logical, until you realize at any moderate scale:</p>

<ol>
<li>The load this exerts on your database will be painful.</li>
<li>It isn&rsquo;t deterministic, should records change as the user flips between pages.</li>
</ol>


<p>The unfortunate part is: pagination is quite complex, and there isn&rsquo;t a one-size-fits-all solution. For more information on solving pagination problems, you can check <a href="https://www.citusdata.com/blog/1872-joe-nelson/409-five-ways-paginate-postgres-basic-exotic">out numerous options</a></p>

<h3>3. Integer primary keys</h3>

<p>The default for almost all ORMs when creating a primary key is to create a serial field. This is a sequence that auto-increments and then you use that number as your primary key. This seems straight forward as an admin, because you can browse from /users/1 to /users/2, etc. And for most applications this can often be fine. And for most applications, this is fine. But, you&rsquo;ll soon realize as you start to scale that integers primary keys can be exhausted, and are not ideal for large-scale systems. The better approach is to start <a href="https://til.hashrocket.com/posts/31a5135e19-generate-a-uuid-in-postgresql">taking advantage of UUIDs</a> from the start.</p>

<p><em>There&rsquo;s also the bonus advantage of not secretly showcasing how many users/listings/whatever the key references directly to users on accident.</em></p>

<h3>4. Default values on new columns</h3>

<p>No matter how long you&rsquo;ve been at it you won&rsquo;t get the perfect schema on day 1. It&rsquo;s better to think of database schemas as continuously evolving documents. Fortunately, it&rsquo;s easy to add a column to your database, but: it&rsquo;s also easy to do this in a horrific way. By default, if you just add a column it&rsquo;ll generally allow NULL values. This operation is fast, but most applications don&rsquo;t truly want null values in their data, instead they want to set the default value.</p>

<p>If you do add a column with a default value on the table, this will trigger a full re-write of your table. <em>Note: this is very bad for any sizable table on an application.</em> Instead, it&rsquo;s far better to allow null values at first so the operation is instant, then set your default, and then, with a background process go and retroactively update the data.</p>

<p>This is more complicated than it should be, but fortunately there are some <a href="http://pedro.herokuapp.com/past/2011/7/13/rails_migrations_with_no_downtime/">handy guides</a> to help.</p>

<h3>5. Over normalization</h3>

<p>As you start to learn about normalization it feels like the right thing to do. You create a <code>posts</code> table, which contains <code>authors</code>, and each post belongs in a category. So you create a <code>categories</code> table, and then you create a join table <code>post_categories</code>. At the real root of it there&rsquo;s not anything fundamentally wrong with normalizing your data, but at a certain point there are diminishing returns.</p>

<p>In the above case categories could very easily just be an array of varchar fields on a post. Normalization makes plenty of sense, but thinking through it a bit more every time you have a many to many table and wondering if you really need a full table on both sides is worth giving a second thought.</p>

<h3>Conclusion</h3>

<p>When I asked about this on twitter I got a pretty great responses, but they were all over the map. From the basics of never looking at the queries the ORM is generating, to much more advanced topics such as isolation levels. The one I didn&rsquo;t hit on that does seem to be a worthwhile one for anyone building a real world app is indexing. Knowing how <a href="http://www.craigkerstiens.com/2012/10/01/understanding-postgres-performance/">indexing works</a>, and understanding <a href="http://www.craigkerstiens.com/2013/05/30/a-collection-of-indexing-tips/">what indexes</a> you need to create is a critical part of getting good database performance. There&rsquo;s a number of posts on indexing that teach the basics, as well as <a href="http://www.craigkerstiens.com/2013/01/10/more-on-postgres-performance/">practical steps</a> for analyzing performance with Postgres.</p>

<p>In general, I encourage you to treat the database as another tool in your chest as opposed to a necessary evil, but hopefully, the above tips will at least prevent you from making some initial mistakes as you dig in as a beginner.</p>

<p><em>Special thanks to <a href="https://twitter.com/mdeggies">@mdeggies</a> and <a href="https://twitter.com/rdegges">@rdegges</a> for the initial conversation to spark the post at PyCon.</em></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hands On Postgres Sharding]]></title>
    <link href="http://www.craigkerstiens.com/2016/02/28/Hands-on-postgres-sharding/"/>
    <updated>2016-02-28T00:00:00-08:00</updated>
    <id>http://www.craigkerstiens.com/2016/02/28/Hands-on-postgres-sharding</id>
    <content type="html"><![CDATA[<p>Back in 2012 I wrote an overview of database sharding. Since then I&rsquo;ve had a few questions about it, which have really increased in frequency over the last two months. As a result I thought I&rsquo;d do a deeper dive with some actual hands on for sharding. Though for this hands on, because I do value my time I&rsquo;m going to take advantage of <code>pg_shard</code> rather than creating mechanisms from scratch.</p>

<p>For those unfamiliar <a href="https://github.com/citusdata/pg_shard/">pg_shard</a> is an open source extension from <a href="http://citusdata.com">Citus data</a> who has a commerical product that you can think of is pg_shard++ (and probably much more). Pg_shard adds a little extra to let data automatically distribute to other Postgres tables (logical shards) and Postgres databases/instances (physical shards) thus letting you outgrow a single Postgres node pretty simply.</p>

<p>Alright, enough talk about it, let&rsquo;s get things up and running.</p>

<!--more-->


<h3>Build, install</h3>

<p><em>The rest assume you have Postgres.app, version 9.5 setup and are on a Mac, much of these steps could be easily adapted for other Postgres installs or OSes.</em></p>

<p>PATH=/Applications/Postgres.app/Contents/Versions/latest/bin/:$PATH make</p>

<p>sudo PATH=/Applications/Postgres.app/Contents/Versions/latest/bin/:$PATH make install</p>

<p>cp /Applications/Postgres.app/Contents/Versions/9.5/share/postgresql/postgresql.conf.sample /Applications/Postgres.app/Contents/Versions/9.5/share/postgresql/postgresql.conf.sample</p>

<p>Edit your <code>postgresql.conf</code>:</p>

<pre><code>#shared_preload_libraries = ''
</code></pre>

<p>TO:</p>

<pre><code>shared_preload_libraries = 'pg_shard'
</code></pre>

<p>Then create a file in <code>/Users/craig/Library/Application\ Support/Postgres/var-9.5/pg_worker_list.conf</code> where <code>craig</code> is your username:</p>

<pre><code># hostname port-number
localhost  5432
localhost  5433
</code></pre>

<p>You&rsquo;ll also need to create a new Postgres instance:</p>

<pre><code>initdb -D /Users/craig/Library/Application\ Support/Postgres/var-9.5-2
</code></pre>

<p>Then edit that <code>postgresql.conf</code> inside that newly created folder with two main edits:</p>

<pre><code>port = 5432
</code></pre>

<p>To</p>

<pre><code>port = 5433
</code></pre>

<p>Finally setup our database then start it up:</p>

<pre><code>createdb instagram
postgres -D /Users/craig/Library/Application\ Support/Postgres/var-9.5-2
</code></pre>

<h3>Setup</h3>

<p>Now you should have two running instances of Postgres, now let&rsquo;s finally turn on the pg_shard extension, create some tables and see what we have. First connect to your main running Postgres instance, so in this case the the instagram database we first created <code>psql instagram</code>, then let&rsquo;s set things up:</p>

<pre><code>CREATE EXTENSION pg_shard;
CREATE TABLE customer_reviews (customer_id TEXT NOT NULL, review_date DATE, review_rating INTEGER, product_id CHAR(10));

 CREATE TABLE
 Time: 4.734 ms

SELECT master_create_distributed_table(table_name := 'customer_reviews',                                                                                                     partition_column := 'customer_id');

 master_create_distributed_table
 ---------------------------------

 (1 row)

SELECT master_create_worker_shards(table_name := 'customer_reviews',                                                                                                     shard_count := 16,                                                                                                                                        replication_factor := 2);

 master_create_worker_shards
 -----------------------------

 (1 row)
</code></pre>

<h3>Understanding and using</h3>

<p>So that was a lot of initial setup. But now we have an application that could in theory scale to a shared application across 16 instances. If you want a refresher, there&rsquo;s a difference between physical and logical shards. In this case above we have 16 logical ones and it&rsquo;s replicated across 2 physical Postgres instances albeit on the same instance.</p>

<p>Alright so a little more poking under the covers to see what happened before we actually start doing something with our data. If you&rsquo;re still connected go ahead and run <code>\d</code>, and you should see:</p>

<pre><code>                List of relations
 Schema |          Name          | Type  | Owner
--------+------------------------+-------+-------
 public | customer_reviews       | table | craig
 public | customer_reviews_10000 | table | craig
 public | customer_reviews_10001 | table | craig
 public | customer_reviews_10002 | table | craig
 public | customer_reviews_10003 | table | craig
 public | customer_reviews_10004 | table | craig
 public | customer_reviews_10005 | table | craig
 public | customer_reviews_10006 | table | craig
 public | customer_reviews_10007 | table | craig
 public | customer_reviews_10008 | table | craig
 public | customer_reviews_10009 | table | craig
 public | customer_reviews_10010 | table | craig
 public | customer_reviews_10011 | table | craig
 public | customer_reviews_10012 | table | craig
 public | customer_reviews_10013 | table | craig
 public | customer_reviews_10014 | table | craig
 public | customer_reviews_10015 | table | craig
(17 rows)
</code></pre>

<p>You can see that under the cover there&rsquo;s a lot more <code>customer_reviews</code> tables, in reality you don&rsquo;t have to think about these or do anything with them. But just for reference they&rsquo;re just plain ole Postgres tables under the cover. You can query them and poke at the data. The now mystical <code>customer_reviews</code> will actually roll up the data across all your logical shards (tables) and physical shards (spanning across machines).</p>

<p><em>It&rsquo;s also of note that in production you might not actually use your primary DB as a worker, we did this more for expediency in setting it up on a local Mac. More typically you&rsquo;d have 2 or more workers which are not the same a the primary, these were the ports we setup in our <code>pg_worker_list.conf</code>.</em>    A common setup would look something more like:</p>

<p><img src="https://s3.amazonaws.com/f.cl.ly/items/3T2N2Q1K041g0a0L0j03/Untitled.png?v=7df00f6b" alt="" /></p>

<p>So now start inserting away:</p>

<pre><code>INSERT INTO customer_reviews (customer_id, review_rating) VALUES ('HN802', 5);
INSERT INTO customer_reviews (customer_id, review_rating) VALUES ('FA2K1', 10);
</code></pre>

<p>For extra homework on your own you can now go and poke at where the underlying data actually surfaced.</p>

<h3>Conclusion</h3>

<p>Yes, there&rsquo;s a number of limitations that you can learn a bit more about over on the <a href="https://github.com/citusdata/pg_shard#limitations">github repo for pg_shard</a>. Though even with those it&rsquo;s very usable as is, and let&rsquo;s you get quite far in prepping an app for sharding. While I will say that all apps think they&rsquo;ll need sharding and few actually do, given <code>pg_shard</code> it&rsquo;s minimal extra effort now to plan for such scaling should you need it.</p>

<p>Up next we&rsquo;ll look at how it&rsquo;d work with a few languages, so you can get an idea of the end to end experience.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Writing more legible SQL]]></title>
    <link href="http://www.craigkerstiens.com/2016/01/08/writing-better-sql/"/>
    <updated>2016-01-08T00:00:00-08:00</updated>
    <id>http://www.craigkerstiens.com/2016/01/08/writing-better-sql</id>
    <content type="html"><![CDATA[<p>A number of times in a crowd I&rsquo;ve asked how many people enjoy writing SQL, and often there&rsquo;s a person or two. The follow up is how many people enjoy reading other people&rsquo;s SQL and that&rsquo;s unanimously 0. The reason for this is that so many people write bad SQL. It&rsquo;s not that it doesn&rsquo;t do the job, it&rsquo;s just that people don&rsquo;t tend to treat SQL the same as other languages and don&rsquo;t follow strong code formatting guidelines. So, of course here&rsquo;s some of my own recommendations on how to make SQL more readable.</p>

<!--more-->


<h3>One thing per line</h3>

<p>Only put a single column/table/join per line. This is going to make for slightly more verbose SQL, but it will be easier to read and edit.. Here&rsquo;s a basic example:</p>

<pre><code>SELECT foo,
       bar
FROM baz
</code></pre>

<h3>Align your projections and conditions</h3>

<p>You can somewhat see this in the above with <code>foo</code> and <code>bar</code> being on the same line. This is reasonably common for columns you&rsquo;re selecting, but it&rsquo;s not applied as often in <code>AND</code> or <code>GROUP BY</code> clauses. As you can see there is a difference though between:</p>

<pre><code>SELECT foo,
       bar
FROM baz
WHERE foo &gt; 3
AND baz = 'craig.kerstiens@gmail.com'
</code></pre>

<p>And a cleaner version:</p>

<pre><code>SELECT foo,
       bar
FROM baz
WHERE foo &gt; 3
  AND baz = 'craig.kerstiens@gmail.com'
</code></pre>

<h3>Use column names when grouping/ordering</h3>

<p>This is personally an awful habit of mine, but it is extremely convenient to just order by the column number. In the above query we could just <code>ORDER BY 1</code>. This is especially easy when column 1 may be something like SUM(foo). However, ensuring you explicitly <code>ORDER BY SUM(foo)</code> will help limit any misunderstanding of the data.</p>

<h3>Comments</h3>

<p>You comment your code all the time, yet so few seem to comment their queries. A simple <code>--</code> allows you to inline a comment, perhaps where there&rsquo;s some oddities to what you&rsquo;re joining or just anywhere it may need clarification. You can of course <a href="/2013/07/29/documenting-your-postgres-database/">go much further</a>, but at least some basic level of commenting should be required.</p>

<h3>Casing</h3>

<p>As highlighted in these examples, having a standard for how you case your queries is especially handy. Sticking with all SQL keywords in caps allows you to easily parse what is SQL and what are columns or literals that you&rsquo;re using in queries.</p>

<h3>CTEs</h3>

<p>First, yes they can be an optimisation boundary. But they can also make your query much more read-able and prevent you from doing the wrong thing because you couldn&rsquo;t reason about a query.</p>

<p>For those unfamiliar CTEs are like a view that exist just for the duration of that query being executed. You can have them reference previous CTEs so you can gradually build on them, much like you would code blocks. I won&rsquo;t repeat too much of what <a href="/2013/11/18/best-postgres-feature-youre-not-using/">I&rsquo;ve already written about them</a>, but if you&rsquo;re unfamiliar with them or not using them <a href="/2013/11/18/best-postgres-feature-youre-not-using/">they are a must</a>. CTEs are easily one of the few pieces of SQL that I use on a daily basis.</p>

<h3>Conclusion</h3>

<p>Of course this isn&rsquo;t the only way to make your SQL more readable and this isn&rsquo;t an exhaustive list. But hopefully you find these tips helpful, and for your favorite tip that I missed&hellip; let me know about it <a href="http://www.twitter.com/craigkerstiens">@craigkerstiens</a>.</p>

<p><em>A special thanks to <a href="http://www.twitter.com/Case">@Case</a> for reviewing.</em></p>

<script type="text/javascript">
  (function() {
    window._pa = window._pa || {};
    var pa = document.createElement('script'); pa.type = 'text/javascript'; pa.async = true;
    pa.src = ('https:' == document.location.protocol ? 'https:' : 'http:') + "//tag.marinsm.com/serve/517fd07cf1409000020002dc.js";
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(pa, s);
  })();
</script>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[My top 10 Postgres features and tips for 2016]]></title>
    <link href="http://www.craigkerstiens.com/2015/12/29/my-postgres-top-10-for-2016/"/>
    <updated>2015-12-29T00:00:00-08:00</updated>
    <id>http://www.craigkerstiens.com/2015/12/29/my-postgres-top-10-for-2016</id>
    <content type="html"><![CDATA[<p>I find during the holiday season many pick up <a href="http://www.amazon.com/Hard-Thing-About-Things-Building/dp/0062273205/ref=sr_1_1?ie=UTF8&amp;qid=1451407536&amp;sr=8-1&amp;keywords=hard+thing+about&amp;tag=mypred-20">new books</a>, learn a <a href="http://crystal-lang.org/">new language</a>, or brush up on some other skill in general. Here&rsquo;s my contribution to hopefully giving you a few new things to learn about Postgres and ideally utilize in the new year. It&rsquo;s not in a top 10 list as much as 10 tips and tricks you should be aware of as when you need them they become incredibly handy. But, first a shameless plug if you find any of the following helpful, consider subscribing to <a href="http://www.postgresweekly.com">Postgres weekly</a> a weekly newsletter with interesting Postgres content.</p>

<!--more-->


<h3>1. CTEs &ndash; Common Table Expressions</h3>

<p>CTEs allow you to do crazy awesome things like recursive queries but even the most simple form of them I don&rsquo;t go a day without using. Think of a CTE or commonly known as with clause as a view inside the time that query is running. This lets you more easily create readable query. Any query that&rsquo;s constructed that&rsquo;s even <a href="/2013/11/18/best-postgres-feature-youre-not-using/">100 lines long</a>, but with 4-5 CTEs is undoubtedly going to be easier for someone new to come in and understand than a 20 line query that does the same thing. A few people like writing SQL, but no one likes reading someone else&rsquo;s so do them a favor and read up on CTEs.</p>

<h3>2. Setup a .psqlrc</h3>

<p>You setup a bashrc, vimrc, etc. Why not do the same for Postgres. Some of the great things you can do:</p>

<ul>
<li>Setup pretty formatting by default with <code>\x auto</code></li>
<li>Set nulls to actually look like something <code>\pset null ¤</code></li>
<li>Turn timing on by default <code>\timing on</code></li>
<li>Customize your prompt <code>\set PROMPT1 '%[%033[33;1m%]%x%[%033[0m%]%[%033[1m%]%/%[%033[0m%]%R%# '</code></li>
<li>Save commonly run queries that you can run by name</li>
</ul>


<p>Here&rsquo;s an example of my own <code>psqlrc</code>:</p>

<pre><code>\set QUIET 1
\pset null '¤'

-- Customize prompts
\set PROMPT1 '%[%033[1m%][%/] # '
\set PROMPT2 '... # '

-- Show how long each query takes to execute
\timing

-- Use best available output format
\x auto
\set VERBOSITY verbose
\set HISTFILE ~/.psql_history- :DBNAME
\set HISTCONTROL ignoredups
\set COMP_KEYWORD_CASE upper
\unset QUIET
</code></pre>

<h3>3. pg_stat_statements for where to index</h3>

<p><code>pg_stat_statements</code> is probably the single most valuable tool for improving performance on your database. Once enabled (with <code>create extension pg_stat_statements</code>) it automatically records all queries run against your database and records often and how long they took. This allows you to then go and find areas you can optimize to get overall time back with one simple query:</p>

<pre><code>SELECT 
  (total_time / 1000 / 60) as total_minutes, 
  (total_time/calls) as average_time, 
  query 
FROM pg_stat_statements 
ORDER BY 1 DESC 
LIMIT 100;
</code></pre>

<p><em>Yes, there is some performance cost to leaving this always on, but it&rsquo;s pretty small. I&rsquo;ve found it&rsquo;s far more useful to be on and get major performance wins vs. the small cost of it always recording.</em></p>

<p>You can read much more on Postgres performance on a <a href="http://www.craigkerstiens.com/2013/01/10/more-on-postgres-performance/">previous post</a></p>

<h3>4. Slow down with ETL, use FDWs</h3>

<p>If you have a lot of <em>microservices</em> or different apps then you likely have a lot of different databases backing them. The default for about anything you want to do is do create some data warehouse and ETL it all together. This often goes a bit too far to the extreme of aggregating <strong>everything</strong> together.</p>

<p>For the times you just need to pull something together once or on rare occasion <a href="http://www.craigkerstiens.com/2013/08/05/a-look-at-FDWs/">foreign data wrappers</a> will let you query from one Postgres database to another, or potentially from Postgres to anything else such as <a href="https://github.com/citusdata/mongo_fdw">Mongo</a> or Redis.</p>

<h3>5. array and array_agg</h3>

<p>There&rsquo;s little chance if you&rsquo;re building an app you&rsquo;re not using arrays somewhere within it. There&rsquo;s no reason you shouldn&rsquo;t be doing the same within your database as well. Arrays can be just another datatype within Postgres and have some great use cases like tags for blog posts directly in a single column.</p>

<p>But, even if you&rsquo;re not using arrays as a datatype there&rsquo;s often a time when you want to rollup something like an array in a query then comma separate it. Something similar to the following could allow you to easily roll up a comma separated list of projects per user:</p>

<pre><code>SELECT 
  users.email,
  array_to_string(array_agg(projects.name), ',')) as projects
FROM
  projects,
  tasks,
  users
WHERE projects.id = tasks.project_id
  AND tasks.due_at &gt; tasks.completed_at
  AND tasks.due_at &gt; now()
  AND users.id = projects.user_id
GROUP BY 
  users.email
</code></pre>

<h3>6. Use materialized views cautiously</h3>

<p>If you&rsquo;re not familiar with materialized view they&rsquo;re a query that has been actually created as a table. So it&rsquo;s a materialized or basically snapshotted version of some query or &ldquo;view&rdquo;. In their initial version materialized versions, which were long requested in Postgres, were entirely unusuable because when you it was a locking transaction which could hold up other reads and acticities avainst that view.</p>

<p>They&rsquo;ve since gotten much better, but there&rsquo;s no tooling for refreshing them out of the box. This means you have to setup some scheduler job or cron job to regularly refresh your materialized views. If you&rsquo;re building some reporting or BI app you may undoubtedly need them, but their usability could still be advanced so that Postgres knew how to more automatically refresh them.</p>

<p><em>If you&rsquo;re on Postgres 9.3, the above caveats about preventing reads still does exist</em></p>

<h3>7. Window functions</h3>

<p>Window functions are perhaps still one of the more complex things of SQL to understand. In short they let you order the results of a query, then compute something from one row to the next, something generally hard to do without procedural SQL. You can do some very basic things with them such as rank where <a href="http://postgresguide.com/sql/window.html">each result appears</a> ordered by some value, or something more complex like compute <a href="http://www.craigkerstiens.com/2014/02/26/Tracking-MoM-growth-in-SQL/">MoM growth directly in SQL</a>.</p>

<h3>8. A simpler method for pivot tables</h3>

<p>Table_func is often referenced as the way to compute a pivot table in Postgres. Sadly though it&rsquo;s pretty difficult to use, and the more basic method would be to just do it with raw SQL. This will get much better with <a href="http://www.craigkerstiens.com/2015/12/27/postgres-9-5-feature-rundown/">Postgres 9.5</a>, but until then something where you sum up each condition where it&rsquo;s true or false and then totals is much simpler to reason about:</p>

<pre><code>select date,
       sum(case when type = 'OSX' then val end) as osx,
       sum(case when type = 'Windows' then val end) as windows,
       sum(case when type = 'Linux' then val end) as linux
from daily_visits_per_os
group by date
order by date
limit 4;
</code></pre>

<p><em>Example query courtesy of <a href="http://www.twitter.com/tapoueh">Dimitri Fontaine</a> and <a href="http://tapoueh.org/blog/2013/07/04-Simple-case-for-pivoting">his blog</a>.</em></p>

<h3>9. PostGIS</h3>

<p>Sadly on this one I&rsquo;m far from an expert. PostGIS is arguably the best option of any GIS database options. The fact that you get all of the standard Postgres benefits with it makes it even more powerful–a great example of this is GiST indexes which came to Postgres in recent years and offers great performance gains for PostGIS.</p>

<p>If you&rsquo;re doing something with geospatial data and need something more than the easy to use <code>earth_distance</code> extension then crack open PostGIS.</p>

<h3>10. JSONB</h3>

<p>I almost debated leaving this one off the list, ever since Postgres 9.2 JSON has been at least one of the marquees in each Postgres release. JSON arrived with much hype, and JSONB fulfilled on the initial hype of Postgres starting to truly compete as a document database. JSONB only continues to become more powerful with <a href="http://www.craigkerstiens.com/2015/12/08/massive-json/">better libraries</a> for taking advantage of it, and it&rsquo;s <a href="https://wiki.postgresql.org/wiki/What's_new_in_PostgreSQL_9.5#JSONB-modifying_operators_and_functions">functions improving</a> with each release.</p>

<p>If you&rsquo;re doing anything with JSON or playing with another document database and ignoring JSONB you&rsquo;re missing out, of course don&rsquo;t forget the GIN and GiST indexes to really get the benefits of it.</p>

<h3>The year ahead</h3>

<p>Postgres 9.5/9.6 should continue to improve and bring many new features in the years ahead, what&rsquo;s your preference for something that doesn&rsquo;t exist yet but you do want to see land in Postgres. Let me know <a href="http://www.twitter.com/craigkerstiens">@craigkerstiens</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Postgres 9.5 - The feature rundown]]></title>
    <link href="http://www.craigkerstiens.com/2015/12/27/postgres-9-5-feature-rundown/"/>
    <updated>2015-12-27T00:00:00-08:00</updated>
    <id>http://www.craigkerstiens.com/2015/12/27/postgres-9-5-feature-rundown</id>
    <content type="html"><![CDATA[<p>The headline of Postgres 9.5 is undoubtedly: Insert&hellip; on conflict do nothing/update or more commonly known as Upsert or Merge. This removes one of the last remaining features which other databases had over Postgres. Sure we&rsquo;ll take a look at it, but first let&rsquo;s browse through some of the other features you can look forward to when Postgres 9.5 lands:</p>

<!--more-->


<h3>Grouping sets, cube, rollup</h3>

<p>Pivoting in Postgres has <a href="http://www.craigkerstiens.com/2013/06/27/Pivoting-in-Postgres/">sort of been possible</a> as has rolling up data, but it required you to know what those values and what you were projecting to, to be known. With the new functionality to allow you to group various sets together rollups as you&rsquo;d normally expect to do in something like Excel become trivial.</p>

<p>So now instead you simply add the grouping type just as you would on a normal group by:</p>

<pre><code>SELECT department, role, gender, count(*)
FROM employees
GROUP BY your_grouping_type_here;
</code></pre>

<p>By simply selecting the type of rollup you want to do Postgres will do the hard work for you. Let&rsquo;s take a look at the given example of department, role, gender:</p>

<ul>
<li><code>grouping sets</code> will project out the count for each specific key. As a result you&rsquo;d get each department key, with other keys as null, and the count for each that met that department.</li>
<li><code>cube</code> will give you the same values as above, but also the rollups of every individual combination. So in addition to the total for each department, you&rsquo;d get breakups by the department and gender, and department and role, and department and role and gender.</li>
<li><code>rollup</code> will give you a slightly similar version to cube but only give you the detailed groupings in the order they&rsquo;re presented. So if you specified <code>roll (department, role, gender)</code> you&rsquo;d have no rollup for department and gender alone.</li>
</ul>


<p><em>Check the what&rsquo;s new wiki for a bit more clarity on <a href="https://wiki.postgresql.org/wiki/What's_new_in_PostgreSQL_9.5#GROUPING_SETS.2C_CUBE_and_ROLLUP">examples and output</a></em></p>

<h3>Import foreign  schemas</h3>

<p>I only use foreign tables about once a month, but when I do use them they&rsquo;ve inevitably saved many hours of creating a one off ETL process. Even still the effort to setup new foreign tables has shown a bit of their infancy in Postgres. Now once you&rsquo;ve setup your foreign database, you can import the schema, either all of it or specific tables you prefer.</p>

<p>It&rsquo;s as simple as:</p>

<pre><code>IMPORT FOREIGN SCHEMA public
FROM SERVER some_other_db INTO reference_to_other_db;
</code></pre>

<h3>pg_rewind</h3>

<p>If you&rsquo;re managing your own Postgres instance for some reason and running HA, pg_rewind could become especially handy. Typically to spin up replication you have to first download the physical, also known as base, backup. Then you have to replay the Write-Ahead-Log or WAL–so it&rsquo;s up to date then you actually flip on replication.</p>

<p>Typically with databases when you fail over you shoot the other node in the head or <a href="https://en.wikipedia.org/wiki/STONITH">STONITH</a>. This means just get rid of it, completely throw it out. This is still a good practice, so bring it offline, make it inactive, but from there now you could then flip it into a mode and use pg_rewind. This could save you pulling down lots and lots of data to get a replica back up once you have failed over.</p>

<h3>Upsert</h3>

<p>Upsert of course will be the highlight of Postgres 9.5. I already talked about it some when <a href="http://www.craigkerstiens.com/2015/05/08/upsert-lands-in-postgres-9.5/">it initially landed</a>. The short of it is, if you&rsquo;re inserting a record and there&rsquo;s a conflict, you can choose to:</p>

<ul>
<li>Do nothing</li>
<li>Do some form of update</li>
</ul>


<p>Essentially this will let you have the typically experience of create or update that most frameworks provide but without a potential race condition of incorrect data.</p>

<h3>JSONB pretty</h3>

<p>There&rsquo;s a few updates <a href="https://wiki.postgresql.org/wiki/What's_new_in_PostgreSQL_9.5#JSONB-modifying_operators_and_functions">to JSONB</a>. The one I&rsquo;m most excited about is making JSONB output in psql read much more legibly.</p>

<p>If you&rsquo;ve got a JSONB field just give it a try with:</p>

<pre><code>SELECT jsonb_pretty(jsonb_column)
FROM foo;
</code></pre>

<h3>Give it a try</h3>

<p>Just in time for the new year <a href="http://www.postgresql.org/about/news/1631/">the RC is ready</a> and you can get hands on with it. Give it a try, and if there&rsquo;s more you&rsquo;d like to hear about Postgres please feel free to drop me a note <a href="mailto:craig.kerstiens@gmail.com">craig.kerstiens@gmail.com</a>.</p>

<script type="text/javascript">
  (function() {
    window._pa = window._pa || {};
    // _pa.orderId = "myOrderId"; // OPTIONAL: attach unique conversion identifier to conversions
    // _pa.revenue = "19.99"; // OPTIONAL: attach dynamic purchase values to conversions
    // _pa.productId = "myProductId"; // OPTIONAL: Include product ID for use with dynamic ads
    var pa = document.createElement('script'); pa.type = 'text/javascript'; pa.async = true;
    pa.src = ('https:' == document.location.protocol ? 'https:' : 'http:') + "//tag.marinsm.com/serve/517fd07cf1409000020002dc.js";
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(pa, s);
  })();
</script>

]]></content>
  </entry>
  
</feed>
