<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: PostgreSQL | Craig Kerstiens]]></title>
  <link href="http://www.craigkerstiens.com/categories/postgresql/atom.xml" rel="self"/>
  <link href="http://www.craigkerstiens.com/"/>
  <updated>2017-07-01T07:58:21-07:00</updated>
  <id>http://www.craigkerstiens.com/</id>
  <author>
    <name><![CDATA[Craig Kerstiens]]></name>
    <email><![CDATA[craig.kerstiens@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Postgres Open Silicon Valley line-up: First take]]></title>
    <link href="http://www.craigkerstiens.com/2017/07/01/postgresopen-sv-line-up/"/>
    <updated>2017-07-01T00:00:00-07:00</updated>
    <id>http://www.craigkerstiens.com/2017/07/01/postgresopen-sv-line-up</id>
    <content type="html"><![CDATA[<p>This year Postgres open and PGConf SV have combined to great a bigger and better conference right in downtown San Francisco. <em>I&rsquo;m obviously biased as I&rsquo;m one of the co-chairs, and I know every conference organizer says picking the talks was hard, but I&rsquo;m especially excited for the line-up this year</em>. The hard part for me is going to be which talks do I miss out on because I&rsquo;m sitting in the other session that&rsquo;s ongoing. You can see the full list of <a href="https://postgresql.us/events/sessions/pgopen2017/">talk and tutorial sessions</a>, but I thought it&rsquo;d be fun to do a rundown of some of my favorites. <!--more--></p>

<h3>How Postgres could index itself</h3>

<p><a href="https://postgresql.us/events/sessions/pgopen2017/session/399-how-postgres-could-index-itself/">Postgres indexing itself</a> has long been on my wishlist. <a href="https://www.twitter.com/akane">Andrew Kane</a> from Instacart, and creator of <a href="https://github.com/ankane/pghero/">PgHero</a> has bottled up many learnings into a new tool: <a href="https://medium.com/@ankane/introducing-dexter-the-automatic-indexer-for-postgres-5f8fa8b28f27">Dexter</a>. I suspect we&rsquo;ll get a look at all that went into this, how it works, and how you can leverage it to have a more automatically tuned database.</p>

<h3>Scaling a SaaS Application Beyond a Single Postgres with Citus</h3>

<p>Migration talks are all to common, from Postgres to MySQL from MySQL to Postgres, or from <a href="https://containership.engineering/dynamodb-to-postgres-why-and-how-aa891681af4d">Dynamo to Postgres</a>. But this one is a little different flavor from Postgres to sharded Postgres with <a href="https://www.citusdata.com">Citus</a>. Sharding into a distributed system of course brings new things to consider and think about, and <a href="https://postgresql.us/events/sessions/pgopen2017/session/376-scaling-a-saas-application-beyond-a-single-postgres-with-citus/">here you&rsquo;ll learn about them</a> from first hand experience so hopefully you can avoid mistakes yourself.</p>

<h3>Concurrency Deep Dive</h3>

<p><a href="https://postgresql.us/events/sessions/pgopen2017/session/374-concurrency-deep-dive/">This one</a> looks to be a great under the hood look as well as likely very practical. It&rsquo;ll cover MVCC which is really at so much of the core of how Postgres works, but then bring it up to what it means for things like locks. Best of all, this one like so many others comes with lots of real world experience from Segment.</p>

<h3>Postgres window magic</h3>

<p><img src="https://d3vv6lp55qjaqc.cloudfront.net/items/140p363n3b1N1X440Y0a/Image%202017-06-30%20at%2010.43.55%20AM.png?X-CloudApp-Visitor-Id=e4475d145dcf11ebcffabf840edcc11f&v=e35b6d4d" style="float:right; width:20%; margin-left:15px; " />
I love me some windows, though not always the easiest things to work with. They can let you easily do things like compute month over month growth between rows in a single query. Bruce whose always a great presenter <a href="https://postgresql.us/events/sessions/pgopen2017/session/364-postgres-window-magic/">walks us through them</a> and all the things they&rsquo;re capable of.</p>

<h3>Running PostgreSQL @ Instagram</h3>

<p>Instagram is well known as one of the largest apps in the world. They optimized and changed their setup multiple times and probably scaled in about every way possible. <a href="https://postgresql.us/events/sessions/pgopen2017/session/371-running-postgresql-instagram/">Here we get to learn</a> about all the various things you need in running at a truly astonishing scale.</p>

<h3>Many many more</h3>

<p>Of course there&rsquo;s many more. Talks range from looks at new features, to how certain companies are using Postgres. We&rsquo;ve got companies like Instacart and Instagram as mentioned giving talks, to Postgres core committers. Whether you want to learn about the inner workings of Postgres (which often hurts my brain) to how you can simply speed up your app you should find something you like, as long as you like Postgres that is. Take a look at the <a href="https://postgresql.us/events/sessions/pgopen2017/">full list of sessions</a> and we hope to see you there.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Working with time in Postgres]]></title>
    <link href="http://www.craigkerstiens.com/2017/06/08/working-with-time-in-postgres/"/>
    <updated>2017-06-08T00:00:00-07:00</updated>
    <id>http://www.craigkerstiens.com/2017/06/08/working-with-time-in-postgres</id>
    <content type="html"><![CDATA[<p>A massive amount of reporting queries, whether really intensive data analysis, or just basic insights into your business involving looking at data over a certain time period. Postgres has really rich support for dealing with time out of the box, something that&rsquo;s often very underweighted when dealing with a database. Sure, if you have a time-series database it&rsquo;s implied, but even then how flexible and friendly is it from a query perspective? With Postgres there&rsquo;s a lot of key items available to you, let&rsquo;s dig in at the things that make your life easier when querying. <!--more--></p>

<h3>Date math</h3>

<p>The most common thing I find myself doing is looking at users that have done something within some specific time window. If I&rsquo;m executing this all from my app I can easily inject specific dates, but Postgres makes this really easy for you. Within Postgres you have a type called an interval that is some window of time. And fortunately Postgres takes care of the heavy lifting of how might something translate to or from hours/seconds/milliseconds/etc. Here&rsquo;s just a few examples of things you could do with interals:</p>

<ul>
<li>&lsquo;1 day&rsquo;::interval</li>
<li>&lsquo;5 days&rsquo;::interval</li>
<li>&lsquo;1 week&rsquo;::interval</li>
<li>&lsquo;30 days&rsquo;::interval</li>
<li>&lsquo;1 month&rsquo;::interval</li>
</ul>


<p><em>A note that if you&rsquo;re looking to remove something like a full month, you actually want to use 1 month instead of trying to calculate yourself.</em></p>

<p>With a given interval you can easily shift some window of time, such as finding all users that have signed up for your service within the past week:</p>

<p><code>sql
SELECT *
FROM users
WHERE created_at &gt;= now() - '1 week'::interval
</code></p>

<h3>Date functions</h3>

<p>Date math makes it pretty easy for you to go and find some specific set of data that applies, but what do you do when you want a broader report around time? There&rsquo;s a few options here. One is to leverage the built-in Postgres functions that help you work with dates and times. <code>date_trunc</code> is one of the most used ones that will truncate a date down to some interval level. Here you can use the same general values as the above, but simply pass in the type of interval it will be. So if we wanted to find the count of users that signed up per week:</p>

<p>```
SELECT date_trunc(&lsquo;week&rsquo;, created_at),</p>

<pre><code>   count(*)
</code></pre>

<p>FROM users
GROUP BY 1
ORDER BY 1 DESC;
```</p>

<p>This gives us a nice roll-up of how many users signed up each week. What&rsquo;s missing here though is if you have a week that has no users. In that case because no users signed up there is no count of 0, it just simply doesn&rsquo;t exist. If you did want something like this you could generate some range of time and then do a cross join with it against users to see which week they fell into. To do this first you&rsquo;d generate a series of dates:</p>

<p><code>sql
SELECT generate_series('2017-01-01'::date, now()::date, '1 week'::interval) weeks
</code></p>

<p>Then we&rsquo;re going to join this against the actual users table and check that the <code>created_at</code> falls within the right range.</p>

<p>```
with weeks as (
  select week
  from generate_series(&lsquo;2017-01-01&rsquo;::date, now()::date, &lsquo;1 week&rsquo;::interval) week
)</p>

<p>SELECT weeks.week,</p>

<pre><code>   count(*)
</code></pre>

<p>FROM weeks,</p>

<pre><code> users
</code></pre>

<p>WHERE users.created_at > weeks.week
  AND users.created_at &lt;= (weeks.week &ndash; &lsquo;1 week&rsquo;::interval)
GROUP BY 1
ORDER BY 1 DESC;
```</p>

<h3>Timestamp vs. Timestamptz</h3>

<p>What about storing the times themselves? Postgres has two types of timestamps. It has a generic timestamp and one with timezone embedded in it. In most cases you should generally opt for timestamptz. Why not timestamp? What happens if you move a server, or your server somehow swaps its configuration. Or perhaps more practically what about daylight savings time? In general you might think that you can simply just put in the time as you see it, but when different countries around the world observe things like daylight savings time differently it introduces complexities into your application.</p>

<p>With timestamptz it&rsquo;ll be aware of the extra parts of your timezone as it comes in. Then when you query from one timezone that accounts for daylights savings you&rsquo;re all covered. There&rsquo;s a <a href="http://phili.pe/posts/timestamps-and-time-zones-in-postgresql/">number of articles</a> that cover a bit more in depth on the logic between timestamp and timestamp with timezone, so if you&rsquo;re curious I encourage you to check them out, but by default you mostly just need to use timestamptz.</p>

<h3>More</h3>

<p>There&rsquo;s a number of other functions and capabilities when it comes to dealing with time in Postrges. You can <code>extract</code> various parts of a timesetamp or interval such as hour of the day or the month. You can grab the day of the week with <code>dow</code>. And one of my favorites which is when we celebrate happy hour at Citus, there&rsquo;s a literal for UTC 00:00:00 00:00:00 which is <a href="https://www.postgresql.org/message-id/20050124200645.GA6126%40winnie.fuhr.org"><code>allballs()</code></a>. If you need to work with dates and times in Postgres I encourage you to check out the <a href="https://www.postgresql.org/docs/current/static/functions-datetime.html">docs</a> before you try to re-write something of your own, chances are what you need may already be there.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Why use Postgres (Updated for last 5 years)]]></title>
    <link href="http://www.craigkerstiens.com/2017/04/30/why-postgres-five-years-later/"/>
    <updated>2017-04-30T00:00:00-07:00</updated>
    <id>http://www.craigkerstiens.com/2017/04/30/why-postgres-five-years-later</id>
    <content type="html"><![CDATA[<p>Five years ago <a href="http://www.craigkerstiens.com/2012/04/30/why-postgres/">I wrote a post</a> that got some good attention on why you should use Postgres. Almost a year later I <a href="http://www.craigkerstiens.com/2012/05/07/why-postgres-part-2/">added a bunch of things</a> I missed. Many of those items bear repeating, and I&rsquo;ll recap a few of those in the latter half of this post. But in the last 4-5 years there&rsquo;s been a lot of improvements and more reasons added to the list of why you should use Postgres. Here&rsquo;s the rundown of the things that make Postgres a great database you should consider using. <!--more--></p>

<h3>Datatypes, including JSONB and range types</h3>

<p>Postgres has long had an open and friendly attitude for adding datatypes. It&rsquo;s had arrays, geospatical and more for some time. A few years ago it got two datatypes worth thinking about using:</p>

<h4>JSONB</h4>

<p>JSONB is a binary representation of JSON. It&rsquo;s capable of being indexed on with <code>GIN</code> and <code>GIST</code> index types. You can also query into your full JSON document for quick lookups.</p>

<h4>Range types</h4>

<p>While it didn&rsquo;t arrive to the same fame as JSONB, <a href="https://wiki.postgresql.org/images/7/73/Range-types-pgopen-2012.pdf">range types</a> can be especially handy if they&rsquo;re what you need. Within a single column you can have a range from one value to another–this is especially helpful for time ranges. If you&rsquo;re building a calendaring application or often have a from and to of timestamps then range types can let you put that in a single column. The real benefit is that you can then have constraints that certain time stamps can&rsquo;t overlap or other constraints that may make sense for your application.</p>

<h3>Extensions</h3>

<p>It&rsquo;d be hard to talk about Postgres without all the ecosystem around it. Extensions are increasingly quite key when it comes to the community and growth of Postgres. Extensions allow you to hook into Postgres very natively without requiring them to be committed back to the core of Postgres. This means they can add rich functionality without being tied to a Postgres release and review cycle. Some great examples of this are:</p>

<h4>Citus</h4>

<p><a href="https://www.citusdata.com">Citus</a> (who I work for) turns Postgres into a distributed database allowing you to easily shard your database across multiple nodes. To your application it still looks like a single database, but then under the covers it&rsquo;s spread across multiple physical machines and Postgres instances.</p>

<h4>HyperLogLog</h4>

<p>This is a personal favorite of mine that allows you to easily have close-enough distinct counts pre-aggregated, but then also do various operations on them across days such as unions, intersections, and more. <a href="https://www.citusdata.com/blog/2017/04/04/distributed_count_distinct_with_postgresql/">HyperLogLog and other sketch algorithms</a> can be extremely common across large datasets and distributed systems, but it&rsquo;s especially exciting to find them pretty close to out of the box in Postgres.</p>

<h4>PostGIS</h4>

<p>PostGIS isn&rsquo;t new, but it&rsquo;s worth highlighting again. It&rsquo;s commonly regarded as the most advanced geospatial database. PostGIS adds new advanced geospatial datatypes, operators, and makes it easy to do many of the location based activities you need if you&rsquo;re dealing with mapping or routing.</p>

<h3>Logical replication</h3>

<p>For many years the biggest knock against Postgres was the difficulty in setting up replication. Originally this was any form of replication, but then streaming replication came along (this is streaming of the binary WAL or write-ahead-log format). Tools like <a href="https://github.com/wal-e/wal-e">wal-e</a> help leverage much of the Postgres mechanisms for things like disaster recovery.</p>

<p>Then we had the foundation for logical replication in recent releases, though it still required an <a href="https://github.com/2ndQuadrant/pglogical">extension</a> to Postgres so it wasn&rsquo;t 100% out of the box. And, then finally we got full logical replication. Logical replication allows the sending of more or less actual commands, this means you could replicate only certain commands or certain tables.</p>

<h3>Scale</h3>

<p>In addition to all of the usability featuers we&rsquo;ve seen Postgres continue to get better and better at <a href="https://www.slideshare.net/fuzzycz/postgresql-performance-improvements-in-95-and-96">performance</a>. In particular we now have the foundations for <a href="https://www.postgresql.org/docs/current/static/parallel-query.html">parallelism</a> and on some queries you&rsquo;ll see much better performance. Then if you need even greater scale than single node Postgres (such as 122 or 244 GB of RAM on RDS or Heroku) you have options like <a href="https://www.citusdata.com">Citus</a> which was mentioned earlier that can help you scale out.</p>

<h3>Richer indexing</h3>

<p>Postgres already had some pretty powerful indexing before with <a href="https://www.postgresql.org/docs/9.5/static/textsearch-indexes.html">GIN and GiST</a>, those are now useful for JSONB. But we&rsquo;ve also seen the arrival of KNN indexes and Sp-GiST and have even more on the way.</p>

<h3>Upsert</h3>

<p>Upsert was a work in progress for several years. It was one of those features that most people hacked around with <a href="http://www.craigkerstiens.com/2013/11/18/best-postgres-feature-youre-not-using/">CTEs</a>, but that could create race conditions. It was also one of the few features MySQL had over Postgres. And just over a year ago we got <a href="http://www.craigkerstiens.com/2015/05/08/upsert-lands-in-postgres-9.5/">official upsert</a> support.</p>

<h3>Foreign Data Wrappers</h3>

<p>Okay, yes foreign data wrappers did exist many years ago. If you&rsquo;re not familiar with foreign data wrappers, they allow you map an external data system to tables directly in Postgres. This means could could for example interact and query your <a href="http://www.craigkerstiens.com/2012/10/18/connecting_to_redis_from_postgres/">Redis</a> database from directly in Postgres with SQL. They&rsquo;ve continued to be improved more and more from what we had over 5 years ago. In particular we got support for write-able foreign data wrappers, meaning you can write data to other systems from directly in Postgres. There&rsquo;s also now an official Postgres FDW which comes out of the box with Postgres and it by itself is quite useful when querying across various Postgres instances.</p>

<h3>Much more</h3>

<p>And if you missed the <a href="http://www.craigkerstiens.com/2012/04/30/why-postgres/">earlier editions</a> of this, please feel free to <a href="http://www.craigkerstiens.com/2012/05/07/why-postgres-part-2/">check them out</a>. The cliff notes of them include:</p>

<ul>
<li>Window functions</li>
<li>Functions</li>
<li>Custom languages (PLV8 anyone?)</li>
<li>NoSQL datatypes</li>
<li>Custom functions</li>
<li>Common table expressions</li>
<li>Concurrent index creation</li>
<li>Transactional DDL</li>
<li>Foreign Data Wrappers</li>
<li>Conditional and functional indexes</li>
<li>Listen/Notify</li>
<li>Table inheritance</li>
<li>Per transaction synchronous replication</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Getting started with JSONB in Postgres]]></title>
    <link href="http://www.craigkerstiens.com/2017/03/12/getting-started-with-jsonb-in-postgres/"/>
    <updated>2017-03-12T00:00:00-08:00</updated>
    <id>http://www.craigkerstiens.com/2017/03/12/getting-started-with-jsonb-in-postgres</id>
    <content type="html"><![CDATA[<p>JSONB is an awesome datatype in Postgres. I find myself using it on a weekly basis these days. Often in using some API (such as <a href="https://www.clearbit.com">clearbit</a>) I&rsquo;ll get a JSON response back, instead of parsing that out into a table structure it&rsquo;s really easy to throw it into a JSONB then query for various parts of it.</p>

<p><em>If you&rsquo;re not familiar with JSONB, it&rsquo;s a binary representation of JSON in your database. You can read a bit more about it vs. JSON <a href="https://www.citusdata.com/blog/2016/07/14/choosing-nosql-hstore-json-jsonb/">here</a>.</em></p>

<p>In working with JSONB here&rsquo;s a few quick tips to get up and running with it even faster: <!--more--></p>

<h3>Indexing</h3>

<p>For the most part you don&rsquo;t have to think to much about this. With Postgres powerful indexing types you can add one index and have everything within the JSON document, all the keys and all the values, automatically indexed. The key here is to add a <code>GIN</code> index. Once this is done queries should be much faster where you&rsquo;re searching for some value:</p>

<p><code>sql
CREATE INDEX idx_data ON companies USING GIN (data);
</code></p>

<h3>Querying</h3>

<p>Querying is a little bit more work, but once you get the basics it can be pretty straight forward. There&rsquo;s a few new operators you&rsquo;ll want to quickly ramp up on and from there querying becomes easy.</p>

<p>For the most basic part you now have an operator so traverse down the various keys. First let&rsquo;s get some idea of what the JSON looks like so we can have something to work with. Here&rsquo;s a sample set of data that we get back from Clearbit:</p>

<p>```
{
  &ldquo;domain&rdquo;: &ldquo;citusdata.com&rdquo;,
  &ldquo;company&rdquo;: {</p>

<pre><code>"id": "b1ff2bdf-0d8d-4d6d-8bcc-313f6d45996a",
"url": "http:\/\/citusdata.com",
"logo": "https:\/\/logo.clearbit.com\/citusdata.com",
"name": "Citus Data",
"site": {
  "h1": null,
  "url": "http:\/\/citusdata.com",
  "title": "Citus Data",
},
"tags": [
  "SAAS",
  "Enterprise",
  "B2B",
  "Information Technology &amp; Services",
  "Technology",
  "Software"
],
"domain": "citusdata.com",
"twitter": {
  "id": "304455171",
  "bio": "Builders of Citus, the extremely scalable PostgreSQL database.",
  "site": "https:\/\/t.co\/hKpZjIy7Ej",
  "avatar": "https:\/\/pbs.twimg.com\/profile_images\/630900468995108865\/GJFCCXrv_normal.png",
  "handle": "citusdata",
  "location": "San Francisco, CA",
  "followers": 3770,
  "following": 570
},
"category": {
  "sector": "Information Technology",
  "industry": "Internet Software &amp; Services",
  "subIndustry": "Internet Software &amp; Services",
  "industryGroup": "Software &amp; Services"
},
"emailProvider": false
</code></pre>

<p>  }
}
```</p>

<p>Sorry it&rsquo;s a bit long, but it gives us a good example to work with.</p>

<h3>Basic lookups</h3>

<p>Now let&rsquo;s query something fairly basic, the domain:</p>

<p>```sql</p>

<h1>SELECT data->&lsquo;domain&rsquo;</h1>

<p>FROM companies
WHERE domain=&lsquo;citusdata.com&rsquo;
LIMIT 1;</p>

<pre><code>?column?
</code></pre>

<hr />

<p> &ldquo;citusdata.com&rdquo;
```</p>

<p>The <code>-&gt;</code> is likely the first operator you&rsquo;ll use in JSONB. It&rsquo;s helpful to traverse the JSON. Though of you&rsquo;re looking to get the value as text you&rsquo;ll actually want to use <code>-&gt;&gt;</code>. Instead of giving you some quoted response back or JSON object you&rsquo;re going to get it as text which will be a bit cleaner:</p>

<p>```sql</p>

<h1>SELECT data->>&lsquo;domain&rsquo;</h1>

<p>FROM companies
WHERE domain=&lsquo;citusdata.com&rsquo;
LIMIT 1;</p>

<pre><code>?column?
</code></pre>

<hr />

<p> citusdata.com
```</p>

<h3>Filtering for values</h3>

<p>Now with something like clearbit you may want to filter out for only certain type of companies. We can see in the example data that there&rsquo;s a bunch of tags. If we wanted to find only companies that had the tag B2B we could use the <code>?</code> operator once we&rsquo;ve targetted down to that part of the JSON. The <code>?</code> operator will tell us if some part of JSON has a top level key:</p>

<p><code>sql
SELECT *
FROM companies
WHERE data-&gt;'company'-&gt;'tags' ? 'B2B'
</code></p>

<h3>JSONB but pretty</h3>

<p>In querying JSONB you&rsquo;ll typically get a nice compressed set of JSON back. While this is all fine if you&rsquo;re putting it into your application, if you&rsquo;re manually debugging and testing things you probably want something a bit more readable. Of course Postgres has your back here and you can wrap your JSONB with a pretty print function:</p>

<p><code>sql
SELECT jsonb_pretty(data)
FROM companies;
</code></p>

<h3>Much more</h3>

<p>There&rsquo;s a lot more in the <a href="https://www.postgresql.org/docs/9.5/static/functions-json.html">docs</a> that you can find handy for the specialized cases when you need them. <code>jsonb_each</code> will expand a JSONB document into individual rows. So if you wanted to count the number of occurences of every tag for a company, this would help. Want to parse out a JSONB to a row/record in Postgres there&rsquo;s <code>jsonb_to_record</code>. The docs are your friends for about everything you want to do but hopefully these few steps help kick start things if you want to get started with <code>JSONB</code>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Simple but handy Postgres features]]></title>
    <link href="http://www.craigkerstiens.com/2017/01/08/simple-but-handy-postgresql-features/"/>
    <updated>2017-01-08T00:00:00-08:00</updated>
    <id>http://www.craigkerstiens.com/2017/01/08/simple-but-handy-postgresql-features</id>
    <content type="html"><![CDATA[<p>It seems each week when I&rsquo;m reviewing data with someone a feature comes up that they had no idea existed within Postgres. In an effort to continue documenting many of the features and functionality that are useful, here&rsquo;s a list of just a few that you may find handy the next time your working with your data.</p>

<h3>Psql, and \e</h3>

<p>This one I&rsquo;ve <a href="http://www.craigkerstiens.com/2013/02/13/How-I-Work-With-Postgres/">covered before</a>, but it&rsquo;s worth restating. Psql is a great editor that already comes with Postgres. If you&rsquo;re comfortable on the CLI you should consider giving it a try. You can even setup you&rsquo;re own <code>.psqlrc</code> for it so that it&rsquo;s well customized to your liking. In particular turning <code>\timing</code> on is especially useful. But even with all sorts of customization if you&rsquo;re not aware that you can use your preferred editor by using <code>\e</code> then you&rsquo;re missing out. This will allow you to open up the last run query, edit it, save–and then it&rsquo;ll run for you. Vim, Emacs, even Sublime text works just take your pick by setting your <code>$EDITOR</code> variable.</p>

<!--more-->


<h3>Watch</h3>

<p>Ever sit at a terminal running a query over and over to see if something on your system changed? If you&rsquo;re debugging something whether locally or even live in production, watching data change can be key to figuring out. Instead of re-running your query you could simply use the <code>\watch</code> command in Postgres, this will re-run your query automatically every few seconds.</p>

<p>```sql
SELECT now() &ndash;</p>

<pre><code>   query_start, 
   state, 
   query 
</code></pre>

<p>FROM pg_stat_activity
\watch
```</p>

<h3>JSONB pretty print</h3>

<p>I love <a href="https://www.citusdata.com/blog/2016/07/14/choosing-nosql-hstore-json-jsonb/">JSONB</a> as a datatype. Yes, in cases it won&rsquo;t be the <a href="http://blog.heapanalytics.com/when-to-avoid-jsonb-in-a-postgresql-schema/">optimal</a> for performance (though at times it can be perfectly fine). If I&rsquo;m hitting some API that returns a ton of data, I&rsquo;m usually not using all of it right away. But, you never know when you&rsquo;ll want to use the rest of it. I use <a href="https://www.clearbit.com">Clearbit</a> this way today, and for safety sake I save all the JSON result instead of de-normalizing it. Unfortunately, when you query this in Postgres you get one giant compressed text of JSON. Yes, you could pipe out to something like jq, or you could simply use Postgres built in function to make it legible:</p>

<p>```sql
SELECT jsonb_pretty(clearbit_response)
FROM lookup_data;</p>

<pre><code>                            jsonb_pretty
</code></pre>

<hr />

<p> {</p>

<pre><code> "person": { 
     "id": "063f6192-935b-4f31-af6b-b24f63287a60", 
     "bio": null, 
     "geo": { 
         "lat": 37.7749295, 
         "lng": -122.4194155,                                              
         "city": "San Francisco", 
         "state": "California", 
         "country": "United States", 
         "stateCode": "CA", 
         "countryCode": "US" 
     }, 
     "name": { 
     ...
</code></pre>

<p>```</p>

<h3>Importing my data into Google</h3>

<p>This one isn&rsquo;t Postgres specific, but I use it on a weekly basis and it&rsquo;s key for us at <a href="https://www.citusdata.com">Citus</a>. If you use something like Heroku Postgres, dataclips is an extremely handy feature that lets you have a real-time view of a query and the results of it, including an anonymous URL you can it for it. At Citus much like we did at Heroku Postgres we have a dashboard in google sheets which pulls in this data in real-time. To do this simple select a cell then put in: <code>=importdata("pathtoyourdataclip.csv")</code>. Google will import any data using this as long as it&rsquo;s in CSV form. It&rsquo;s a great lightweight way to build out a dashboard for your business without rolling your own complicated dashboarding or building out a complex ETL pipeline.</p>

<p>I&rsquo;m sure I&rsquo;m missing a ton of the smaller features that you use on a daily basis. Let me know <a href="https://www.twitter.com/craigkerstiens">@craigkerstiens</a> the ones I forgot that you feel should be listed.</p>
]]></content>
  </entry>
  
</feed>
