<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Craig Kerstiens]]></title>
  <link href="http://www.craigkerstiens.com/atom.xml" rel="self"/>
  <link href="http://www.craigkerstiens.com/"/>
  <updated>2017-10-31T13:48:14-07:00</updated>
  <id>http://www.craigkerstiens.com/</id>
  <author>
    <name><![CDATA[Craig Kerstiens]]></name>
    <email><![CDATA[craig.kerstiens@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[About Postgres - the non-code bits]]></title>
    <link href="http://www.craigkerstiens.com/2017/10/31/postgres-the-non-code-bits/"/>
    <updated>2017-10-31T13:00:00-07:00</updated>
    <id>http://www.craigkerstiens.com/2017/10/31/postgres-the-non-code-bits</id>
    <content type="html"><![CDATA[<p>Postgres is an interesting open source project. It&rsquo;s truly one of a kind, it has it&rsquo;s own license to prove it as opposed to falling under something like Apache or GPL. The Postgres community structure is something that is pretty well defined if you&rsquo;re involved in the community, but to those outside it&rsquo;s likely a little less clear. In case you&rsquo;re curious to learn more about the community here&rsquo;s a rundown of a few various aspects of it: <!--more--></p>

<h3>PostgreSQL License</h3>

<p>Let&rsquo;s start with the legal part first. <em>First IANAL</em>. PostgreSQL is under it&rsquo;s own license. For those who don&rsquo;t regularly follow software licensing it&rsquo;s extremely liberal and flexible. You can take Postgres, fork it, change it, package it up, and resell it. This is actually one of the reasons you see Postgres at the core of so many other databases like Par Accel, Asterdata, etc. That and that it&rsquo;s such a solid code base that is capable of being extended. <em>I once heard someone describe how they don&rsquo;t really like writing C, but they enjoy writing Postgres C ;)</em></p>

<p><em>A thing you can&rsquo;t do is profit off the PostgreSQL logo without any approval from the core team.</em></p>

<h3>The people</h3>

<p>Within the Postgres community there are 2 major sets of people.</p>

<h3>The core team</h3>

<p>The core team is a smaller team within the Postgres community. The core team is effectively a <a href="https://www.postgresql.org/developer/core/">steering committee</a> for Postgres. They&rsquo;re responsible for coordinating releases, handling confidential issues (read: security issues), managing permissions around PostgreSQL code and infrastructure, defining policy.</p>

<p>The core team is a very small list of people, at the moment 5 individuals.</p>

<h3>Contributors</h3>

<p>Yes, anyone can contribute to Postgres, and with each release there are a laundry of people that write some code that goes into Postgres. If fact Postgres 10 had <a href="https://www.postgresql.org/docs/current/static/release-10.html#idm46046833759776">325 people</a> that contributed in some form. That said there is a hierarchy that exists. The two biggest ones are committers and major contributors.</p>

<p><strong>Committers</strong> gain access after years of contributing to Postgres showing sustained commitment to the project. New committers are voted on each year at PgCon which happens in March/April in Ottawa. If you&rsquo;re ever curious for a conference of what&rsquo;s coming and being deep in the internals of Postgres it&rsquo;s one to check out. Once you do gain your commit bit you&rsquo;re expected to contribute every couple of years to the project. And of course there are a <a href="https://wiki.postgresql.org/wiki/Committers">number of qualifications</a> such as contributing high quality code and perhaps most key is helping review others contributions.</p>

<p><strong>Major contributors</strong> are another notable group. Major contributors don&rsquo;t have full sole commit access, but are held in a higher regard from consistently contributing major features as well as providing review for others.</p>

<p><strong>Contributors</strong>  in general are another area worth calling out. While they may not have a flagship feature to their name like the major contributors, Postgres is what it is because of the contributions of everyone.</p>

<h3>PostgreSQL the company</h3>

<p>Well, it turns out there isn&rsquo;t a company behind Postgres, it&rsquo;s one thing that makes it unique–no one can ever &ldquo;own it&rdquo;. There are some official PostgreSQL non-profits though in particular the US non-profit and the EU non-profit. These non-profits ensure that the core guidelines are enforced and also give coverage for the community to help put on official community conferences. A few of these happen each year which include:</p>

<ul>
<li><a href="https://www.pgcon.org/2018/">PGCon</a> &ndash; The hackers conference</li>
<li><a href="https://2017.postgresopen.org/">PostgresOpen SV</a> &ndash; A consolidation of PostgresOpen and PGConf Silicon Valley</li>
<li><a href="https://postgresopen.org/">PGConf EU</a> &ndash; The largest PG European Conference which moves around each year</li>
</ul>


<p>If you&rsquo;re looking for a way to support the PostgreSQL non-profit organization I&rsquo;d encourage you to consider joining <a href="https://postgresql.us/">PostgreSQL.us</a>.</p>

<h3>Engaging</h3>

<p>So you want to jump into the community, where do you even start? The first place I&rsquo;d encourage is to subscribe to the <a href="https://www.postgresql.org/list/">mailing list</a> or check out the <a href="http://postgres-slack.herokuapp.com/">slack channel</a>. The users mailing is a great one to just jump in and help answer questions and see what people need help with. The hackers list is where you go to get a peek at all the fun debates/discussions/development.</p>

<p>If you&rsquo;re thinking about contributing it&rsquo;s a good idea to lurk on the hackers list for a bit first. Then when the commitfest comes chip in and help review some patches and do some testing. Oh and of course, you can always blog about what you&rsquo;re doing with Postgres and will aim to get it included into <a href="https://www.postgresweekly.com">Postgres Weekly</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Dear Postgres]]></title>
    <link href="http://www.craigkerstiens.com/2017/10/12/dear-postgres/"/>
    <updated>2017-10-12T08:00:00-07:00</updated>
    <id>http://www.craigkerstiens.com/2017/10/12/dear-postgres</id>
    <content type="html"><![CDATA[<p><img src="https://d3vv6lp55qjaqc.cloudfront.net/items/242D1y2p2g0g281S0O3W/dear_postgres.png?X-CloudApp-Visitor-Id=e4475d145dcf11ebcffabf840edcc11f&v=977a17bd" style="border:0px;"/></p>

<p>Dear Postgres,</p>

<p>I&rsquo;ve always felt an affinity for you in my 9 years of working with you. I know others have known you longer, but that doesn&rsquo;t mean they love you more. Years ago when others complained about your rigidness or that you weren&rsquo;t as accommodating as others I found solace in your steadfast values:</p>

<ol>
<li>Don&rsquo;t lose data</li>
<li>Adhere to standards</li>
<li>Move forward with a balancing act between new fads of the day while still continuously improving</li>
</ol>


<p>You&rsquo;ve been there and seen it all. Years ago you were being disrupted by XML databases. As companies made heavy investment into what such a document database would do for their organization you proceeded to &ldquo;simply&rdquo; add a datatype that accomplished the same and brought your years of progress along with it.</p>

<p>In the early years you had the standard format of index <em>b-tree</em> that most database engines leveraged. Then quietly but confidently you started adding more. Then came K-nearest neighbor, generalized inverted indexes (GIN), and generalized search-tree (GiST), only to be followed by space partitioned GiST and block range indexes (BRIN). Now the only question is which do I use?</p>

<p><img src="https://d3vv6lp55qjaqc.cloudfront.net/items/2I43101m250a363W0s0m/Image%202017-10-12%20at%209.11.57%20AM.gif?X-CloudApp-Visitor-Id=e4475d145dcf11ebcffabf840edcc11f&v=100f9c67" style="width:50%" /></p>

<p>All the while there was this other camp using for something that felt cool but outside my world: GIS. GIS, geographical information systems, I thought was something only civil engineers used. Then GPS came along, then the iPhone and location based devices came along and suddenly I wanted to find out the nearest path to my Peets, or manage geographical region for my grocery delivery service. PostGIS had been there all along building up this powerful feature set, sadly to this day I still mostly marvel from the sideline at this whole other feature set I long to take advantage of&hellip; <em>one day&hellip; one day</em>.</p>

<p>A little over 5 years ago I fell in love with your fastly improving analytical capabilities. No you weren&rsquo;t an MPP system yet, but here came window functions and CTEs, then I almost understood recursive CTEs <em>(still working on that one)</em>. I can iterate over data in a recursive fashion without PL/PgSQL? Yes please! I only want to use it more.</p>

<p>And then five years ago, document stores start taking over the world. I feel like I&rsquo;ve seen this story before, wasn&rsquo;t XML going to change the internet? Enter JSON, the JSON datatype, and JSONB. Wow, this is really nice to mix relational, document storage, join against things. I suddenly don&rsquo;t get why more don&rsquo;t take this flexible approach to building on a good foundation and layering on the refinements.</p>

<p>Extensions! Where have you been all my life? There’s <a href="https://www.citusdata.com">Citus</a>, and <a href="https://github.com/aggregateknowledge/postgresql-hll">HyperLogLog</a>, and <a href="https://www.zombodb.com/">ZomboDB</a>, with each I can add functionality to Postgres without it being limited to the standard release, they can be in C or not. Wait, all along so much has been built on this foundation? PostGIS, full-text search, hstore? I like all those things, why didn&rsquo;t you tell me all along about this foundation? Postgres, I like what I&rsquo;m seeing how you&rsquo;re allowing others to do more without having it be in the core of Postgres. This extension stuff is really kinda cool that it&rsquo;s Postgres and then some, kinda like C and then ++, wait nevermind scratch that analogy.</p>

<p>Sorry, I&rsquo;ve rambled a bit. You&rsquo;re a little over twenty years old now. I&rsquo;ve known you for nearly ten of those years so I know there&rsquo;s so much about your background I don&rsquo;t know, I hope we get to spend the time together to share it all. This 10 release is really an exciting one to me. We&rsquo;ve spent all this time together and I feel like each passing year the bond grows fonder.</p>

<p>Now you&rsquo;ve brought me better parallelism so I can further utilize my system resources. I now have partitioning. Thank you! I don&rsquo;t have to roll my own hacks to help age out old data for my time series database. Logical replication will make so many other things possible, such as more online upgrades and integration with other systems.</p>

<p>Postgres, I just want to say thank you for the past ten years together. Thank you for all you’ve done and for all you’ll continue to do in the future.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Tracking and managing your Postgres connections]]></title>
    <link href="http://www.craigkerstiens.com/2017/09/18/postgres-connection-management/"/>
    <updated>2017-09-18T16:00:00-07:00</updated>
    <id>http://www.craigkerstiens.com/2017/09/18/postgres-connection-management</id>
    <content type="html"><![CDATA[<p>Managing connections in Postgres is a topic that seems to come up several times a week in conversations. I&rsquo;ve written some about scaling your connections and the right approach when you truly need a high level of connections, which is to use a connection pooler like pgBouncer. But what do you do before that point and how can you better track what is going on with your connections in Postgres? <!--more--></p>

<p>Postgres under the covers has a lot of metadata about both historical and current activity against a system. Within Postgres you can run the following query which will give you a few results:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
</pre></td><td class='code'><pre><code class='sql'><span class='line'><span class="k">SELECT</span> <span class="k">count</span><span class="p">(</span><span class="o">*</span><span class="p">),</span>
</span><span class='line'>       <span class="k">state</span>
</span><span class='line'><span class="k">FROM</span> <span class="n">pg_stat_activity</span>
</span><span class='line'><span class="k">GROUP</span> <span class="k">BY</span> <span class="mi">2</span><span class="p">;</span>
</span><span class='line'> <span class="k">count</span> <span class="o">|</span>             <span class="k">state</span>
</span><span class='line'><span class="c1">-------+-------------------------------</span>
</span><span class='line'>     <span class="mi">7</span> <span class="o">|</span> <span class="n">active</span>
</span><span class='line'>    <span class="mi">69</span> <span class="o">|</span> <span class="n">idle</span>
</span><span class='line'>    <span class="mi">26</span> <span class="o">|</span> <span class="n">idle</span> <span class="k">in</span> <span class="n">transaction</span>
</span><span class='line'>    <span class="mi">11</span> <span class="o">|</span> <span class="n">idle</span> <span class="k">in</span> <span class="n">transaction</span> <span class="p">(</span><span class="n">aborted</span><span class="p">)</span>
</span><span class='line'><span class="p">(</span><span class="mi">4</span> <span class="k">rows</span><span class="p">)</span>
</span><span class='line'>
</span><span class='line'><span class="n">Time</span><span class="p">:</span> <span class="mi">30</span><span class="p">.</span><span class="mi">337</span> <span class="n">ms</span>
</span></code></pre></td></tr></table></div></figure>


<p>Each of these is useful in determining what you should do to better manage your connection count. All of these numbers can be useful to record every say 30 seconds and chart on your own internal monitoring. Lets break down each:</p>

<ul>
<li><strong>active</strong> &ndash; This is currently running queries, in a sense this is truly how many connections you may require at a time</li>
<li><strong>idle</strong> &ndash; This is where you have opened a connection to the DB (most frameworks do this and maintain a pool of them), but nothing is happening. This is the one area that a connection pooler like pgBouncer can most help.</li>
<li><strong>idle in transaction</strong> &ndash; This is where your app has run a <code>BEGIN</code> but it&rsquo;s now waiting somewhere in a transaction and not doing work.</li>
</ul>


<p>For <strong>idle</strong> as mentioned above it&rsquo;s one that you do want to monitor and if you see a high number here it&rsquo;s worth investing in setting up a pgBouncer.</p>

<p>For <strong>idle in transaction</strong> this one is a bit more interesting. Here what you likely want to do when first investigating is get an idea of how old those are. You can do this by querying pg_stat_activity and filtering for where the state is <code>idle in transaction</code> and checking how old those queries are. For ones that have been running too long you may want to manually kill them.</p>

<p>If you find that you have some stale transactions hanging around this could be for days, hours, or even just a few minutes you may want to set a default to kill those transactions.</p>

<p>To help with this Postgres has a nice feature of a <code>statement_timeout</code>. A statement timeout will automatically kill queries that run longer than the allotted time. You can set this at both a global level and for a specific session. To do this at the database level you&rsquo;d run this with an <code>alter database dbnamehere set statement_timeout = 60000;</code> which is 60 seconds. To do so during a given session simply run <code>set statment_timeout = 6000000;</code>.</p>

<p>For <em>idle in transaction</em> that have been running too long there is its own setting setting that you can set in a similar fashion <code>idle_in_transaction_session_timeout</code> (on Postgres 9.6 and up). Setting both <code>statement_timeout</code> and <code>idle_in_transaction_session_timeout</code> will help with cancelling long running queries and transactions.</p>

<p>Keeping your connection limits in check should lead to a much healthier performing database and thus app.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Better database migrations in Postgres]]></title>
    <link href="http://www.craigkerstiens.com/2017/09/10/better-postgres-migrations/"/>
    <updated>2017-09-10T00:00:00-07:00</updated>
    <id>http://www.craigkerstiens.com/2017/09/10/better-postgres-migrations</id>
    <content type="html"><![CDATA[<p>As your database grows and scales there are some operations that you need to take more care of than you did when you were just starting. When working with your application in your dev environment you may not be fully aware of the cost of some operations until you run them against production. And at some point most of us have been guilty of it, running some migration that starts at 5 minutes, then 15 minutes in it&rsquo;s still running, and suddenly production traffic is impacted.</p>

<p>There are two operations that tend to happen quite frequently, each with some straightforward approaches to mitigate having any noticable amount of downtime. Let&rsquo;s look at each of the operations, how they work and then how you can approach them in a safer way. <!--more--></p>

<h3>Adding new columns</h3>

<p>Adding a new column is actually quite cheap in Postgres. When you do this it updates its underlying tracking of the columns that exist–which is almost instant. The part that becomes expensive is when you have some constraint against the column. A constraint could be a primary or foreign key, or some uniqueness constraint. Here Postgres has to scan through all the records in the table to ensure that it&rsquo;s not being violated. Adding some constraint such as <code>not null</code> does happen some, but is not the most common cause.</p>

<p>The most common reason for slowness of adding a new column is that most frameworks make it very simple for you to set a default value for the new column. It&rsquo;s one thing to do this for all new records, but when you do this when an existing table it means the database has to read all the records and re-write them with the new default value attached. This isn&rsquo;t so bad for a table with a few hundred records, but for a few hundred million run it then go get yourself coffee, or lunch, or a 5 course meal because you&rsquo;ll be waiting for a while.</p>

<p>In short, <code>not null</code> and setting a default value (on creation) of your new column will cause you pain. The solution is to not do those things. But, what if you want to have a default value and don&rsquo;t want to allow <code>nulls</code>. There&rsquo;s a few simple steps you can take, by essentially splitting your migration up from 1 step to 4 migrations:</p>

<ol>
<li>Add your new column <em>that allows nulls</em></li>
<li>Start writing your default value on all new records and updates</li>
<li>Gradually backfill the default value</li>
<li>Apply your constraint</li>
</ol>


<p>Yes, this is a little more work, but it doesn&rsquo;t impact production in nearly the same magnitude.</p>

<h3>Indexes</h3>

<p>Index creation like most DDL operations holds a lock while it&rsquo;s occurring, this means any new data has to wait for the index to be created and then the new writes flow through. Again when firsting creating the table or on a small table this time is not very noticable. On a large database though, you can again wait minutes to possibly even hours. <em>It&rsquo;s a bit ironic when you think about it that adding an index to speed things up can slow things down while it&rsquo;s happening.</em></p>

<p>Postgres of course has the answer for this with <code>CONCURRENT</code> index creation. What this does is gradually build up the index in the background. You can create your index concurrently with: <code>CREATE INDEX CONCURRENTLY</code>. As soon as the index is created and available as long as you did what you were hoping to Postgres will swap over to using it on queries.</p>

<h3>A tool to help</h3>

<p>It&rsquo;s a good practice to understand what is happening when you run a migration and its performance impact. That said you don&rsquo;t have to manage this all on your own. At least for Rails there&rsquo;s a tool to help enforce more of these as you&rsquo;re developing to catch it earlier. <a href="https://github.com/ankane/strong_migrations">Strong migrations</a> aims to catch many of these expensive operations for you to have your back, if you&rsquo;re on Rails consider giving it a look.</p>

<p>Have other tools or tips that can help with database migrations in Postgres? <a href="https://www.twitter.com/craigkerstiens">Drop me a note</a> and I&rsquo;ll work to add them to the list.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Postgres backups: Logical vs. Physical an overview]]></title>
    <link href="http://www.craigkerstiens.com/2017/09/03/postgres-backups-physical-vs-logical/"/>
    <updated>2017-09-03T00:00:00-07:00</updated>
    <id>http://www.craigkerstiens.com/2017/09/03/postgres-backups-physical-vs-logical</id>
    <content type="html"><![CDATA[<p>It&rsquo;s not a very disputed topic that you should backup your database, and further test your backups. What is a little less discussed, at least for Postgres, is the types of backups that exist. Within Postgres there are two forms of backups and understanding them is a useful foundation for anyone working with Postgres. The two backup types are</p>

<ol>
<li>Physical: which consist of the actual bytes on disk,</li>
<li>Logical: which is a more portable format.</li>
</ol>


<p>Let&rsquo;s dig into each a bit more so you can better assess which makes sense for you. <!--more--></p>

<h3>Logical backups</h3>

<p>Logical backups are the most well known type within Postgres. This is what you get when you run <code>pg_dump</code> against a database. There are a number of different formats you can get from <a href="http://postgresguide.com/utilities/backup-restore.html">logical backups</a> and Postgres does a good job of making it easy to compress and configure this backup how you see fit.</p>

<p>When a logical backup is run against a database it is not throttled, this introduces a noticable load on your database.</p>

<p>As it&rsquo;s reading the data from disk and generating (in layman terms) a bunch of SQL <code>INSERT</code> statements, it has to actually see the data. It&rsquo;s of note that older Postgres databases (read: prior to 9.3) there were no checksums against your database. Checksums are just one tool for you to help check against data corruption. Because a logical dump has to actually read and generate the data to insert it will discover any corruption that exists for you.</p>

<p>This portable format is also very useful to pull down copies from production to different environments. I.e. if you need a copy of production data down on your local laptop <code>pg_dump</code> is the way to do it. Logical backups are also database specific, but then allow you to dump only certain tables.</p>

<p>All in all logical backups bring some good features, but come at two cost:</p>

<ul>
<li>Load on your system</li>
<li>The backup contains data as of the time when it ran</li>
</ul>


<h3>Physical backups</h3>

<p>Physical backups are another option when it comes to backing up your database. As we mentioned earlier it is the physical bytes on disk. To understand physical backups we need to know a bit more under the covers about how Postgres works.</p>

<p>Postgres, under the covers, is essentially one giant append only log. When you insert data it gets written to the log known as the write-ahead log (commonly called WAL). When you update data a new record gets written to the WAL. When you delete data a new record gets written to the WAL. Nearly all changes in Postgres including to indexes and otherwise cause an update to the WAL.</p>

<p>With physical backups what you require to be able to create a restore of your database is two things:</p>

<ol>
<li>A <code>base backup</code>, which is a copy of the bytes on disk as of that point and time</li>
<li>Additional segments of the WAL to put the database in some consistent state.</li>
</ol>


<p>A physical backup only requires a small amount of WAL to restore the database to some valid state, <em>but</em> this also gives you some new flexibility. With a base backup plus WAL you can start to replay transactions up to a specific point in time. This is often how point-in-time recovery is performed within Postgres. If you accidentally drop a table, yes&hellip; it happens, you can:</p>

<ol>
<li>Find a base backup before you dropped the table</li>
<li>Restore that base backup</li>
<li>Replay wal segments up to roughly that time just before you dropped the table.</li>
</ol>


<p><em>If you&rsquo;re considering setting up physical backups, consider using a tool like <a href="https://www.citusdata.com/blog/2017/08/18/introducing-wal-g-faster-restores-for-postgres/">WAL-G</a> to help.</em></p>

<h3>Logical vs. Physical which to choose</h3>

<p>Both are useful and provide different benefits. At smaller scale, say under 100 GB of data logical backups via <code>pg_dump</code> are something you should absolutely be doing. Because backups happen quickly on smaller databases you may be able to get out without functionality like point-in-time recovery. At larger scale, as you approach 1 TB physical backups start to become your only option. Because of the load introduced by logical backups and the time lapse between capturing them they become less suitable for production.</p>

<p>Hopefully this primer helps provide a high level overview of the two primary types of backups that exist as options for Postgres. Of course there is much deeper you can go on each, but consider ensuring you have at least one of the two if not both in place. Oh and make sure to test them, an un-tested backup isn&rsquo;t a backup at all.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Postgres Open Silicon Valley line-up: First take]]></title>
    <link href="http://www.craigkerstiens.com/2017/07/01/postgresopen-sv-line-up/"/>
    <updated>2017-07-01T00:00:00-07:00</updated>
    <id>http://www.craigkerstiens.com/2017/07/01/postgresopen-sv-line-up</id>
    <content type="html"><![CDATA[<p>This year Postgres open and PGConf SV have combined to great a bigger and better conference right in downtown San Francisco. <em>I&rsquo;m obviously biased as I&rsquo;m one of the co-chairs, and I know every conference organizer says picking the talks was hard, but I&rsquo;m especially excited for the line-up this year</em>. The hard part for me is going to be which talks do I miss out on because I&rsquo;m sitting in the other session that&rsquo;s ongoing. You can see the full list of <a href="https://postgresql.us/events/sessions/pgopen2017/">talk and tutorial sessions</a>, but I thought it&rsquo;d be fun to do a rundown of some of my favorites. <!--more--></p>

<h3><a href="https://postgresql.us/events/sessions/pgopen2017/session/399-how-postgres-could-index-itself/">How Postgres could index itself</a></h3>

<p><a href="https://postgresql.us/events/sessions/pgopen2017/session/399-how-postgres-could-index-itself/">Postgres indexing itself</a> has long been on my wishlist. <a href="https://www.twitter.com/akane">Andrew Kane</a> from Instacart, and creator of <a href="https://github.com/ankane/pghero/">PgHero</a> has bottled up many learnings into a new tool: <a href="https://medium.com/@ankane/introducing-dexter-the-automatic-indexer-for-postgres-5f8fa8b28f27">Dexter</a>. I suspect we&rsquo;ll get a look at all that went into this, how it works, and how you can leverage it to have a more automatically tuned database.</p>

<h3><a href="https://postgresql.us/events/sessions/pgopen2017/session/376-scaling-a-saas-application-beyond-a-single-postgres-with-citus/">Scaling a SaaS Application Beyond a Single Postgres with Citus</a></h3>

<p>Migration talks are all to common, from Postgres to MySQL from MySQL to Postgres, or from <a href="https://containership.engineering/dynamodb-to-postgres-why-and-how-aa891681af4d">Dynamo to Postgres</a>. But this one is a little different flavor from Postgres to sharded Postgres with <a href="https://www.citusdata.com">Citus</a>. Sharding into a distributed system of course brings new things to consider and think about, and <a href="https://postgresql.us/events/sessions/pgopen2017/session/376-scaling-a-saas-application-beyond-a-single-postgres-with-citus/">here you&rsquo;ll learn about them</a> from first hand experience so hopefully you can avoid mistakes yourself.</p>

<h3><a href="https://postgresql.us/events/sessions/pgopen2017/session/374-concurrency-deep-dive/">Concurrency Deep Dive</a></h3>

<p><a href="https://postgresql.us/events/sessions/pgopen2017/session/374-concurrency-deep-dive/">This one</a> looks to be a great under the hood look as well as likely very practical. It&rsquo;ll cover MVCC which is really at so much of the core of how Postgres works, but then bring it up to what it means for things like locks. Best of all, this one like so many others comes with lots of real world experience from Segment.</p>

<h3><a href="https://postgresql.us/events/sessions/pgopen2017/session/364-postgres-window-magic/">Postgres window magic</a></h3>

<p><img src="https://d3vv6lp55qjaqc.cloudfront.net/items/140p363n3b1N1X440Y0a/Image%202017-06-30%20at%2010.43.55%20AM.png?X-CloudApp-Visitor-Id=e4475d145dcf11ebcffabf840edcc11f&v=e35b6d4d" style="float:right; width:20%; margin-left:15px; " />
I love me some windows, though not always the easiest things to work with. They can let you easily do things like compute month over month growth between rows in a single query. Bruce whose always a great presenter <a href="https://postgresql.us/events/sessions/pgopen2017/session/364-postgres-window-magic/">walks us through them</a> and all the things they&rsquo;re capable of.</p>

<h3><a href="https://postgresql.us/events/sessions/pgopen2017/session/371-running-postgresql-instagram/">Running PostgreSQL @ Instagram</a></h3>

<p>Instagram is well known as one of the largest apps in the world. They optimized and changed their setup multiple times and probably scaled in about every way possible. <a href="https://postgresql.us/events/sessions/pgopen2017/session/371-running-postgresql-instagram/">Here we get to learn</a> about all the various things you need in running at a truly astonishing scale.</p>

<h3><a href="https://postgresql.us/events/sessions/pgopen2017/">Many many more</a></h3>

<p>Of course there&rsquo;s many more. Talks range from looks at new features, to how certain companies are using Postgres. We&rsquo;ve got companies like Instacart and Instagram as mentioned giving talks, to Postgres core committers. Whether you want to learn about the inner workings of Postgres (which often hurts my brain) to how you can simply speed up your app you should find something you like, as long as you like Postgres that is. Take a look at the <a href="https://postgresql.us/events/sessions/pgopen2017/">full list of sessions</a> and we hope to see you there.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Working with time in Postgres]]></title>
    <link href="http://www.craigkerstiens.com/2017/06/08/working-with-time-in-postgres/"/>
    <updated>2017-06-08T00:00:00-07:00</updated>
    <id>http://www.craigkerstiens.com/2017/06/08/working-with-time-in-postgres</id>
    <content type="html"><![CDATA[<p>A massive amount of reporting queries, whether really intensive data analysis, or just basic insights into your business involving looking at data over a certain time period. Postgres has really rich support for dealing with time out of the box, something that&rsquo;s often very underweighted when dealing with a database. Sure, if you have a time-series database it&rsquo;s implied, but even then how flexible and friendly is it from a query perspective? With Postgres there&rsquo;s a lot of key items available to you, let&rsquo;s dig in at the things that make your life easier when querying. <!--more--></p>

<h3>Date math</h3>

<p>The most common thing I find myself doing is looking at users that have done something within some specific time window. If I&rsquo;m executing this all from my app I can easily inject specific dates, but Postgres makes this really easy for you. Within Postgres you have a type called an interval that is some window of time. And fortunately Postgres takes care of the heavy lifting of how might something translate to or from hours/seconds/milliseconds/etc. Here&rsquo;s just a few examples of things you could do with interals:</p>

<ul>
<li>&lsquo;1 day&rsquo;::interval</li>
<li>&lsquo;5 days&rsquo;::interval</li>
<li>&lsquo;1 week&rsquo;::interval</li>
<li>&lsquo;30 days&rsquo;::interval</li>
<li>&lsquo;1 month&rsquo;::interval</li>
</ul>


<p><em>A note that if you&rsquo;re looking to remove something like a full month, you actually want to use 1 month instead of trying to calculate yourself.</em></p>

<p>With a given interval you can easily shift some window of time, such as finding all users that have signed up for your service within the past week:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='sql'><span class='line'><span class="k">SELECT</span> <span class="o">*</span>
</span><span class='line'><span class="k">FROM</span> <span class="n">users</span>
</span><span class='line'><span class="k">WHERE</span> <span class="n">created_at</span> <span class="o">&gt;=</span> <span class="n">now</span><span class="p">()</span> <span class="o">-</span> <span class="s1">&#39;1 week&#39;</span><span class="p">::</span><span class="nb">interval</span>
</span></code></pre></td></tr></table></div></figure>


<h3>Date functions</h3>

<p>Date math makes it pretty easy for you to go and find some specific set of data that applies, but what do you do when you want a broader report around time? There&rsquo;s a few options here. One is to leverage the built-in Postgres functions that help you work with dates and times. <code>date_trunc</code> is one of the most used ones that will truncate a date down to some interval level. Here you can use the same general values as the above, but simply pass in the type of interval it will be. So if we wanted to find the count of users that signed up per week:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class='sql'><span class='line'><span class="k">SELECT</span> <span class="n">date_trunc</span><span class="p">(</span><span class="s1">&#39;week&#39;</span><span class="p">,</span> <span class="n">created_at</span><span class="p">),</span>
</span><span class='line'>       <span class="k">count</span><span class="p">(</span><span class="o">*</span><span class="p">)</span>
</span><span class='line'><span class="k">FROM</span> <span class="n">users</span>
</span><span class='line'><span class="k">GROUP</span> <span class="k">BY</span> <span class="mi">1</span>
</span><span class='line'><span class="k">ORDER</span> <span class="k">BY</span> <span class="mi">1</span> <span class="k">DESC</span><span class="p">;</span>
</span></code></pre></td></tr></table></div></figure>


<p>This gives us a nice roll-up of how many users signed up each week. What&rsquo;s missing here though is if you have a week that has no users. In that case because no users signed up there is no count of 0, it just simply doesn&rsquo;t exist. If you did want something like this you could generate some range of time and then do a cross join with it against users to see which week they fell into. To do this first you&rsquo;d generate a series of dates:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sql'><span class='line'><span class="k">SELECT</span> <span class="n">generate_series</span><span class="p">(</span><span class="s1">&#39;2017-01-01&#39;</span><span class="p">::</span><span class="nb">date</span><span class="p">,</span> <span class="n">now</span><span class="p">()::</span><span class="nb">date</span><span class="p">,</span> <span class="s1">&#39;1 week&#39;</span><span class="p">::</span><span class="nb">interval</span><span class="p">)</span> <span class="n">weeks</span>
</span></code></pre></td></tr></table></div></figure>


<p>Then we&rsquo;re going to join this against the actual users table and check that the <code>created_at</code> falls within the right range.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
</pre></td><td class='code'><pre><code class='sql'><span class='line'><span class="k">with</span> <span class="n">weeks</span> <span class="k">as</span> <span class="p">(</span>
</span><span class='line'>  <span class="k">select</span> <span class="n">week</span>
</span><span class='line'>  <span class="k">from</span> <span class="n">generate_series</span><span class="p">(</span><span class="s1">&#39;2017-01-01&#39;</span><span class="p">::</span><span class="nb">date</span><span class="p">,</span> <span class="n">now</span><span class="p">()::</span><span class="nb">date</span><span class="p">,</span> <span class="s1">&#39;1 week&#39;</span><span class="p">::</span><span class="nb">interval</span><span class="p">)</span> <span class="n">week</span>
</span><span class='line'><span class="p">)</span>
</span><span class='line'>
</span><span class='line'><span class="k">SELECT</span> <span class="n">weeks</span><span class="p">.</span><span class="n">week</span><span class="p">,</span>
</span><span class='line'>       <span class="k">count</span><span class="p">(</span><span class="o">*</span><span class="p">)</span>
</span><span class='line'><span class="k">FROM</span> <span class="n">weeks</span><span class="p">,</span>
</span><span class='line'>     <span class="n">users</span>
</span><span class='line'><span class="k">WHERE</span> <span class="n">users</span><span class="p">.</span><span class="n">created_at</span> <span class="o">&gt;</span> <span class="n">weeks</span><span class="p">.</span><span class="n">week</span>
</span><span class='line'>  <span class="k">AND</span> <span class="n">users</span><span class="p">.</span><span class="n">created_at</span> <span class="o">&lt;=</span> <span class="p">(</span><span class="n">weeks</span><span class="p">.</span><span class="n">week</span> <span class="o">-</span> <span class="s1">&#39;1 week&#39;</span><span class="p">::</span><span class="nb">interval</span><span class="p">)</span>
</span><span class='line'><span class="k">GROUP</span> <span class="k">BY</span> <span class="mi">1</span>
</span><span class='line'><span class="k">ORDER</span> <span class="k">BY</span> <span class="mi">1</span> <span class="k">DESC</span><span class="p">;</span>
</span></code></pre></td></tr></table></div></figure>


<h3>Timestamp vs. Timestamptz</h3>

<p>What about storing the times themselves? Postgres has two types of timestamps. It has a generic timestamp and one with timezone embedded in it. In most cases you should generally opt for timestamptz. Why not timestamp? What happens if you move a server, or your server somehow swaps its configuration. Or perhaps more practically what about daylight savings time? In general you might think that you can simply just put in the time as you see it, but when different countries around the world observe things like daylight savings time differently it introduces complexities into your application.</p>

<p>With timestamptz it&rsquo;ll be aware of the extra parts of your timezone as it comes in. Then when you query from one timezone that accounts for daylights savings you&rsquo;re all covered. There&rsquo;s a <a href="http://phili.pe/posts/timestamps-and-time-zones-in-postgresql/">number of articles</a> that cover a bit more in depth on the logic between timestamp and timestamp with timezone, so if you&rsquo;re curious I encourage you to check them out, but by default you mostly just need to use timestamptz.</p>

<h3>More</h3>

<p>There&rsquo;s a number of other functions and capabilities when it comes to dealing with time in Postrges. You can <code>extract</code> various parts of a timesetamp or interval such as hour of the day or the month. You can grab the day of the week with <code>dow</code>. And one of my favorites which is when we celebrate happy hour at Citus, there&rsquo;s a literal for UTC 00:00:00 00:00:00 which is <a href="https://www.postgresql.org/message-id/20050124200645.GA6126%40winnie.fuhr.org"><code>allballs()</code></a>. If you need to work with dates and times in Postgres I encourage you to check out the <a href="https://www.postgresql.org/docs/current/static/functions-datetime.html">docs</a> before you try to re-write something of your own, chances are what you need may already be there.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Why use Postgres (Updated for last 5 years)]]></title>
    <link href="http://www.craigkerstiens.com/2017/04/30/why-postgres-five-years-later/"/>
    <updated>2017-04-30T00:00:00-07:00</updated>
    <id>http://www.craigkerstiens.com/2017/04/30/why-postgres-five-years-later</id>
    <content type="html"><![CDATA[<p>Five years ago <a href="http://www.craigkerstiens.com/2012/04/30/why-postgres/">I wrote a post</a> that got some good attention on why you should use Postgres. Almost a year later I <a href="http://www.craigkerstiens.com/2012/05/07/why-postgres-part-2/">added a bunch of things</a> I missed. Many of those items bear repeating, and I&rsquo;ll recap a few of those in the latter half of this post. But in the last 4-5 years there&rsquo;s been a lot of improvements and more reasons added to the list of why you should use Postgres. Here&rsquo;s the rundown of the things that make Postgres a great database you should consider using. <!--more--></p>

<h3>Datatypes, including JSONB and range types</h3>

<p>Postgres has long had an open and friendly attitude for adding datatypes. It&rsquo;s had arrays, geospatical and more for some time. A few years ago it got two datatypes worth thinking about using:</p>

<h4>JSONB</h4>

<p>JSONB is a binary representation of JSON. It&rsquo;s capable of being indexed on with <code>GIN</code> and <code>GIST</code> index types. You can also query into your full JSON document for quick lookups.</p>

<h4>Range types</h4>

<p>While it didn&rsquo;t arrive to the same fame as JSONB, <a href="https://wiki.postgresql.org/images/7/73/Range-types-pgopen-2012.pdf">range types</a> can be especially handy if they&rsquo;re what you need. Within a single column you can have a range from one value to another–this is especially helpful for time ranges. If you&rsquo;re building a calendaring application or often have a from and to of timestamps then range types can let you put that in a single column. The real benefit is that you can then have constraints that certain time stamps can&rsquo;t overlap or other constraints that may make sense for your application.</p>

<h3>Extensions</h3>

<p>It&rsquo;d be hard to talk about Postgres without all the ecosystem around it. Extensions are increasingly quite key when it comes to the community and growth of Postgres. Extensions allow you to hook into Postgres very natively without requiring them to be committed back to the core of Postgres. This means they can add rich functionality without being tied to a Postgres release and review cycle. Some great examples of this are:</p>

<h4>Citus</h4>

<p><a href="https://www.citusdata.com">Citus</a> (who I work for) turns Postgres into a distributed database allowing you to easily shard your database across multiple nodes. To your application it still looks like a single database, but then under the covers it&rsquo;s spread across multiple physical machines and Postgres instances.</p>

<h4>HyperLogLog</h4>

<p>This is a personal favorite of mine that allows you to easily have close-enough distinct counts pre-aggregated, but then also do various operations on them across days such as unions, intersections, and more. <a href="https://www.citusdata.com/blog/2017/04/04/distributed_count_distinct_with_postgresql/">HyperLogLog and other sketch algorithms</a> can be extremely common across large datasets and distributed systems, but it&rsquo;s especially exciting to find them pretty close to out of the box in Postgres.</p>

<h4>PostGIS</h4>

<p>PostGIS isn&rsquo;t new, but it&rsquo;s worth highlighting again. It&rsquo;s commonly regarded as the most advanced geospatial database. PostGIS adds new advanced geospatial datatypes, operators, and makes it easy to do many of the location based activities you need if you&rsquo;re dealing with mapping or routing.</p>

<h3>Logical replication</h3>

<p>For many years the biggest knock against Postgres was the difficulty in setting up replication. Originally this was any form of replication, but then streaming replication came along (this is streaming of the binary WAL or write-ahead-log format). Tools like <a href="https://github.com/wal-e/wal-e">wal-e</a> help leverage much of the Postgres mechanisms for things like disaster recovery.</p>

<p>Then we had the foundation for logical replication in recent releases, though it still required an <a href="https://github.com/2ndQuadrant/pglogical">extension</a> to Postgres so it wasn&rsquo;t 100% out of the box. And, then finally we got full logical replication. Logical replication allows the sending of more or less actual commands, this means you could replicate only certain commands or certain tables.</p>

<h3>Scale</h3>

<p>In addition to all of the usability featuers we&rsquo;ve seen Postgres continue to get better and better at <a href="https://www.slideshare.net/fuzzycz/postgresql-performance-improvements-in-95-and-96">performance</a>. In particular we now have the foundations for <a href="https://www.postgresql.org/docs/current/static/parallel-query.html">parallelism</a> and on some queries you&rsquo;ll see much better performance. Then if you need even greater scale than single node Postgres (such as 122 or 244 GB of RAM on RDS or Heroku) you have options like <a href="https://www.citusdata.com">Citus</a> which was mentioned earlier that can help you scale out.</p>

<h3>Richer indexing</h3>

<p>Postgres already had some pretty powerful indexing before with <a href="https://www.postgresql.org/docs/9.5/static/textsearch-indexes.html">GIN and GiST</a>, those are now useful for JSONB. But we&rsquo;ve also seen the arrival of KNN indexes and Sp-GiST and have even more on the way.</p>

<h3>Upsert</h3>

<p>Upsert was a work in progress for several years. It was one of those features that most people hacked around with <a href="http://www.craigkerstiens.com/2013/11/18/best-postgres-feature-youre-not-using/">CTEs</a>, but that could create race conditions. It was also one of the few features MySQL had over Postgres. And just over a year ago we got <a href="http://www.craigkerstiens.com/2015/05/08/upsert-lands-in-postgres-9.5/">official upsert</a> support.</p>

<h3>Foreign Data Wrappers</h3>

<p>Okay, yes foreign data wrappers did exist many years ago. If you&rsquo;re not familiar with foreign data wrappers, they allow you map an external data system to tables directly in Postgres. This means could could for example interact and query your <a href="http://www.craigkerstiens.com/2012/10/18/connecting_to_redis_from_postgres/">Redis</a> database from directly in Postgres with SQL. They&rsquo;ve continued to be improved more and more from what we had over 5 years ago. In particular we got support for write-able foreign data wrappers, meaning you can write data to other systems from directly in Postgres. There&rsquo;s also now an official Postgres FDW which comes out of the box with Postgres and it by itself is quite useful when querying across various Postgres instances.</p>

<h3>Much more</h3>

<p>And if you missed the <a href="http://www.craigkerstiens.com/2012/04/30/why-postgres/">earlier editions</a> of this, please feel free to <a href="http://www.craigkerstiens.com/2012/05/07/why-postgres-part-2/">check them out</a>. The cliff notes of them include:</p>

<ul>
<li>Window functions</li>
<li>Functions</li>
<li>Custom languages (PLV8 anyone?)</li>
<li>NoSQL datatypes</li>
<li>Custom functions</li>
<li>Common table expressions</li>
<li>Concurrent index creation</li>
<li>Transactional DDL</li>
<li>Foreign Data Wrappers</li>
<li>Conditional and functional indexes</li>
<li>Listen/Notify</li>
<li>Table inheritance</li>
<li>Per transaction synchronous replication</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Getting started with JSONB in Postgres]]></title>
    <link href="http://www.craigkerstiens.com/2017/03/12/getting-started-with-jsonb-in-postgres/"/>
    <updated>2017-03-12T00:00:00-08:00</updated>
    <id>http://www.craigkerstiens.com/2017/03/12/getting-started-with-jsonb-in-postgres</id>
    <content type="html"><![CDATA[<p>JSONB is an awesome datatype in Postgres. I find myself using it on a weekly basis these days. Often in using some API (such as <a href="https://www.clearbit.com">clearbit</a>) I&rsquo;ll get a JSON response back, instead of parsing that out into a table structure it&rsquo;s really easy to throw it into a JSONB then query for various parts of it.</p>

<p><em>If you&rsquo;re not familiar with JSONB, it&rsquo;s a binary representation of JSON in your database. You can read a bit more about it vs. JSON <a href="https://www.citusdata.com/blog/2016/07/14/choosing-nosql-hstore-json-jsonb/">here</a>.</em></p>

<p>In working with JSONB here&rsquo;s a few quick tips to get up and running with it even faster: <!--more--></p>

<h3>Indexing</h3>

<p>For the most part you don&rsquo;t have to think to much about this. With Postgres powerful indexing types you can add one index and have everything within the JSON document, all the keys and all the values, automatically indexed. The key here is to add a <code>GIN</code> index. Once this is done queries should be much faster where you&rsquo;re searching for some value:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sql'><span class='line'><span class="k">CREATE</span> <span class="k">INDEX</span> <span class="n">idx_data</span> <span class="k">ON</span> <span class="n">companies</span> <span class="k">USING</span> <span class="n">GIN</span> <span class="p">(</span><span class="k">data</span><span class="p">);</span>
</span></code></pre></td></tr></table></div></figure>


<h3>Querying</h3>

<p>Querying is a little bit more work, but once you get the basics it can be pretty straight forward. There&rsquo;s a few new operators you&rsquo;ll want to quickly ramp up on and from there querying becomes easy.</p>

<p>For the most basic part you now have an operator so traverse down the various keys. First let&rsquo;s get some idea of what the JSON looks like so we can have something to work with. Here&rsquo;s a sample set of data that we get back from Clearbit:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
</pre></td><td class='code'><pre><code class='sql'><span class='line'><span class="err">{</span>
</span><span class='line'>  <span class="ss">&quot;domain&quot;</span><span class="p">:</span> <span class="ss">&quot;citusdata.com&quot;</span><span class="p">,</span>
</span><span class='line'>  <span class="ss">&quot;company&quot;</span><span class="p">:</span> <span class="err">{</span>
</span><span class='line'>    <span class="ss">&quot;id&quot;</span><span class="p">:</span> <span class="ss">&quot;b1ff2bdf-0d8d-4d6d-8bcc-313f6d45996a&quot;</span><span class="p">,</span>
</span><span class='line'>    <span class="ss">&quot;url&quot;</span><span class="p">:</span> <span class="ss">&quot;http:\/\/citusdata.com&quot;</span><span class="p">,</span>
</span><span class='line'>    <span class="ss">&quot;logo&quot;</span><span class="p">:</span> <span class="ss">&quot;https:\/\/logo.clearbit.com\/citusdata.com&quot;</span><span class="p">,</span>
</span><span class='line'>    <span class="ss">&quot;name&quot;</span><span class="p">:</span> <span class="ss">&quot;Citus Data&quot;</span><span class="p">,</span>
</span><span class='line'>    <span class="ss">&quot;site&quot;</span><span class="p">:</span> <span class="err">{</span>
</span><span class='line'>      <span class="ss">&quot;h1&quot;</span><span class="p">:</span> <span class="k">null</span><span class="p">,</span>
</span><span class='line'>      <span class="ss">&quot;url&quot;</span><span class="p">:</span> <span class="ss">&quot;http:\/\/citusdata.com&quot;</span><span class="p">,</span>
</span><span class='line'>      <span class="ss">&quot;title&quot;</span><span class="p">:</span> <span class="ss">&quot;Citus Data&quot;</span><span class="p">,</span>
</span><span class='line'>    <span class="err">}</span><span class="p">,</span>
</span><span class='line'>    <span class="ss">&quot;tags&quot;</span><span class="p">:</span> <span class="p">[</span>
</span><span class='line'>      <span class="ss">&quot;SAAS&quot;</span><span class="p">,</span>
</span><span class='line'>      <span class="ss">&quot;Enterprise&quot;</span><span class="p">,</span>
</span><span class='line'>      <span class="ss">&quot;B2B&quot;</span><span class="p">,</span>
</span><span class='line'>      <span class="ss">&quot;Information Technology &amp; Services&quot;</span><span class="p">,</span>
</span><span class='line'>      <span class="ss">&quot;Technology&quot;</span><span class="p">,</span>
</span><span class='line'>      <span class="ss">&quot;Software&quot;</span>
</span><span class='line'>    <span class="p">],</span>
</span><span class='line'>    <span class="ss">&quot;domain&quot;</span><span class="p">:</span> <span class="ss">&quot;citusdata.com&quot;</span><span class="p">,</span>
</span><span class='line'>    <span class="ss">&quot;twitter&quot;</span><span class="p">:</span> <span class="err">{</span>
</span><span class='line'>      <span class="ss">&quot;id&quot;</span><span class="p">:</span> <span class="ss">&quot;304455171&quot;</span><span class="p">,</span>
</span><span class='line'>      <span class="ss">&quot;bio&quot;</span><span class="p">:</span> <span class="ss">&quot;Builders of Citus, the extremely scalable PostgreSQL database.&quot;</span><span class="p">,</span>
</span><span class='line'>      <span class="ss">&quot;site&quot;</span><span class="p">:</span> <span class="ss">&quot;https:\/\/t.co\/hKpZjIy7Ej&quot;</span><span class="p">,</span>
</span><span class='line'>      <span class="ss">&quot;avatar&quot;</span><span class="p">:</span> <span class="ss">&quot;https:\/\/pbs.twimg.com\/profile_images\/630900468995108865\/GJFCCXrv_normal.png&quot;</span><span class="p">,</span>
</span><span class='line'>      <span class="ss">&quot;handle&quot;</span><span class="p">:</span> <span class="ss">&quot;citusdata&quot;</span><span class="p">,</span>
</span><span class='line'>      <span class="ss">&quot;location&quot;</span><span class="p">:</span> <span class="ss">&quot;San Francisco, CA&quot;</span><span class="p">,</span>
</span><span class='line'>      <span class="ss">&quot;followers&quot;</span><span class="p">:</span> <span class="mi">3770</span><span class="p">,</span>
</span><span class='line'>      <span class="ss">&quot;following&quot;</span><span class="p">:</span> <span class="mi">570</span>
</span><span class='line'>    <span class="err">}</span><span class="p">,</span>
</span><span class='line'>    <span class="ss">&quot;category&quot;</span><span class="p">:</span> <span class="err">{</span>
</span><span class='line'>      <span class="ss">&quot;sector&quot;</span><span class="p">:</span> <span class="ss">&quot;Information Technology&quot;</span><span class="p">,</span>
</span><span class='line'>      <span class="ss">&quot;industry&quot;</span><span class="p">:</span> <span class="ss">&quot;Internet Software &amp; Services&quot;</span><span class="p">,</span>
</span><span class='line'>      <span class="ss">&quot;subIndustry&quot;</span><span class="p">:</span> <span class="ss">&quot;Internet Software &amp; Services&quot;</span><span class="p">,</span>
</span><span class='line'>      <span class="ss">&quot;industryGroup&quot;</span><span class="p">:</span> <span class="ss">&quot;Software &amp; Services&quot;</span>
</span><span class='line'>    <span class="err">}</span><span class="p">,</span>
</span><span class='line'>    <span class="ss">&quot;emailProvider&quot;</span><span class="p">:</span> <span class="k">false</span>
</span><span class='line'>  <span class="err">}</span>
</span><span class='line'><span class="err">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>Sorry it&rsquo;s a bit long, but it gives us a good example to work with.</p>

<h3>Basic lookups</h3>

<p>Now let&rsquo;s query something fairly basic, the domain:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class='sql'><span class='line'><span class="o">#</span> <span class="k">SELECT</span> <span class="k">data</span><span class="o">-&gt;</span><span class="s1">&#39;domain&#39;</span>
</span><span class='line'><span class="k">FROM</span> <span class="n">companies</span>
</span><span class='line'><span class="k">WHERE</span> <span class="k">domain</span><span class="o">=</span><span class="s1">&#39;citusdata.com&#39;</span>
</span><span class='line'><span class="k">LIMIT</span> <span class="mi">1</span><span class="p">;</span>
</span><span class='line'>
</span><span class='line'>    <span class="o">?</span><span class="k">column</span><span class="o">?</span>
</span><span class='line'><span class="c1">-----------------</span>
</span><span class='line'> <span class="ss">&quot;citusdata.com&quot;</span>
</span></code></pre></td></tr></table></div></figure>


<p>The <code>-&gt;</code> is likely the first operator you&rsquo;ll use in JSONB. It&rsquo;s helpful to traverse the JSON. Though of you&rsquo;re looking to get the value as text you&rsquo;ll actually want to use <code>-&gt;&gt;</code>. Instead of giving you some quoted response back or JSON object you&rsquo;re going to get it as text which will be a bit cleaner:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class='sql'><span class='line'><span class="o">#</span> <span class="k">SELECT</span> <span class="k">data</span><span class="o">-&gt;&gt;</span><span class="s1">&#39;domain&#39;</span>
</span><span class='line'><span class="k">FROM</span> <span class="n">companies</span>
</span><span class='line'><span class="k">WHERE</span> <span class="k">domain</span><span class="o">=</span><span class="s1">&#39;citusdata.com&#39;</span>
</span><span class='line'><span class="k">LIMIT</span> <span class="mi">1</span><span class="p">;</span>
</span><span class='line'>
</span><span class='line'>    <span class="o">?</span><span class="k">column</span><span class="o">?</span>
</span><span class='line'><span class="c1">-----------------</span>
</span><span class='line'> <span class="n">citusdata</span><span class="p">.</span><span class="n">com</span>
</span></code></pre></td></tr></table></div></figure>


<h3>Filtering for values</h3>

<p>Now with something like clearbit you may want to filter out for only certain type of companies. We can see in the example data that there&rsquo;s a bunch of tags. If we wanted to find only companies that had the tag B2B we could use the <code>?</code> operator once we&rsquo;ve targetted down to that part of the JSON. The <code>?</code> operator will tell us if some part of JSON has a top level key:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='sql'><span class='line'><span class="k">SELECT</span> <span class="o">*</span>
</span><span class='line'><span class="k">FROM</span> <span class="n">companies</span>
</span><span class='line'><span class="k">WHERE</span> <span class="k">data</span><span class="o">-&gt;</span><span class="s1">&#39;company&#39;</span><span class="o">-&gt;</span><span class="s1">&#39;tags&#39;</span> <span class="o">?</span> <span class="s1">&#39;B2B&#39;</span>
</span></code></pre></td></tr></table></div></figure>


<h3>JSONB but pretty</h3>

<p>In querying JSONB you&rsquo;ll typically get a nice compressed set of JSON back. While this is all fine if you&rsquo;re putting it into your application, if you&rsquo;re manually debugging and testing things you probably want something a bit more readable. Of course Postgres has your back here and you can wrap your JSONB with a pretty print function:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='sql'><span class='line'><span class="k">SELECT</span> <span class="n">jsonb_pretty</span><span class="p">(</span><span class="k">data</span><span class="p">)</span>
</span><span class='line'><span class="k">FROM</span> <span class="n">companies</span><span class="p">;</span>
</span></code></pre></td></tr></table></div></figure>


<h3>Much more</h3>

<p>There&rsquo;s a lot more in the <a href="https://www.postgresql.org/docs/9.5/static/functions-json.html">docs</a> that you can find handy for the specialized cases when you need them. <code>jsonb_each</code> will expand a JSONB document into individual rows. So if you wanted to count the number of occurences of every tag for a company, this would help. Want to parse out a JSONB to a row/record in Postgres there&rsquo;s <code>jsonb_to_record</code>. The docs are your friends for about everything you want to do but hopefully these few steps help kick start things if you want to get started with <code>JSONB</code>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Simple but handy Postgres features]]></title>
    <link href="http://www.craigkerstiens.com/2017/01/08/simple-but-handy-postgresql-features/"/>
    <updated>2017-01-08T00:00:00-08:00</updated>
    <id>http://www.craigkerstiens.com/2017/01/08/simple-but-handy-postgresql-features</id>
    <content type="html"><![CDATA[<p>It seems each week when I&rsquo;m reviewing data with someone a feature comes up that they had no idea existed within Postgres. In an effort to continue documenting many of the features and functionality that are useful, here&rsquo;s a list of just a few that you may find handy the next time your working with your data.</p>

<h3>Psql, and \e</h3>

<p>This one I&rsquo;ve <a href="http://www.craigkerstiens.com/2013/02/13/How-I-Work-With-Postgres/">covered before</a>, but it&rsquo;s worth restating. Psql is a great editor that already comes with Postgres. If you&rsquo;re comfortable on the CLI you should consider giving it a try. You can even setup you&rsquo;re own <code>.psqlrc</code> for it so that it&rsquo;s well customized to your liking. In particular turning <code>\timing</code> on is especially useful. But even with all sorts of customization if you&rsquo;re not aware that you can use your preferred editor by using <code>\e</code> then you&rsquo;re missing out. This will allow you to open up the last run query, edit it, save–and then it&rsquo;ll run for you. Vim, Emacs, even Sublime text works just take your pick by setting your <code>$EDITOR</code> variable.</p>

<!--more-->


<h3>Watch</h3>

<p>Ever sit at a terminal running a query over and over to see if something on your system changed? If you&rsquo;re debugging something whether locally or even live in production, watching data change can be key to figuring out. Instead of re-running your query you could simply use the <code>\watch</code> command in Postgres, this will re-run your query automatically every few seconds.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class='sql'><span class='line'><span class="k">SELECT</span> <span class="n">now</span><span class="p">()</span> <span class="o">-</span>
</span><span class='line'>       <span class="n">query_start</span><span class="p">,</span>
</span><span class='line'>       <span class="k">state</span><span class="p">,</span>
</span><span class='line'>       <span class="n">query</span>
</span><span class='line'><span class="k">FROM</span> <span class="n">pg_stat_activity</span>
</span><span class='line'><span class="err">\</span><span class="n">watch</span>
</span></code></pre></td></tr></table></div></figure>


<h3>JSONB pretty print</h3>

<p>I love <a href="https://www.citusdata.com/blog/2016/07/14/choosing-nosql-hstore-json-jsonb/">JSONB</a> as a datatype. Yes, in cases it won&rsquo;t be the <a href="http://blog.heapanalytics.com/when-to-avoid-jsonb-in-a-postgresql-schema/">optimal</a> for performance (though at times it can be perfectly fine). If I&rsquo;m hitting some API that returns a ton of data, I&rsquo;m usually not using all of it right away. But, you never know when you&rsquo;ll want to use the rest of it. I use <a href="https://www.clearbit.com">Clearbit</a> this way today, and for safety sake I save all the JSON result instead of de-normalizing it. Unfortunately, when you query this in Postgres you get one giant compressed text of JSON. Yes, you could pipe out to something like jq, or you could simply use Postgres built in function to make it legible:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
</pre></td><td class='code'><pre><code class='sql'><span class='line'><span class="k">SELECT</span> <span class="n">jsonb_pretty</span><span class="p">(</span><span class="n">clearbit_response</span><span class="p">)</span>
</span><span class='line'><span class="k">FROM</span> <span class="n">lookup_data</span><span class="p">;</span>
</span><span class='line'>
</span><span class='line'>                                <span class="n">jsonb_pretty</span>
</span><span class='line'><span class="c1">-------------------------------------------------------------------------------</span>
</span><span class='line'> <span class="err">{</span>
</span><span class='line'>     <span class="ss">&quot;person&quot;</span><span class="p">:</span> <span class="err">{</span>
</span><span class='line'>         <span class="ss">&quot;id&quot;</span><span class="p">:</span> <span class="ss">&quot;063f6192-935b-4f31-af6b-b24f63287a60&quot;</span><span class="p">,</span>
</span><span class='line'>         <span class="ss">&quot;bio&quot;</span><span class="p">:</span> <span class="k">null</span><span class="p">,</span>
</span><span class='line'>         <span class="ss">&quot;geo&quot;</span><span class="p">:</span> <span class="err">{</span>
</span><span class='line'>             <span class="ss">&quot;lat&quot;</span><span class="p">:</span> <span class="mi">37</span><span class="p">.</span><span class="mi">7749295</span><span class="p">,</span>
</span><span class='line'>             <span class="ss">&quot;lng&quot;</span><span class="p">:</span> <span class="o">-</span><span class="mi">122</span><span class="p">.</span><span class="mi">4194155</span><span class="p">,</span>
</span><span class='line'>             <span class="ss">&quot;city&quot;</span><span class="p">:</span> <span class="ss">&quot;San Francisco&quot;</span><span class="p">,</span>
</span><span class='line'>             <span class="ss">&quot;state&quot;</span><span class="p">:</span> <span class="ss">&quot;California&quot;</span><span class="p">,</span>
</span><span class='line'>             <span class="ss">&quot;country&quot;</span><span class="p">:</span> <span class="ss">&quot;United States&quot;</span><span class="p">,</span>
</span><span class='line'>             <span class="ss">&quot;stateCode&quot;</span><span class="p">:</span> <span class="ss">&quot;CA&quot;</span><span class="p">,</span>
</span><span class='line'>             <span class="ss">&quot;countryCode&quot;</span><span class="p">:</span> <span class="ss">&quot;US&quot;</span>
</span><span class='line'>         <span class="err">}</span><span class="p">,</span>
</span><span class='line'>         <span class="ss">&quot;name&quot;</span><span class="p">:</span> <span class="err">{</span>
</span><span class='line'>         <span class="p">...</span>
</span></code></pre></td></tr></table></div></figure>


<h3>Importing my data into Google</h3>

<p>This one isn&rsquo;t Postgres specific, but I use it on a weekly basis and it&rsquo;s key for us at <a href="https://www.citusdata.com">Citus</a>. If you use something like Heroku Postgres, dataclips is an extremely handy feature that lets you have a real-time view of a query and the results of it, including an anonymous URL you can it for it. At Citus much like we did at Heroku Postgres we have a dashboard in google sheets which pulls in this data in real-time. To do this simple select a cell then put in: <code>=importdata("pathtoyourdataclip.csv")</code>. Google will import any data using this as long as it&rsquo;s in CSV form. It&rsquo;s a great lightweight way to build out a dashboard for your business without rolling your own complicated dashboarding or building out a complex ETL pipeline.</p>

<p>I&rsquo;m sure I&rsquo;m missing a ton of the smaller features that you use on a daily basis. Let me know <a href="https://www.twitter.com/craigkerstiens">@craigkerstiens</a> the ones I forgot that you feel should be listed.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Syncing from Postgres to Salesforce - Data Mappings]]></title>
    <link href="http://www.craigkerstiens.com/2016/11/23/syncing-from-postgres-to-salesforce-part-1/"/>
    <updated>2016-11-23T00:00:00-08:00</updated>
    <id>http://www.craigkerstiens.com/2016/11/23/syncing-from-postgres-to-salesforce-part-1</id>
    <content type="html"><![CDATA[<p>For the second time now I&rsquo;ve had to implement a system that syncs from my system of record into Salesforce.com, the first at Heroku and now at <a href="https://www.citusdata.com">Citus Data</a>. The case here is pretty simple, I have a software-as-a-service, B2B product. It&rsquo;s a homegrown application in these cases in Ruby, but could be Python, .Net, any language of your choosing. The problem is I don&rsquo;t want to have to be rebuilding my own CRM, reporting, etc. on top of all of my internal database. And as soon as you&rsquo;re at some reasonable size (sales guy of 1 or more) you need to be able to provide insights on what&rsquo;s in that system of record database to others.</p>

<p>While my tooling isn&rsquo;t a full fledged product by any means, here&rsquo;s a bit of how I&rsquo;ve developed this process a few times over and some of the annoying bits of code to help get you started. In this post I&rsquo;ll walk through some of the basic datatypes, then we&rsquo;ll follow-up with the overall architecture and tweaks you need to make to Salesforce, and finally we&rsquo;ll provide some example code to help you create this setup yourself.<!--more--></p>

<h3>Leads, Contacts, Accounts oh my</h3>

<p>Despite being some of the largest as-a-service vendors in the world, Salesforce is still primarily setup for traditional high touch sales. What this means is some of the data you&rsquo;ll commonly have, or in this case not have, can make it difficult to figure out what maps from your internal system to Salesforce. Within Salesforce there&rsquo;s really 4 key data models you&rsquo;re going to care about.</p>

<h3>Lead vs. Contact</h3>

<p>In every as a service product you&rsquo;ll have some user that creates and account which usually has an email address tied to it. This seems simple enough to load up to Salesforce as there is a clear email field. Within Salesforce there are two key data types which have a default field for this lead and contact, in Salesforce terms a lead is someone <a href="https://success.salesforce.com/answers?id=90630000000gvTiAAI">considering doing business with you</a>, a contact someone who more so is doing business with you. If you have a freemium or timed trial model you might think to start classifying everyone that they&rsquo;re a lead. Then, when they convert to a paying customer you turn them into a contact.</p>

<p>If you&rsquo;re anything like me, in running your SaaS business, you want a sign-up process that&rsquo;s frictionless. This means give me an email address, password, and you&rsquo;re off and running. Salesforce immediately starts to breakdown a bit in this regard. First you&rsquo;re required for both lead and contact to provide a first and last name. In my case I do ask for name, and do a little bit of work on the code side to get values into both. You&rsquo;ll see later that our process does result in some regular cleanup work needing to happen, but in our case we&rsquo;re optimizing to get them signed up more than capturing every detail perfectly about them from the start.</p>

<p>Leads are even more broken than contacts though. Leads require you to enter a company. While you may be able to just drop a company form field onto your sign-up page you&rsquo;re likely to end up with junk data at least, if not actually driving some sign-ups away. Some of my favorite pieces of junk data I&rsquo;ve seen users enter for company name: &ldquo;pissed off developer&rdquo;, &ldquo;Acme Inc.&rdquo;, and the all too common &ldquo;Test Co.&rdquo;. In reality these are often real developers, with real problems, and real budget, they just don&rsquo;t want to share details before they&rsquo;re ready.</p>

<p>So in this case the TLDR; is that leads require:</p>

<ul>
<li>First name</li>
<li>Last name</li>
<li>Email</li>
<li>Company name</li>
</ul>


<p>This results in contacts being a more favorable datatype because it only requires:</p>

<ul>
<li>First name</li>
<li>Last name</li>
<li>Email</li>
</ul>


<h3>Accounts vs. Opportunities</h3>

<p>We have in some ways a similar but different dichotomy with Accounts and Opportunities as we did Leads and Contacts. Though this one can often map a bit more cleanly than we saw with leads. From a <a href="https://success.salesforce.com/answers?id=90630000000gnvcAAA">pretty straight forward definition</a>:</p>

<ul>
<li>Account &ndash; A business entity. Contacts work for Accounts.</li>
<li>Opportunities &ndash; Sales events related to an Account and one or more Contacts.</li>
</ul>


<p>This again can become problematic if you have no notion of Accounts at all in your system of record. Though if you are building a B2B application there is a good chance you may have something that makes sense. If you let uses free-form enter this instead of AT&amp;T they may put &ldquo;interactive team&rdquo;, but you at least have some logical team that in their mind they roll up to.</p>

<p>Opportunities is a much harder one in the SaaS world. In traditional marketing you have your standard stages of MQL (Marketing Qualified Lead), progressing to SQL (Sales Qualified Lead), etc. that you expect these potential customers to flow through. In the as-a-service world you may have people look from afar for weeks, then suddenly sign-up and give you a credit card and start paying within minutes. While there is still steps the customer may go through before buying you often have less insight into these. How you decide to structure your opportunities flow is entirely up to you. In my case I tend to opt to still have htem, but they&rsquo;re an exception basis where a salesperson is actively engaged vs. the other 90%+ of fully self-service customers.</p>

<p>Shifting back a little bit on accounts. The key with accounts is that if you have some notion of an team or org within your system of record then it makes sense to have that same structure setup in Salesforce. The most basic of this might be an idea of &ldquo;Account owner&rdquo; and &ldquo;Team members&rdquo;. You may have a person in there just for billing, an admin, and then users. Even if you don&rsquo;t want to recreate the entire structure at least having all the contacts tied to the account is critical. I can&rsquo;t count the number of times I&rsquo;ve seen teams setup a &ldquo;<a href="&#x6d;&#x61;&#105;&#x6c;&#x74;&#111;&#x3a;&#x62;&#105;&#108;&#x6c;&#105;&#110;&#x67;&#64;&#x6d;&#x79;&#99;&#111;&#109;&#112;&#97;&#110;&#121;&#x2e;&#99;&#111;&#109;">&#98;&#105;&#x6c;&#x6c;&#x69;&#110;&#x67;&#64;&#109;&#121;&#x63;&#111;&#109;&#x70;&#97;&#x6e;&#121;&#46;&#99;&#111;&#109;</a>&rdquo; email, seen people try to interact with that email when in reality they wanted to be talking to &ldquo;<a href="&#x6d;&#97;&#x69;&#x6c;&#x74;&#x6f;&#58;&#x6a;&#x61;&#110;&#101;&#x40;&#109;&#x79;&#99;&#111;&#109;&#x70;&#97;&#110;&#121;&#46;&#x63;&#x6f;&#x6d;">&#106;&#97;&#x6e;&#101;&#64;&#109;&#x79;&#99;&#x6f;&#109;&#x70;&#97;&#x6e;&#x79;&#x2e;&#x63;&#x6f;&#x6d;</a>&rdquo; who logged in yesterday.</p>

<h3>In summary</h3>

<p>For the most part Salesforce doesn&rsquo;t quite let you map to what many of you&rsquo;ll want to do in terms of mapping your data from your system of record to Salesforce. Expect to have to contort a bit and likely pump Salesforce with some garbage data. In general you&rsquo;ll want to skip leads and go straight for contacts as contacts don&rsquo;t require the same restrictions. Tying contacts to an account is the right level anyway, and from there up to you on how you&rsquo;ll more manage the opportunities.</p>

<p><img src="https://craig-pixeltracker.herokuapp.com/pixel.gif"/></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Open DNS for when DNS outages occur]]></title>
    <link href="http://www.craigkerstiens.com/2016/10/21/opendns/"/>
    <updated>2016-10-21T00:00:00-07:00</updated>
    <id>http://www.craigkerstiens.com/2016/10/21/opendns</id>
    <content type="html"><![CDATA[<p>Open DNS is a DNS resolver that caches records beyond their TTL if the upstream DNS server cannot be found. In cases like today&rsquo;s major outage it can be handy to swap your DNS settings out for this, or it may be worth using as a standard default. Resolution may be a bit slow as it will try to see if the upstream server cannot be found, but it at least can get you back to a working state.</p>

<p>If you know what you&rsquo;re doing then all you need to do is configure your DNS settings to: <code>208.67.220.220</code> and <code>208.67.222.222</code>.</p>

<p>If you need a little more guidance you can go into your System Preferences on Mac, select Network, then Advanced and finally the DNS tab. You should set it up to look as follows:</p>

<p><img src="https://d3vv6lp55qjaqc.cloudfront.net/items/0c2T1M251T0D3r1D2Q3x/Network.png?X-CloudApp-Visitor-Id=e4475d145dcf11ebcffabf840edcc11f&amp;v=cd767ce0" alt="DNS Configuration" /></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[A tour of Postgres' Foreign Data Wrappers]]></title>
    <link href="http://www.craigkerstiens.com/2016/09/11/a-tour-of-fdws/"/>
    <updated>2016-09-11T00:00:00-07:00</updated>
    <id>http://www.craigkerstiens.com/2016/09/11/a-tour-of-fdws</id>
    <content type="html"><![CDATA[<p>SQL can be a powerful language for reporting. Whether you&rsquo;re just exploring some data, or generating reports that show <a href="http://www.craigkerstiens.com/2014/02/26/Tracking-MoM-growth-in-SQL/">month over month revenue growth</a> it&rsquo;s the <a href="https://www.amazon.com/SQL-Relational-Theory-Write-Accurate/dp/1491941170/ref=sr_1_1?s=books&amp;ie=UTF8&amp;qid=1473612603&amp;sr=1-1&amp;keywords=sql+relational&amp;tag=mypred-20">lingua franca</a> for data analysis. But, your data isn&rsquo;t always in a SQL database, even then if you&rsquo;re using Postgres you can still likely use SQL to analyze, query, even joing with that data. Foreign data wrappers have been around for years in Postgres, but are continuing to mature and be a great option for joining disparate systems.</p>

<h3>Overview of foreign data wrappers</h3>

<p>If you&rsquo;re unfamiliar, foreign data wrappers, or FDW, allow you to connect from within Postgres to a remote system. Then you can query them from directly within Postgres. While there is an official Postgres FDW that ships with Postgres itself, that allows you to connect from one Postgres DB to another, there&rsquo;s also a broad community of others.</p>

<p>At the core of it Postgres provides certain APIs under the covers which each FDW extension can implement. This can include the ability to map SQL to whatever makes sense for a given system, push down various operators like where clauses, and as of Postgres 9.3 can even write data. <!--more--></p>

<p>To setup a FDW you first would install the extension, then provide the connection to the remote system, setup your schema/tables, and then you&rsquo;re off to the races–or well ready to query. If you&rsquo;ve got more than 2-3 databases or systems in your infrastructure, you&rsquo;ll often benefit from FDWs as opposed to introducing a heavyweight ETL pipeline. Don&rsquo;t mistake FDWs as the most performant method for joining data, but they are often the developer time efficient means of joining these data sets.</p>

<p>Let&rsquo;s look at just a few of the more popular and interesting ones.</p>

<h3>Postgres FDW</h3>

<p>The Postgres one is the easiest to get started with. First you&rsquo;ll just enable it with <code>CREATE EXTENSION</code>, then you&rsquo;ll setup your remote server:</p>

<pre><code>CREATE EXTENSION postgres_fdw;

CREATE SERVER core_db 
 FOREIGN DATA WRAPPER postgres_fdw 
 OPTIONS (host 'foo', dbname 'core_db', port '5432');
</code></pre>

<p>Then you&rsquo;ll create the user that has access to that database:</p>

<pre><code>CREATE USER MAPPING FOR bi SERVER core OPTIONS (user 'bi', password 'secret');
</code></pre>

<p>Finally, create your foreign table:</p>

<pre><code>CREATE FOREIGN TABLE core_users (
  id          integer NOT NULL,
  username    varchar(255),
  password    varchar(255),
  last_login  timestamptz
)
SERVER core;
</code></pre>

<p>Now you&rsquo;ll see a new table in the database you created this in called <code>core_users</code>. You can query this table just like you&rsquo;d expect:</p>

<pre><code>SELECT *
FROM core_users
WHERE last_login &gt;= now() - '1 day'::interval;
</code></pre>

<p>You can also join against local tables, such as getting all the invoices for users that have logged in within the last month:</p>

<pre><code>SELECT *
FROM invoices, core_users
WHERE core_users.last_login &gt;= now() - '1 month::interval'
  AND invoices.user_id = core_users.id
</code></pre>

<p>Hopefully this is all straight forward enough, but let&rsquo;s also take a quick look at some of the other interesting ones:</p>

<h3>MySQL FDW</h3>

<p>For MySQL you&rsquo;ll also have to <a href="https://github.com/EnterpriseDB/mysql_fdw">download it</a> and install it as well since it doesn&rsquo;t ship directly with Postgres. This should be fairly straight forward:</p>

<pre><code>$ export PATH=/usr/local/pgsql/bin/:$PATH
$ export PATH=/usr/local/mysql/bin/:$PATH
$ make USE_PGXS=1
$ make USE_PGXS=1 install
</code></pre>

<p>Now that you&rsquo;ve built it you&rsquo;d follow a very similar path to setting it up as we did for Postgres:</p>

<pre><code>CREATE EXTENSION mysql_fdw;

CREATE SERVER mysql_server
 FOREIGN DATA WRAPPER mysql_fdw
 OPTIONS (host '127.0.0.1', port '3306');

CREATE USER MAPPING FOR postgres
 SERVER mysql_server
 OPTIONS (username 'foo', password 'bar');

CREATE FOREIGN TABLE core_users (
  id          integer NOT NULL,
  username    varchar(255),
  password    varchar(255),
  last_login  timestamptz
 )
 SERVER mysql_server;
</code></pre>

<p>But MySQL while different than Postgres is also more similar in SQL support than say a more exotic NoSQL store. How well do they work as a foreign data wrapper? Let&rsquo;s look at our next one:</p>

<h3>MongoDB</h3>

<p>First you&rsquo;ll go through much of the <a href="https://github.com/EnterpriseDB/mongo_fdw">same setup</a> as you did for MySQL. The one major difference though is in the final step to setup the <code>table</code>. Since a table doesn&rsquo;t quite map in the same way with Mongo you have the ability to set two items: 1. the database and 2. the collection name.</p>

<pre><code>CREATE FOREIGN TABLE core_users(
     _id NAME,
     user_id int,
     user_username text,
     user_last_login timestamptz)
SERVER mongo_server
     OPTIONS (database 'db', collection 'users');
</code></pre>

<p>With this you can do some basic level of filtering as well:</p>

<pre><code>SELECT * 
FROM core_users
WHERE user_last_login &gt;= now() - '1 day'::interval;
</code></pre>

<p>You can also write and delete data as well now just using SQL:</p>

<pre><code>DELETE FROM core_users 
WHERE user_id = 100;
</code></pre>

<p>Of course just putting SQL on top of Mongo doesn&rsquo;t mean you get all the flexibility of analysis that you&rsquo;d have directly within Postgres, this does go a long way towards allowing you to analyze data that lives across two different systems.</p>

<h3>Many more</h3>

<p>A few years ago there were some key ones which already made FDWs useful. Now there&rsquo;s a rich list covering probably every system you could want. Whether it&rsquo;s <a href="http://www.craigkerstiens.com/2012/10/18/connecting_to_redis_from_postgres/">Redis</a>, a simple <a href="https://www.postgresql.org/docs/9.5/static/file-fdw.html">CSV</a> one, or something newer like <a href="https://github.com/snaga/monetdb_fdw">MonetDB</a> chances are you can find an <a href="https://wiki.postgresql.org/wiki/Foreign_data_wrappers#NoSQL_Database_Wrappers">FDW</a> for the system you need that makes your life easier.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Five mistakes beginners make when working with databases]]></title>
    <link href="http://www.craigkerstiens.com/2016/06/07/five-mistakes-databases/"/>
    <updated>2016-06-07T00:00:00-07:00</updated>
    <id>http://www.craigkerstiens.com/2016/06/07/five-mistakes-databases</id>
    <content type="html"><![CDATA[<p>When you start out as a developer there&rsquo;s an overwhelming amount of things to grasp. First there&rsquo;s the language itself, then all the quirks of the specific framework you&rsquo;re using,and after that (or maybe before) we&rsquo;ll throw front-end development into the mix, and somewhere along the line you have to decide to store your data somewhere.</p>

<p>Early on, with so many things to quickly master, the database tends to be an after-though in application design (perhaps because it doesn&rsquo;t make an impact to end user experience). As a result there&rsquo;s a number of bad practices that tend to get picked up when working with databases, here&rsquo;s a rundown of just a few.</p>

<!--more-->


<h3>1. Storing images</h3>

<p>Images don&rsquo;t belong in your database. Just because you can do something, it doesn&rsquo;t mean you should.Images take up a massive amount of space in databases, and slow applications down by unnecessarily eating your database&rsquo;s IO resources. The most common way this mistake occurs is when new developers base64 encode an image and store it in a database large text/blob field.</p>

<p>The better approach is to upload your images directly to a service like Amazon S3, then store the image URL (hosted by Amazon) in your database as a text field. This way, each time you need to load an image, you need to simply output the image URL into a valid <code>&lt;img&gt;</code> tag. This will greatly improve website responsiveness, and generally help scale web applications.</p>

<h3>2. Limit/Offset</h3>

<p>Pagination is extremely common in a number of applications.As soon as you start to learn SQL, the most straight-forward way to handle pagination is to <code>ORDER BY</code> some column then <code>LIMIT</code> the number of results returned, and for each extra page you&rsquo;ll <code>OFFSET</code> by so many records. This all seems entirely logical, until you realize at any moderate scale:</p>

<ol>
<li>The load this exerts on your database will be painful.</li>
<li>It isn&rsquo;t deterministic, should records change as the user flips between pages.</li>
</ol>


<p>The unfortunate part is: pagination is quite complex, and there isn&rsquo;t a one-size-fits-all solution. For more information on solving pagination problems, you can check <a href="https://www.citusdata.com/blog/1872-joe-nelson/409-five-ways-paginate-postgres-basic-exotic">out numerous options</a></p>

<h3>3. Integer primary keys</h3>

<p>The default for almost all ORMs when creating a primary key is to create a serial field. This is a sequence that auto-increments and then you use that number as your primary key. This seems straight forward as an admin, because you can browse from /users/1 to /users/2, etc. And for most applications this can often be fine. And for most applications, this is fine. But, you&rsquo;ll soon realize as you start to scale that integers primary keys can be exhausted, and are not ideal for large-scale systems. Further you&rsquo;re reliant on that single system generating your keys. If a time comes when you have to scale the pain here will be huge. The better approach is to start <a href="https://til.hashrocket.com/posts/31a5135e19-generate-a-uuid-in-postgresql">taking advantage of UUIDs</a> from the start.</p>

<p><em>There&rsquo;s also the bonus advantage of not secretly showcasing how many users/listings/whatever the key references directly to users on accident.</em></p>

<h3>4. Default values on new columns</h3>

<p>No matter how long you&rsquo;ve been at it you won&rsquo;t get the perfect schema on day 1. It&rsquo;s better to think of database schemas as continuously evolving documents. Fortunately, it&rsquo;s easy to add a column to your database, but: it&rsquo;s also easy to do this in a horrific way. By default, if you just add a column it&rsquo;ll generally allow NULL values. This operation is fast, but most applications don&rsquo;t truly want null values in their data, instead they want to set the default value.</p>

<p>If you do add a column with a default value on the table, this will trigger a full re-write of your table. <em>Note: this is very bad for any sizable table on an application.</em> Instead, it&rsquo;s far better to allow null values at first so the operation is instant, then set your default, and then, with a background process go and retroactively update the data.</p>

<p>This is more complicated than it should be, but fortunately there are some <a href="http://pedro.herokuapp.com/past/2011/7/13/rails_migrations_with_no_downtime/">handy guides</a> to help.</p>

<h3>5. Over normalization</h3>

<p>As you start to learn about normalization it feels like the right thing to do. You create a <code>posts</code> table, which contains <code>authors</code>, and each post belongs in a category. So you create a <code>categories</code> table, and then you create a join table <code>post_categories</code>. At the real root of it there&rsquo;s not anything fundamentally wrong with normalizing your data, but at a certain point there are diminishing returns.</p>

<p>In the above case categories could very easily just be an array of varchar fields on a post. Normalization makes plenty of sense, but thinking through it a bit more every time you have a many to many table and wondering if you really need a full table on both sides is worth giving a second thought.</p>

<p><em>Edit: It&rsquo;s probably worth saying that under-normalization is also a problem as well. There isn&rsquo;t a one size fits all here. In general there are times where it does make sense to have a completely de-normalized and a completely normalized approach. As <a href="https://twitter.com/fuzzychef/status/740248400243785728">@fuzzychef</a> described: &ldquo;use an appropriate amount of normalization i.e. The goldilocks principle&rdquo;</em></p>

<h3>Conclusion</h3>

<p>When I asked about this on twitter I got a pretty great responses, but they were all over the map. From the basics of never looking at the queries the ORM is generating, to much more advanced topics such as isolation levels. The one I didn&rsquo;t hit on that does seem to be a worthwhile one for anyone building a real world app is indexing. Knowing how <a href="http://www.craigkerstiens.com/2012/10/01/understanding-postgres-performance/">indexing works</a>, and understanding <a href="http://www.craigkerstiens.com/2013/05/30/a-collection-of-indexing-tips/">what indexes</a> you need to create is a critical part of getting good database performance. There&rsquo;s a number of posts on indexing that teach the basics, as well as <a href="http://www.craigkerstiens.com/2013/01/10/more-on-postgres-performance/">practical steps</a> for analyzing performance with Postgres.</p>

<p>In general, I encourage you to treat the database as another tool in your chest as opposed to a necessary evil, but hopefully, the above tips will at least prevent you from making some initial mistakes as you dig in as a beginner.</p>

<p><em>Special thanks to <a href="https://twitter.com/mdeggies">@mdeggies</a> and <a href="https://twitter.com/rdegges">@rdegges</a> for the initial conversation to spark the post at PyCon.</em></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hands On Postgres Sharding]]></title>
    <link href="http://www.craigkerstiens.com/2016/02/28/Hands-on-postgres-sharding/"/>
    <updated>2016-02-28T00:00:00-08:00</updated>
    <id>http://www.craigkerstiens.com/2016/02/28/Hands-on-postgres-sharding</id>
    <content type="html"><![CDATA[<p><em><strong>Notice</strong>: Much of this post still applies, but now applies more directly to <a href="https://www.citusdata.com">Citus</a>. Since this post originally published, pg_shard is now deprecated. Citus now has an open source version which offer a superset of the features of pg_shard, as well as a <a href="https://www.citusdata.com/products/cloud">cloud</a> offering. Finally you can find some further guidance for sharding on the <a href="https://www.citusdata.com/blog/">Citus blog</a> and <a href="http://docs.citusdata.com">docs</a></em></p>

<p>Back in 2012 I wrote an overview of database sharding. Since then I&rsquo;ve had a few questions about it, which have really increased in frequency over the last two months. As a result I thought I&rsquo;d do a deeper dive with some actual hands on for sharding. Though for this hands on, because I do value my time I&rsquo;m going to take advantage of <code>pg_shard</code> rather than creating mechanisms from scratch.</p>

<p>For those unfamiliar <a href="https://github.com/citusdata/pg_shard/">pg_shard</a> is an open source extension from <a href="http://citusdata.com">Citus data</a> who has a commerical product that you can think of is pg_shard++ (and probably much more). Pg_shard adds a little extra to let data automatically distribute to other Postgres tables (logical shards) and Postgres databases/instances (physical shards) thus letting you outgrow a single Postgres node pretty simply.</p>

<p>Alright, enough talk about it, let&rsquo;s get things up and running.</p>

<!--more-->


<h3>Build, install</h3>

<p><em>The rest assume you have Postgres.app, version 9.5 setup and are on a Mac, much of these steps could be easily adapted for other Postgres installs or OSes.</em></p>

<p>PATH=/Applications/Postgres.app/Contents/Versions/latest/bin/:$PATH make</p>

<p>sudo PATH=/Applications/Postgres.app/Contents/Versions/latest/bin/:$PATH make install</p>

<p>cp /Applications/Postgres.app/Contents/Versions/9.5/share/postgresql/postgresql.conf.sample /Applications/Postgres.app/Contents/Versions/9.5/share/postgresql/postgresql.conf.sample</p>

<p>Edit your <code>postgresql.conf</code>:</p>

<pre><code>#shared_preload_libraries = ''
</code></pre>

<p>TO:</p>

<pre><code>shared_preload_libraries = 'pg_shard'
</code></pre>

<p>Then create a file in <code>/Users/craig/Library/Application\ Support/Postgres/var-9.5/pg_worker_list.conf</code> where <code>craig</code> is your username:</p>

<pre><code># hostname port-number
localhost  5432
localhost  5433
</code></pre>

<p>You&rsquo;ll also need to create a new Postgres instance:</p>

<pre><code>initdb -D /Users/craig/Library/Application\ Support/Postgres/var-9.5-2
</code></pre>

<p>Then edit that <code>postgresql.conf</code> inside that newly created folder with two main edits:</p>

<pre><code>port = 5432
</code></pre>

<p>To</p>

<pre><code>port = 5433
</code></pre>

<p>Finally setup our database then start it up:</p>

<pre><code>createdb instagram
postgres -D /Users/craig/Library/Application\ Support/Postgres/var-9.5-2
</code></pre>

<h3>Setup</h3>

<p>Now you should have two running instances of Postgres, now let&rsquo;s finally turn on the pg_shard extension, create some tables and see what we have. First connect to your main running Postgres instance, so in this case the the instagram database we first created <code>psql instagram</code>, then let&rsquo;s set things up:</p>

<pre><code>CREATE EXTENSION pg_shard;
CREATE TABLE customer_reviews (customer_id TEXT NOT NULL, review_date DATE, review_rating INTEGER, product_id CHAR(10));

 CREATE TABLE
 Time: 4.734 ms

SELECT master_create_distributed_table(table_name := 'customer_reviews',                                                                                                     partition_column := 'customer_id');

 master_create_distributed_table
 ---------------------------------

 (1 row)

SELECT master_create_worker_shards(table_name := 'customer_reviews',                                                                                                     shard_count := 16,                                                                                                                                        replication_factor := 2);

 master_create_worker_shards
 -----------------------------

 (1 row)
</code></pre>

<h3>Understanding and using</h3>

<p>So that was a lot of initial setup. But now we have an application that could in theory scale to a shared application across 16 instances. If you want a refresher, there&rsquo;s a difference between physical and logical shards. In this case above we have 16 logical ones and it&rsquo;s replicated across 2 physical Postgres instances albeit on the same instance.</p>

<p>Alright so a little more poking under the covers to see what happened before we actually start doing something with our data. If you&rsquo;re still connected go ahead and run <code>\d</code>, and you should see:</p>

<pre><code>                List of relations
 Schema |          Name          | Type  | Owner
--------+------------------------+-------+-------
 public | customer_reviews       | table | craig
 public | customer_reviews_10000 | table | craig
 public | customer_reviews_10001 | table | craig
 public | customer_reviews_10002 | table | craig
 public | customer_reviews_10003 | table | craig
 public | customer_reviews_10004 | table | craig
 public | customer_reviews_10005 | table | craig
 public | customer_reviews_10006 | table | craig
 public | customer_reviews_10007 | table | craig
 public | customer_reviews_10008 | table | craig
 public | customer_reviews_10009 | table | craig
 public | customer_reviews_10010 | table | craig
 public | customer_reviews_10011 | table | craig
 public | customer_reviews_10012 | table | craig
 public | customer_reviews_10013 | table | craig
 public | customer_reviews_10014 | table | craig
 public | customer_reviews_10015 | table | craig
(17 rows)
</code></pre>

<p>You can see that under the cover there&rsquo;s a lot more <code>customer_reviews</code> tables, in reality you don&rsquo;t have to think about these or do anything with them. But just for reference they&rsquo;re just plain ole Postgres tables under the cover. You can query them and poke at the data. The now mystical <code>customer_reviews</code> will actually roll up the data across all your logical shards (tables) and physical shards (spanning across machines).</p>

<p><em>It&rsquo;s also of note that in production you might not actually use your primary DB as a worker, we did this more for expediency in setting it up on a local Mac. More typically you&rsquo;d have 2 or more workers which are not the same a the primary, these were the ports we setup in our <code>pg_worker_list.conf</code>.</em>    A common setup would look something more like:</p>

<p><img src="https://s3.amazonaws.com/f.cl.ly/items/3T2N2Q1K041g0a0L0j03/Untitled.png?v=7df00f6b" alt="" /></p>

<p>So now start inserting away:</p>

<pre><code>INSERT INTO customer_reviews (customer_id, review_rating) VALUES ('HN802', 5);
INSERT INTO customer_reviews (customer_id, review_rating) VALUES ('FA2K1', 10);
</code></pre>

<p>For extra homework on your own you can now go and poke at where the underlying data actually surfaced.</p>

<h3>Conclusion</h3>

<p>Yes, there&rsquo;s a number of limitations that you can learn a bit more about over on the <a href="https://github.com/citusdata/pg_shard#limitations">github repo for pg_shard</a>. Though even with those it&rsquo;s very usable as is, and let&rsquo;s you get quite far in prepping an app for sharding. While I will say that all apps think they&rsquo;ll need sharding and few actually do, given <code>pg_shard</code> it&rsquo;s minimal extra effort now to plan for such scaling should you need it.</p>

<p>Up next we&rsquo;ll look at how it&rsquo;d work with a few languages, so you can get an idea of the end to end experience.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[What being a PM is really like - Software is easy, People are hard]]></title>
    <link href="http://www.craigkerstiens.com/2016/01/28/On-being-a-PM-software-is-easy-people-are-hard/"/>
    <updated>2016-01-28T00:00:00-08:00</updated>
    <id>http://www.craigkerstiens.com/2016/01/28/On-being-a-PM-software-is-easy-people-are-hard</id>
    <content type="html"><![CDATA[<p>In recent months I&rsquo;ve had the question nearly once a week about advice/tips for becoming a Product Manager or more commonly referred to as PM. These are generally coming from people that are either currently engineers, or previously were and are in some engineer/customer role such as a sales engineer or solution architect. There&rsquo;s a number of <a href="http://www.amazon.com/Inspired-Create-Products-Customers-Love/dp/0981690408?tag=mypred-20">high level</a> pieces talking about PM and it often feels glorious, I mean you get to make product decisions right? You get to call some shots. Well that sometimes may be true, but don&rsquo;t assume it&rsquo;s all rainbows and sparkles.</p>

<p>Especially as a first time PM what your day to day will look like won&rsquo;t be debating strategy all day long. Here&rsquo;s a few of the good and the bad sides of being a PM.</p>

<!--more-->


<h3>Plenty of grunt work</h3>

<p>While you may get to make a decision or two, the bulk of your time will not be thinking about grandiose visions, instead you&rsquo;ll be doing a lot to gather data. There&rsquo;s a lot of means for gathering data across lots of sources, the more you use the better you&rsquo;ll be. Knowing the ones you steer towards, as well as ones you steer away from is useful so you can balance a bias more fairly. For myself SQL is a go-to, then customer interactions both qualitative and quantitative such as surveys, following what media is saying about your space is important as well. And while user studies are often relegated to design and UX, as a PM you need to make sure it at least happens (<a href="http://www.invisionapp.com/">Invision App</a> is a favorite for lightweight tests).</p>

<p>In a given week I probably spend 10 hrs interacting with customers, looking at data, and sadly that&rsquo;s probably not enough.</p>

<p><em>A few practical examples of this</em></p>

<p>Each morning I send emails to 10-20 users who used the product for the first time, yes this is automated but carving out 30 minutes of my day to actually follow-up with each of them is less automated.</p>

<p>Another example is keeping a health of business dashboard up to date. Personally I use google sheets for this. Within one spreadsheet I have monthly and weekly targets as well as how we’re tracking against them. These are all updated on actual real time data, powered by Heroku’s dataclips with a simple <code>=importCSV(‘http://dataclips.heroku.com/abcdefghij….csv’)</code>. In total my google spreadsheets has 1 high level overview, with about 20 underlying sheets that do all of the computations. In any given month 1 of my key 4-5 goals may be missed, which then spawns digging in deeper to figure out why and what we can do about it.</p>

<h3>Dictating vs. consensus</h3>

<p>From a product decision making perspective you can force alignment by explicitly making every decision, or you can allow decisions to be made as a group voting if needed. Expect to use some balance of both of these among the team, and neither is never perfect. When it comes to outside the team you may still use both, but steer more strongly one way or the other. For example with the executive team it may be more consensus, with marketing it may be dictating your product roadmap which they can help support.</p>

<p>Even within the team there will be times a decision must be made and there will be some people that don&rsquo;t align. It&rsquo;s key that you make the decision clearly and explicitly. Even though some individuals don&rsquo;t like it, they won&rsquo;t fight against it&hellip; unless you make a habit of taking the input, then discarding it and going along your &lsquo;intuition&rsquo;. Even when there&rsquo;s a strong case based on the data it may not be as clear as you think.</p>

<p>In contrast, decision making by consensus most people will feel happier that they provided input into the product. If you take everyone&rsquo;s input expect to end up with a product that feels like 10 people designed it, needless to say incoherent.</p>

<p>As a PM expect to do a lot of listening, a good bit of convincing, and some occasional big decision making.</p>

<h3>The pain you feel inside the building doesn&rsquo;t matter</h3>

<p>You may think you&rsquo;re solving a problem that exists for users, when in reality there was no problem at all. This is just a reminder that you need to keep empathy in mind above so much else.</p>

<p>As an example, once a data team put in place a tool, supposedly for me. I looked at the tool and more or less didn&rsquo;t understand why it was in place. They proceeded to explain that my problem was it took me too long to write SQL, so this tool will help me get the reports I need without SQL. At that point I proceeded to actually list off all the issues I did have, none of which <a href="http://www.craigkerstiens.com/categories/postgres/">were SQL</a>.</p>

<p>Prescribing a solution without knowing clearly <strong>from customers</strong> what the problem is will leave you in a bad spot. All this means you have to set aside building the tool you want to use, and make sure you know what the <a href="http://headrush.typepad.com/creating_passionate_users/2005/01/keeping_users_e.html">customer wants</a>.</p>

<h3>Marketing is your job</h3>

<p>Not external marketing, though often that work may still fall to you, but rather internal marketing.</p>

<p>It’s important that you internally market the wins for your <em>team</em>. These wins should very much be for the team, and not for your own benefit. The best PMs seem to disappear into the background, this is because you’re more surfacing all the work your team is doing than any of the details you helped coordinate. This is often counter to our natural instinct to tout our own accomplishments. This is only exaggerated in PM role, one where things can still ship if you’re not there, so there can often be a tendency to try to highlight the roles value. Fight that urge to self market.</p>

<p><img src="https://s3.amazonaws.com/f.cl.ly/items/412j330z173E1d432M05/Untitled.png?v=4ceec9d2" style="float:right; width:50%; margin-left:20px;" /></p>

<p>Rest assured though–getting the team focused on solving the right problems, and then surfacing their wins will only help you go faster.</p>

<h3>On leading</h3>

<p><img src="https://s3.amazonaws.com/f.cl.ly/items/3E2c3k1p1j2g2O1O0b0L/Untitled.png?v=9daf9e58" style="float:right; width:50%; margin-left:20px;margin-bottom:15px;" />
You’ll have to lead at times, follow at others, and all of this will likely be with no reports. Don’t confuse a lack of engineers reporting to you (which they should not, a common team structure can be seen on the right) for not needing to lead the team at times. <em>For those less familiar it’s very rare for a PM to have engineers report to them, though occasionally designers will. It’s typically the cross-reporting structure you’ll find talked about by Andy Grove in <a href="http://www.amazon.com/High-Output-Management-Andrew-Grove/dp/0679762884?tag=mypred-20">High output management</a>.</em> If the team is firing on all cylinders, knows the direction, and executing then step back and let things roll forward. If that’s not the case and leadership around direction or otherwise is needed be prepared to step up.</p>

<p>At the end of the day ensuring the product is advancing is your job, so be prepared to do what you need to whether it’s leading or not to accomplish that.</p>

<h3>It’s not all rainbows, but it is fun</h3>

<p>The range of things you’ll have to focus on can be diverse and complex. In the end if you get a rush out of shipping and launching products, then all the work that goes into it can make it all worthwhile. It’s as much about figuring out what customers want and then getting your team building the right thing. For a first time PM it can be summed up by the notion that software is easy, people are hard.</p>

<p><em>Special thanks to <a href="http://www.twitter.com/lukasfittl">Lukas Fittl</a> and <a href="http://www.twitter.com/iamclovin">Arun Thampi</a> for reviews and feedback on this post.</em></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Marketing definitions for developers]]></title>
    <link href="http://www.craigkerstiens.com/2016/01/17/marketing-definitions-for-developers/"/>
    <updated>2016-01-17T00:00:00-08:00</updated>
    <id>http://www.craigkerstiens.com/2016/01/17/marketing-definitions-for-developers</id>
    <content type="html"><![CDATA[<p>Marketing often feels like a dirty-icky thing to many developers. Well until you feel like you have a great product, but no one using it then you have to get a crash course in all of that. And while I might cover some of the actual basics in the future, just knowing what marketing people actually mean when they’re talking can be a huge jump start. Here&rsquo;s a guide that distills many of the acronyms and terms down to what they actually mean in reality.</p>

<!--more-->


<p><strong>SEO &ndash; Search engine optimization.</strong> There&rsquo;s two sides to this, one where you&rsquo;re attempting to game the system known known as black hat. The other is simply creating good content.</p>

<p>Tip: Now, unlike several years ago social sharing helps impact this.</p>

<p><strong>SEM &ndash; Search engine marketing.</strong> The short of this is adwords, but broadly it&rsquo;s any search engine.</p>

<p>Tip: Be wary here, you can spend a lot of money quickly. Properly managing it takes time and effort otherwise you&rsquo;re wasting money.</p>

<p><strong>Display ads</strong> &ndash; Banner ads on websites. There&rsquo;s a few common form factors in this world, so you&rsquo;ll create a few then reuse them across lots of properties.</p>

<p>Tip: Results may vary here, there are some hidden gems when advertising on various long tail sites.</p>

<p><strong>Retargeting</strong> &ndash; This is where you&rsquo;re serving an ad (most commonly display) to someone that&rsquo;s previously visited your site. The process happens due to you &lsquo;pixeling&rsquo; them, and a cookie being set so the ad server knows they&rsquo;ve seen you.</p>

<p>Tip: Generally good bang for the buck here, but you still need initial visitors to even retarget to.</p>

<p><strong>Funnel</strong> &ndash; The process of someone going from finding you to paying to paying more. Generally a process will look something like: Anonymous visitor by referral, sign ups, low money bucket, big money bucket.</p>

<p><strong>Top of the funnel</strong> &ndash; Hopefully clear from the previous one, top of the funnel would be the max of users you reach out to or get to your site, usually down to getting them to sign up.</p>

<p><strong>Bottom of the funnel</strong> &ndash; This is usually going from time you have a user to customer and then growing that customer via cross-selling and upselling.</p>

<p><strong>Drip marketing</strong> &ndash; This is the process of gradually sending emails/notifications to your customers to get them to engage and learn about the product. Think of it as a welcome email on day 1, an intro on day 3, and on day 5 a different email based on what they&rsquo;ve done so far. Really good drip marketing will create a different email for the user based on what they have or haven&rsquo;t done.</p>

<p><strong>Attribution (last/first/multi)</strong> &ndash; Attribution relates to how you got the user or customer (via web referral). There&rsquo;s a few different ways of looking at this, last touch is the last website they visited before signing up, first touch is the first referral they were sent from, and multi touch (often more complicated to put in place) attributes something to all referrals they&rsquo;ve come from.</p>

<p><strong>AR &ndash; Analyst relations</strong>. Analysts cover particular products or areas in an industry, write reports, and often consult with large enterprises when making buying decisions. Analyst relations or AR is the common term for interacting with them, you can <a href="http://www.craigkerstiens.com/2015/07/25/A-guide-to-analyst-relations-for-startups/">learn more here</a></p>

<p><strong>PR &ndash; Public relations</strong>. This is generally the press/media side. It often involves launches, press releases, pitching media etc. You can read more of a <a href="http://www.craigkerstiens.com/2015/07/21/An-intro-PR-guide-for-startups/">guide on it here</a></p>

<p><strong>Briefing</strong> &ndash; This is normally with an analyst or press, and is typically a quick 30 minute call occasionally a demo of an upcoming launch.Usually</p>

<p><strong>Inquiry</strong> &ndash; This refers more to the analyst side. Where a briefing is often more one sided for you to pitch/update them on what you’ve been doing, an inquiry is more a back and forth where you can ask what they’re seeing in the market and for input on direction.</p>

<p><strong>Campaign</strong> &ndash; A collection of activities that go on around a certain thing focused on specific keywords or theme. This can be as little as a search engine marketing campaign which is the most common, or much larger and coordinated with billboards, webinars, etc.</p>

<p><strong>Lead Gen</strong> &ndash; The sales funnel usually goes from just getting an email, to talking to them, to getting them to try a demo or run a POC, to eventually buying. Lead gen is the activity of just getting that initial contact so you can then further engage with them. In practice this can often involve giving something away, like a t-shirt in exchange for an email.</p>

<h3>Conclusion</h3>

<p>While this doesn&rsquo;t cover every marketing activity under the sun, hopefully it&rsquo;s a good primer on things you may have heard but been confused by. If there&rsquo;s important ones I&rsquo;ve missed please feel free to let me know <a href="http://www.twitter.com/craigkerstiens">@craigkerstiens</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Writing more legible SQL]]></title>
    <link href="http://www.craigkerstiens.com/2016/01/08/writing-better-sql/"/>
    <updated>2016-01-08T00:00:00-08:00</updated>
    <id>http://www.craigkerstiens.com/2016/01/08/writing-better-sql</id>
    <content type="html"><![CDATA[<p>A number of times in a crowd I&rsquo;ve asked how many people enjoy writing SQL, and often there&rsquo;s a person or two. The follow up is how many people enjoy reading other people&rsquo;s SQL and that&rsquo;s unanimously 0. The reason for this is that so many people write bad SQL. It&rsquo;s not that it doesn&rsquo;t do the job, it&rsquo;s just that people don&rsquo;t tend to treat SQL the same as other languages and don&rsquo;t follow strong code formatting guidelines. So, of course here&rsquo;s some of my own recommendations on how to make SQL more readable.</p>

<!--more-->


<h3>One thing per line</h3>

<p>Only put a single column/table/join per line. This is going to make for slightly more verbose SQL, but it will be easier to read and edit.. Here&rsquo;s a basic example:</p>

<pre><code>SELECT foo,
       bar
FROM baz
</code></pre>

<h3>Align your projections and conditions</h3>

<p>You can somewhat see this in the above with <code>foo</code> and <code>bar</code> being on the same line. This is reasonably common for columns you&rsquo;re selecting, but it&rsquo;s not applied as often in <code>AND</code> or <code>GROUP BY</code> clauses. As you can see there is a difference though between:</p>

<pre><code>SELECT foo,
       bar
FROM baz
WHERE foo &gt; 3
AND bar = 'craig.kerstiens@gmail.com'
</code></pre>

<p>And a cleaner version:</p>

<pre><code>SELECT foo,
       bar
FROM baz
WHERE foo &gt; 3
  AND bar = 'craig.kerstiens@gmail.com'
</code></pre>

<h3>Use column names when grouping/ordering</h3>

<p>This is personally an awful habit of mine, but it is extremely convenient to just order by the column number. In the above query we could just <code>ORDER BY 1</code>. This is especially easy when column 1 may be something like SUM(foo). However, ensuring you explicitly <code>ORDER BY SUM(foo)</code> will help limit any misunderstanding of the data.</p>

<h3>Comments</h3>

<p>You comment your code all the time, yet so few seem to comment their queries. A simple <code>--</code> allows you to inline a comment, perhaps where there&rsquo;s some oddities to what you&rsquo;re joining or just anywhere it may need clarification. You can of course <a href="http://www.craigkerstiens.com/2013/07/29/documenting-your-postgres-database/">go much further</a>, but at least some basic level of commenting should be required.</p>

<h3>Casing</h3>

<p>As highlighted in these examples, having a standard for how you case your queries is especially handy. Sticking with all SQL keywords in caps allows you to easily parse what is SQL and what are columns or literals that you&rsquo;re using in queries.</p>

<h3>CTEs</h3>

<p>First, yes they can be an optimisation boundary. But they can also make your query much more read-able and prevent you from doing the wrong thing because you couldn&rsquo;t reason about a query.</p>

<p>For those unfamiliar CTEs are like a view that exist just for the duration of that query being executed. You can have them reference previous CTEs so you can gradually build on them, much like you would code blocks. I won&rsquo;t repeat too much of what <a href="http://www.craigkerstiens.com/2013/11/18/best-postgres-feature-youre-not-using/">I&rsquo;ve already written about them</a>, but if you&rsquo;re unfamiliar with them or not using them <a href="http://www.craigkerstiens.com/2013/11/18/best-postgres-feature-youre-not-using/">they are a must</a>. CTEs are easily one of the few pieces of SQL that I use on a daily basis.</p>

<h3>Conclusion</h3>

<p>Of course this isn&rsquo;t the only way to make your SQL more readable and this isn&rsquo;t an exhaustive list. But hopefully you find these tips helpful, and for your favorite tip that I missed&hellip; let me know about it <a href="http://www.twitter.com/craigkerstiens">@craigkerstiens</a>.</p>

<p><em>A special thanks to <a href="http://www.twitter.com/Case">@Case</a> for reviewing.</em></p>

<script type="text/javascript">
  (function() {
    window._pa = window._pa || {};
    var pa = document.createElement('script'); pa.type = 'text/javascript'; pa.async = true;
    pa.src = ('https:' == document.location.protocol ? 'https:' : 'http:') + "//tag.marinsm.com/serve/517fd07cf1409000020002dc.js";
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(pa, s);
  })();
</script>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[My top 10 Postgres features and tips for 2016]]></title>
    <link href="http://www.craigkerstiens.com/2015/12/29/my-postgres-top-10-for-2016/"/>
    <updated>2015-12-29T00:00:00-08:00</updated>
    <id>http://www.craigkerstiens.com/2015/12/29/my-postgres-top-10-for-2016</id>
    <content type="html"><![CDATA[<p>I find during the holiday season many pick up <a href="http://www.amazon.com/Hard-Thing-About-Things-Building/dp/0062273205/ref=sr_1_1?ie=UTF8&amp;qid=1451407536&amp;sr=8-1&amp;keywords=hard+thing+about&amp;tag=mypred-20">new books</a>, learn a <a href="http://crystal-lang.org/">new language</a>, or brush up on some other skill in general. Here&rsquo;s my contribution to hopefully giving you a few new things to learn about Postgres and ideally utilize in the new year. It&rsquo;s not in a top 10 list as much as 10 tips and tricks you should be aware of as when you need them they become incredibly handy. But, first a shameless plug if you find any of the following helpful, consider subscribing to <a href="http://www.postgresweekly.com">Postgres weekly</a> a weekly newsletter with interesting Postgres content.</p>

<!--more-->


<h3>1. CTEs &ndash; Common Table Expressions</h3>

<p>CTEs allow you to do crazy awesome things like recursive queries but even the most simple form of them I don&rsquo;t go a day without using. Think of a CTE or commonly known as with clause as a view inside the time that query is running. This lets you more easily create readable query. Any query that&rsquo;s constructed that&rsquo;s even <a href="http://www.craigkerstiens.com/2013/11/18/best-postgres-feature-youre-not-using/">100 lines long</a>, but with 4-5 CTEs is undoubtedly going to be easier for someone new to come in and understand than a 20 line query that does the same thing. A few people like writing SQL, but no one likes reading someone else&rsquo;s so do them a favor and read up on CTEs.</p>

<h3>2. Setup a .psqlrc</h3>

<p>You setup a bashrc, vimrc, etc. Why not do the same for Postgres. Some of the great things you can do:</p>

<ul>
<li>Setup pretty formatting by default with <code>\x auto</code></li>
<li>Set nulls to actually look like something <code>\pset null ¤</code></li>
<li>Turn timing on by default <code>\timing on</code></li>
<li>Customize your prompt <code>\set PROMPT1 '%[%033[33;1m%]%x%[%033[0m%]%[%033[1m%]%/%[%033[0m%]%R%# '</code></li>
<li>Save commonly run queries that you can run by name</li>
</ul>


<p>Here&rsquo;s an example of my own <code>psqlrc</code>:</p>

<pre><code>\set QUIET 1
\pset null '¤'

-- Customize prompts
\set PROMPT1 '%[%033[1m%][%/] # '
\set PROMPT2 '... # '

-- Show how long each query takes to execute
\timing

-- Use best available output format
\x auto
\set VERBOSITY verbose
\set HISTFILE ~/.psql_history- :DBNAME
\set HISTCONTROL ignoredups
\set COMP_KEYWORD_CASE upper
\unset QUIET
</code></pre>

<h3>3. pg_stat_statements for where to index</h3>

<p><code>pg_stat_statements</code> is probably the single most valuable tool for improving performance on your database. Once enabled (with <code>create extension pg_stat_statements</code>) it automatically records all queries run against your database and records often and how long they took. This allows you to then go and find areas you can optimize to get overall time back with one simple query:</p>

<pre><code>SELECT 
  (total_time / 1000 / 60) as total_minutes, 
  (total_time/calls) as average_time, 
  query 
FROM pg_stat_statements 
ORDER BY 1 DESC 
LIMIT 100;
</code></pre>

<p><em>Yes, there is some performance cost to leaving this always on, but it&rsquo;s pretty small. I&rsquo;ve found it&rsquo;s far more useful to be on and get major performance wins vs. the small cost of it always recording.</em></p>

<p>You can read much more on Postgres performance on a <a href="http://www.craigkerstiens.com/2013/01/10/more-on-postgres-performance/">previous post</a></p>

<h3>4. Slow down with ETL, use FDWs</h3>

<p>If you have a lot of <em>microservices</em> or different apps then you likely have a lot of different databases backing them. The default for about anything you want to do is do create some data warehouse and ETL it all together. This often goes a bit too far to the extreme of aggregating <strong>everything</strong> together.</p>

<p>For the times you just need to pull something together once or on rare occasion <a href="http://www.craigkerstiens.com/2013/08/05/a-look-at-FDWs/">foreign data wrappers</a> will let you query from one Postgres database to another, or potentially from Postgres to anything else such as <a href="https://github.com/citusdata/mongo_fdw">Mongo</a> or Redis.</p>

<h3>5. array and array_agg</h3>

<p>There&rsquo;s little chance if you&rsquo;re building an app you&rsquo;re not using arrays somewhere within it. There&rsquo;s no reason you shouldn&rsquo;t be doing the same within your database as well. Arrays can be just another datatype within Postgres and have some great use cases like tags for blog posts directly in a single column.</p>

<p>But, even if you&rsquo;re not using arrays as a datatype there&rsquo;s often a time when you want to rollup something like an array in a query then comma separate it. Something similar to the following could allow you to easily roll up a comma separated list of projects per user:</p>

<pre><code>SELECT 
  users.email,
  array_to_string(array_agg(projects.name), ',')) as projects
FROM
  projects,
  tasks,
  users
WHERE projects.id = tasks.project_id
  AND tasks.due_at &gt; tasks.completed_at
  AND tasks.due_at &gt; now()
  AND users.id = projects.user_id
GROUP BY 
  users.email
</code></pre>

<h3>6. Use materialized views cautiously</h3>

<p>If you&rsquo;re not familiar with materialized view they&rsquo;re a query that has been actually created as a table. So it&rsquo;s a materialized or basically snapshotted version of some query or &ldquo;view&rdquo;. In their initial version materialized versions, which were long requested in Postgres, were entirely unusuable because when you it was a locking transaction which could hold up other reads and acticities avainst that view.</p>

<p>They&rsquo;ve since gotten much better, but there&rsquo;s no tooling for refreshing them out of the box. This means you have to setup some scheduler job or cron job to regularly refresh your materialized views. If you&rsquo;re building some reporting or BI app you may undoubtedly need them, but their usability could still be advanced so that Postgres knew how to more automatically refresh them.</p>

<p><em>If you&rsquo;re on Postgres 9.3, the above caveats about preventing reads still does exist</em></p>

<h3>7. Window functions</h3>

<p>Window functions are perhaps still one of the more complex things of SQL to understand. In short they let you order the results of a query, then compute something from one row to the next, something generally hard to do without procedural SQL. You can do some very basic things with them such as rank where <a href="http://postgresguide.com/sql/window.html">each result appears</a> ordered by some value, or something more complex like compute <a href="http://www.craigkerstiens.com/2014/02/26/Tracking-MoM-growth-in-SQL/">MoM growth directly in SQL</a>.</p>

<h3>8. A simpler method for pivot tables</h3>

<p>Table_func is often referenced as the way to compute a pivot table in Postgres. Sadly though it&rsquo;s pretty difficult to use, and the more basic method would be to just do it with raw SQL. This will get much better with <a href="http://www.craigkerstiens.com/2015/12/27/postgres-9-5-feature-rundown/">Postgres 9.5</a>, but until then something where you sum up each condition where it&rsquo;s true or false and then totals is much simpler to reason about:</p>

<pre><code>select date,
       sum(case when type = 'OSX' then val end) as osx,
       sum(case when type = 'Windows' then val end) as windows,
       sum(case when type = 'Linux' then val end) as linux
from daily_visits_per_os
group by date
order by date
limit 4;
</code></pre>

<p><em>Example query courtesy of <a href="http://www.twitter.com/tapoueh">Dimitri Fontaine</a> and <a href="http://tapoueh.org/blog/2013/07/04-Simple-case-for-pivoting">his blog</a>.</em></p>

<h3>9. PostGIS</h3>

<p>Sadly on this one I&rsquo;m far from an expert. PostGIS is arguably the best option of any GIS database options. The fact that you get all of the standard Postgres benefits with it makes it even more powerful–a great example of this is GiST indexes which came to Postgres in recent years and offers great performance gains for PostGIS.</p>

<p>If you&rsquo;re doing something with geospatial data and need something more than the easy to use <code>earth_distance</code> extension then crack open PostGIS.</p>

<h3>10. JSONB</h3>

<p>I almost debated leaving this one off the list, ever since Postgres 9.2 JSON has been at least one of the marquees in each Postgres release. JSON arrived with much hype, and JSONB fulfilled on the initial hype of Postgres starting to truly compete as a document database. JSONB only continues to become more powerful with <a href="http://www.craigkerstiens.com/2015/12/08/massive-json/">better libraries</a> for taking advantage of it, and it&rsquo;s <a href="https://wiki.postgresql.org/wiki/What's_new_in_PostgreSQL_9.5#JSONB-modifying_operators_and_functions">functions improving</a> with each release.</p>

<p>If you&rsquo;re doing anything with JSON or playing with another document database and ignoring JSONB you&rsquo;re missing out, of course don&rsquo;t forget the GIN and GiST indexes to really get the benefits of it.</p>

<h3>The year ahead</h3>

<p>Postgres 9.5/9.6 should continue to improve and bring many new features in the years ahead, what&rsquo;s your preference for something that doesn&rsquo;t exist yet but you do want to see land in Postgres. Let me know <a href="http://www.twitter.com/craigkerstiens">@craigkerstiens</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Postgres 9.5 - The feature rundown]]></title>
    <link href="http://www.craigkerstiens.com/2015/12/27/postgres-9-5-feature-rundown/"/>
    <updated>2015-12-27T00:00:00-08:00</updated>
    <id>http://www.craigkerstiens.com/2015/12/27/postgres-9-5-feature-rundown</id>
    <content type="html"><![CDATA[<p>The headline of Postgres 9.5 is undoubtedly: Insert&hellip; on conflict do nothing/update or more commonly known as Upsert or Merge. This removes one of the last remaining features which other databases had over Postgres. Sure we&rsquo;ll take a look at it, but first let&rsquo;s browse through some of the other features you can look forward to when Postgres 9.5 lands:</p>

<!--more-->


<h3>Grouping sets, cube, rollup</h3>

<p>Pivoting in Postgres has <a href="http://www.craigkerstiens.com/2013/06/27/Pivoting-in-Postgres/">sort of been possible</a> as has rolling up data, but it required you to know what those values and what you were projecting to, to be known. With the new functionality to allow you to group various sets together rollups as you&rsquo;d normally expect to do in something like Excel become trivial.</p>

<p>So now instead you simply add the grouping type just as you would on a normal group by:</p>

<pre><code>SELECT department, role, gender, count(*)
FROM employees
GROUP BY your_grouping_type_here;
</code></pre>

<p>By simply selecting the type of rollup you want to do Postgres will do the hard work for you. Let&rsquo;s take a look at the given example of department, role, gender:</p>

<ul>
<li><code>grouping sets</code> will project out the count for each specific key. As a result you&rsquo;d get each department key, with other keys as null, and the count for each that met that department.</li>
<li><code>cube</code> will give you the same values as above, but also the rollups of every individual combination. So in addition to the total for each department, you&rsquo;d get breakups by the department and gender, and department and role, and department and role and gender.</li>
<li><code>rollup</code> will give you a slightly similar version to cube but only give you the detailed groupings in the order they&rsquo;re presented. So if you specified <code>roll (department, role, gender)</code> you&rsquo;d have no rollup for department and gender alone.</li>
</ul>


<p><em>Check the what&rsquo;s new wiki for a bit more clarity on <a href="https://wiki.postgresql.org/wiki/What's_new_in_PostgreSQL_9.5#GROUPING_SETS.2C_CUBE_and_ROLLUP">examples and output</a></em></p>

<h3>Import foreign  schemas</h3>

<p>I only use foreign tables about once a month, but when I do use them they&rsquo;ve inevitably saved many hours of creating a one off ETL process. Even still the effort to setup new foreign tables has shown a bit of their infancy in Postgres. Now once you&rsquo;ve setup your foreign database, you can import the schema, either all of it or specific tables you prefer.</p>

<p>It&rsquo;s as simple as:</p>

<pre><code>IMPORT FOREIGN SCHEMA public
FROM SERVER some_other_db INTO reference_to_other_db;
</code></pre>

<h3>pg_rewind</h3>

<p>If you&rsquo;re managing your own Postgres instance for some reason and running HA, pg_rewind could become especially handy. Typically to spin up replication you have to first download the physical, also known as base, backup. Then you have to replay the Write-Ahead-Log or WAL–so it&rsquo;s up to date then you actually flip on replication.</p>

<p>Typically with databases when you fail over you shoot the other node in the head or <a href="https://en.wikipedia.org/wiki/STONITH">STONITH</a>. This means just get rid of it, completely throw it out. This is still a good practice, so bring it offline, make it inactive, but from there now you could then flip it into a mode and use pg_rewind. This could save you pulling down lots and lots of data to get a replica back up once you have failed over.</p>

<h3>Upsert</h3>

<p>Upsert of course will be the highlight of Postgres 9.5. I already talked about it some when <a href="http://www.craigkerstiens.com/2015/05/08/upsert-lands-in-postgres-9.5/">it initially landed</a>. The short of it is, if you&rsquo;re inserting a record and there&rsquo;s a conflict, you can choose to:</p>

<ul>
<li>Do nothing</li>
<li>Do some form of update</li>
</ul>


<p>Essentially this will let you have the typically experience of create or update that most frameworks provide but without a potential race condition of incorrect data.</p>

<h3>JSONB pretty</h3>

<p>There&rsquo;s a few updates <a href="https://wiki.postgresql.org/wiki/What's_new_in_PostgreSQL_9.5#JSONB-modifying_operators_and_functions">to JSONB</a>. The one I&rsquo;m most excited about is making JSONB output in psql read much more legibly.</p>

<p>If you&rsquo;ve got a JSONB field just give it a try with:</p>

<pre><code>SELECT jsonb_pretty(jsonb_column)
FROM foo;
</code></pre>

<h3>Give it a try</h3>

<p>Just in time for the new year <a href="http://www.postgresql.org/about/news/1631/">the RC is ready</a> and you can get hands on with it. Give it a try, and if there&rsquo;s more you&rsquo;d like to hear about Postgres please feel free to drop me a note <a href="mailto:craig.kerstiens@gmail.com">craig.kerstiens@gmail.com</a>.</p>

<script type="text/javascript">
  (function() {
    window._pa = window._pa || {};
    // _pa.orderId = "myOrderId"; // OPTIONAL: attach unique conversion identifier to conversions
    // _pa.revenue = "19.99"; // OPTIONAL: attach dynamic purchase values to conversions
    // _pa.productId = "myProductId"; // OPTIONAL: Include product ID for use with dynamic ads
    var pa = document.createElement('script'); pa.type = 'text/javascript'; pa.async = true;
    pa.src = ('https:' == document.location.protocol ? 'https:' : 'http:') + "//tag.marinsm.com/serve/517fd07cf1409000020002dc.js";
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(pa, s);
  })();
</script>

]]></content>
  </entry>
  
</feed>
